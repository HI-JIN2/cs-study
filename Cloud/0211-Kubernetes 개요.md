yaml
- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd


- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd


- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd


- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958


- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl
쿠버네티스의 서비스 개념을 구현하기 위해 각 노드에서 실행됨
클러스터 내부에서 서비스 와 파드 간 트래픽을 적절히 분배

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl
쿠버네티스의 서비스 개념을 구현하기 위해 각 노드에서 실행됨
클러스터 내부에서 서비스 와 파드 간 트래픽을 적절히 분배
쿠버네티스 서비스가 가상 IP(Cluster IP)를 사용해도 실제 파드로 트래픽이 전달되도록 합니다.
iptables, ipvs, userspace 모드를 활용해서 트래픽을 제어

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl
쿠버네티스의 서비스 개념을 구현하기 위해 각 노드에서 실행됨
클러스터 내부에서 서비스 와 파드 간 트래픽을 적절히 분배
쿠버네티스 서비스가 가상 IP(Cluster IP)를 사용해도 실제 파드로 트래픽이 전달되도록 합니다.
iptables, ipvs, userspace 모드를 활용해서 트래픽을 제어

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl
쿠버네티스의 서비스 개념을 구현하기 위해 각 노드에서 실행됨
클러스터 내부에서 서비스 와 파드 간 트래픽을 적절히 분배
쿠버네티스 서비스가 가상 IP(Cluster IP)를 사용해도 실제 파드로 트래픽이 전달되도록 합니다.
iptables, ipvs, userspace 모드를 활용해서 트래픽을 제어
kubectl get pods -n kube-system 명령으로 확인 가능

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl
쿠버네티스의 서비스 개념을 구현하기 위해 각 노드에서 실행됨
클러스터 내부에서 서비스 와 파드 간 트래픽을 적절히 분배
쿠버네티스 서비스가 가상 IP(Cluster IP)를 사용해도 실제 파드로 트래픽이 전달되도록 합니다.
iptables, ipvs, userspace 모드를 활용해서 트래픽을 제어
kubectl get pods -n kube-system 명령으로 확인 가능

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl
쿠버네티스의 서비스 개념을 구현하기 위해 각 노드에서 실행됨
클러스터 내부에서 서비스 와 파드 간 트래픽을 적절히 분배
쿠버네티스 서비스가 가상 IP(Cluster IP)를 사용해도 실제 파드로 트래픽이 전달되도록 합니다.
iptables, ipvs, userspace 모드를 활용해서 트래픽을 제어
kubectl get pods -n kube-system 명령으로 확인 가능

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl
쿠버네티스의 서비스 개념을 구현하기 위해 각 노드에서 실행됨
클러스터 내부에서 서비스 와 파드 간 트래픽을 적절히 분배
쿠버네티스 서비스가 가상 IP(Cluster IP)를 사용해도 실제 파드로 트래픽이 전달되도록 합니다.
iptables, ipvs, userspace 모드를 활용해서 트래픽을 제어
kubectl get pods -n kube-system 명령으로 확인 가능

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl
쿠버네티스의 서비스 개념을 구현하기 위해 각 노드에서 실행됨
클러스터 내부에서 서비스 와 파드 간 트래픽을 적절히 분배
쿠버네티스 서비스가 가상 IP(Cluster IP)를 사용해도 실제 파드로 트래픽이 전달되도록 합니다.
iptables, ipvs, userspace 모드를 활용해서 트래픽을 제어
kubectl get pods -n kube-system 명령으로 확인 가능

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl
쿠버네티스의 서비스 개념을 구현하기 위해 각 노드에서 실행됨
클러스터 내부에서 서비스 와 파드 간 트래픽을 적절히 분배
쿠버네티스 서비스가 가상 IP(Cluster IP)를 사용해도 실제 파드로 트래픽이 전달되도록 합니다.
iptables, ipvs, userspace 모드를 활용해서 트래픽을 제어
kubectl get pods -n kube-system 명령으로 확인 가능

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl
쿠버네티스의 서비스 개념을 구현하기 위해 각 노드에서 실행됨
클러스터 내부에서 서비스 와 파드 간 트래픽을 적절히 분배
쿠버네티스 서비스가 가상 IP(Cluster IP)를 사용해도 실제 파드로 트래픽이 전달되도록 합니다.
iptables, ipvs, userspace 모드를 활용해서 트래픽을 제어
kubectl get pods -n kube-system 명령으로 확인 가능

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl
쿠버네티스의 서비스 개념을 구현하기 위해 각 노드에서 실행됨
클러스터 내부에서 서비스 와 파드 간 트래픽을 적절히 분배
쿠버네티스 서비스가 가상 IP(Cluster IP)를 사용해도 실제 파드로 트래픽이 전달되도록 합니다.
iptables, ipvs, userspace 모드를 활용해서 트래픽을 제어
kubectl get pods -n kube-system 명령으로 확인 가능

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl
쿠버네티스의 서비스 개념을 구현하기 위해 각 노드에서 실행됨
클러스터 내부에서 서비스 와 파드 간 트래픽을 적절히 분배
쿠버네티스 서비스가 가상 IP(Cluster IP)를 사용해도 실제 파드로 트래픽이 전달되도록 합니다.
iptables, ipvs, userspace 모드를 활용해서 트래픽을 제어
kubectl get pods -n kube-system 명령으로 확인 가능

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl
쿠버네티스의 서비스 개념을 구현하기 위해 각 노드에서 실행됨
클러스터 내부에서 서비스 와 파드 간 트래픽을 적절히 분배
쿠버네티스 서비스가 가상 IP(Cluster IP)를 사용해도 실제 파드로 트래픽이 전달되도록 합니다.
iptables, ipvs, userspace 모드를 활용해서 트래픽을 제어
kubectl get pods -n kube-system 명령으로 확인 가능

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl
쿠버네티스의 서비스 개념을 구현하기 위해 각 노드에서 실행됨
클러스터 내부에서 서비스 와 파드 간 트래픽을 적절히 분배
쿠버네티스 서비스가 가상 IP(Cluster IP)를 사용해도 실제 파드로 트래픽이 전달되도록 합니다.
iptables, ipvs, userspace 모드를 활용해서 트래픽을 제어
kubectl get pods -n kube-system 명령으로 확인 가능

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl
쿠버네티스의 서비스 개념을 구현하기 위해 각 노드에서 실행됨
클러스터 내부에서 서비스 와 파드 간 트래픽을 적절히 분배
쿠버네티스 서비스가 가상 IP(Cluster IP)를 사용해도 실제 파드로 트래픽이 전달되도록 합니다.
iptables, ipvs, userspace 모드를 활용해서 트래픽을 제어
kubectl get pods -n kube-system 명령으로 확인 가능

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl
쿠버네티스의 서비스 개념을 구현하기 위해 각 노드에서 실행됨
클러스터 내부에서 서비스 와 파드 간 트래픽을 적절히 분배
쿠버네티스 서비스가 가상 IP(Cluster IP)를 사용해도 실제 파드로 트래픽이 전달되도록 합니다.
iptables, ipvs, userspace 모드를 활용해서 트래픽을 제어
kubectl get pods -n kube-system 명령으로 확인 가능

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl
쿠버네티스의 서비스 개념을 구현하기 위해 각 노드에서 실행됨
클러스터 내부에서 서비스 와 파드 간 트래픽을 적절히 분배
쿠버네티스 서비스가 가상 IP(Cluster IP)를 사용해도 실제 파드로 트래픽이 전달되도록 합니다.
iptables, ipvs, userspace 모드를 활용해서 트래픽을 제어
kubectl get pods -n kube-system 명령으로 확인 가능

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl
쿠버네티스의 서비스 개념을 구현하기 위해 각 노드에서 실행됨
클러스터 내부에서 서비스 와 파드 간 트래픽을 적절히 분배
쿠버네티스 서비스가 가상 IP(Cluster IP)를 사용해도 실제 파드로 트래픽이 전달되도록 합니다.
iptables, ipvs, userspace 모드를 활용해서 트래픽을 제어
kubectl get pods -n kube-system 명령으로 확인 가능

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl
쿠버네티스의 서비스 개념을 구현하기 위해 각 노드에서 실행됨
클러스터 내부에서 서비스 와 파드 간 트래픽을 적절히 분배
쿠버네티스 서비스가 가상 IP(Cluster IP)를 사용해도 실제 파드로 트래픽이 전달되도록 합니다.
iptables, ipvs, userspace 모드를 활용해서 트래픽을 제어
kubectl get pods -n kube-system 명령으로 확인 가능

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl
쿠버네티스의 서비스 개념을 구현하기 위해 각 노드에서 실행됨
클러스터 내부에서 서비스 와 파드 간 트래픽을 적절히 분배
쿠버네티스 서비스가 가상 IP(Cluster IP)를 사용해도 실제 파드로 트래픽이 전달되도록 합니다.
iptables, ipvs, userspace 모드를 활용해서 트래픽을 제어
kubectl get pods -n kube-system 명령으로 확인 가능

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl
쿠버네티스의 서비스 개념을 구현하기 위해 각 노드에서 실행됨
클러스터 내부에서 서비스 와 파드 간 트래픽을 적절히 분배
쿠버네티스 서비스가 가상 IP(Cluster IP)를 사용해도 실제 파드로 트래픽이 전달되도록 합니다.
iptables, ipvs, userspace 모드를 활용해서 트래픽을 제어
kubectl get pods -n kube-system 명령으로 확인 가능

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl
쿠버네티스의 서비스 개념을 구현하기 위해 각 노드에서 실행됨
클러스터 내부에서 서비스 와 파드 간 트래픽을 적절히 분배
쿠버네티스 서비스가 가상 IP(Cluster IP)를 사용해도 실제 파드로 트래픽이 전달되도록 합니다.
iptables, ipvs, userspace 모드를 활용해서 트래픽을 제어
kubectl get pods -n kube-system 명령으로 확인 가능

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl
쿠버네티스의 서비스 개념을 구현하기 위해 각 노드에서 실행됨
클러스터 내부에서 서비스 와 파드 간 트래픽을 적절히 분배
쿠버네티스 서비스가 가상 IP(Cluster IP)를 사용해도 실제 파드로 트래픽이 전달되도록 합니다.
iptables, ipvs, userspace 모드를 활용해서 트래픽을 제어
kubectl get pods -n kube-system 명령으로 확인 가능

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl
쿠버네티스의 서비스 개념을 구현하기 위해 각 노드에서 실행됨
클러스터 내부에서 서비스 와 파드 간 트래픽을 적절히 분배
쿠버네티스 서비스가 가상 IP(Cluster IP)를 사용해도 실제 파드로 트래픽이 전달되도록 합니다.
iptables, ipvs, userspace 모드를 활용해서 트래픽을 제어
kubectl get pods -n kube-system 명령으로 확인 가능

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl
쿠버네티스의 서비스 개념을 구현하기 위해 각 노드에서 실행됨
클러스터 내부에서 서비스 와 파드 간 트래픽을 적절히 분배
쿠버네티스 서비스가 가상 IP(Cluster IP)를 사용해도 실제 파드로 트래픽이 전달되도록 합니다.
iptables, ipvs, userspace 모드를 활용해서 트래픽을 제어
kubectl get pods -n kube-system 명령으로 확인 가능

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl
쿠버네티스의 서비스 개념을 구현하기 위해 각 노드에서 실행됨
클러스터 내부에서 서비스 와 파드 간 트래픽을 적절히 분배
쿠버네티스 서비스가 가상 IP(Cluster IP)를 사용해도 실제 파드로 트래픽이 전달되도록 합니다.
iptables, ipvs, userspace 모드를 활용해서 트래픽을 제어
kubectl get pods -n kube-system 명령으로 확인 가능

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl
쿠버네티스의 서비스 개념을 구현하기 위해 각 노드에서 실행됨
클러스터 내부에서 서비스 와 파드 간 트래픽을 적절히 분배
쿠버네티스 서비스가 가상 IP(Cluster IP)를 사용해도 실제 파드로 트래픽이 전달되도록 합니다.
iptables, ipvs, userspace 모드를 활용해서 트래픽을 제어
kubectl get pods -n kube-system 명령으로 확인 가능

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl
쿠버네티스의 서비스 개념을 구현하기 위해 각 노드에서 실행됨
클러스터 내부에서 서비스 와 파드 간 트래픽을 적절히 분배
쿠버네티스 서비스가 가상 IP(Cluster IP)를 사용해도 실제 파드로 트래픽이 전달되도록 합니다.
iptables, ipvs, userspace 모드를 활용해서 트래픽을 제어
kubectl get pods -n kube-system 명령으로 확인 가능

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl
쿠버네티스의 서비스 개념을 구현하기 위해 각 노드에서 실행됨
클러스터 내부에서 서비스 와 파드 간 트래픽을 적절히 분배
쿠버네티스 서비스가 가상 IP(Cluster IP)를 사용해도 실제 파드로 트래픽이 전달되도록 합니다.
iptables, ipvs, userspace 모드를 활용해서 트래픽을 제어
kubectl get pods -n kube-system 명령으로 확인 가능

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl
쿠버네티스의 서비스 개념을 구현하기 위해 각 노드에서 실행됨
클러스터 내부에서 서비스 와 파드 간 트래픽을 적절히 분배
쿠버네티스 서비스가 가상 IP(Cluster IP)를 사용해도 실제 파드로 트래픽이 전달되도록 합니다.
iptables, ipvs, userspace 모드를 활용해서 트래픽을 제어
kubectl get pods -n kube-system 명령으로 확인 가능

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl
쿠버네티스의 서비스 개념을 구현하기 위해 각 노드에서 실행됨
클러스터 내부에서 서비스 와 파드 간 트래픽을 적절히 분배
쿠버네티스 서비스가 가상 IP(Cluster IP)를 사용해도 실제 파드로 트래픽이 전달되도록 합니다.
iptables, ipvs, userspace 모드를 활용해서 트래픽을 제어
kubectl get pods -n kube-system 명령으로 확인 가능

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl
쿠버네티스의 서비스 개념을 구현하기 위해 각 노드에서 실행됨
클러스터 내부에서 서비스 와 파드 간 트래픽을 적절히 분배
쿠버네티스 서비스가 가상 IP(Cluster IP)를 사용해도 실제 파드로 트래픽이 전달되도록 합니다.
iptables, ipvs, userspace 모드를 활용해서 트래픽을 제어
kubectl get pods -n kube-system 명령으로 확인 가능

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl
쿠버네티스의 서비스 개념을 구현하기 위해 각 노드에서 실행됨
클러스터 내부에서 서비스 와 파드 간 트래픽을 적절히 분배
쿠버네티스 서비스가 가상 IP(Cluster IP)를 사용해도 실제 파드로 트래픽이 전달되도록 합니다.
iptables, ipvs, userspace 모드를 활용해서 트래픽을 제어
kubectl get pods -n kube-system 명령으로 확인 가능

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl
쿠버네티스의 서비스 개념을 구현하기 위해 각 노드에서 실행됨
클러스터 내부에서 서비스 와 파드 간 트래픽을 적절히 분배
쿠버네티스 서비스가 가상 IP(Cluster IP)를 사용해도 실제 파드로 트래픽이 전달되도록 합니다.
iptables, ipvs, userspace 모드를 활용해서 트래픽을 제어
kubectl get pods -n kube-system 명령으로 확인 가능

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl
쿠버네티스의 서비스 개념을 구현하기 위해 각 노드에서 실행됨
클러스터 내부에서 서비스 와 파드 간 트래픽을 적절히 분배
쿠버네티스 서비스가 가상 IP(Cluster IP)를 사용해도 실제 파드로 트래픽이 전달되도록 합니다.
iptables, ipvs, userspace 모드를 활용해서 트래픽을 제어
kubectl get pods -n kube-system 명령으로 확인 가능

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl
쿠버네티스의 서비스 개념을 구현하기 위해 각 노드에서 실행됨
클러스터 내부에서 서비스 와 파드 간 트래픽을 적절히 분배
쿠버네티스 서비스가 가상 IP(Cluster IP)를 사용해도 실제 파드로 트래픽이 전달되도록 합니다.
iptables, ipvs, userspace 모드를 활용해서 트래픽을 제어
kubectl get pods -n kube-system 명령으로 확인 가능

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl
쿠버네티스의 서비스 개념을 구현하기 위해 각 노드에서 실행됨
클러스터 내부에서 서비스 와 파드 간 트래픽을 적절히 분배
쿠버네티스 서비스가 가상 IP(Cluster IP)를 사용해도 실제 파드로 트래픽이 전달되도록 합니다.
iptables, ipvs, userspace 모드를 활용해서 트래픽을 제어
kubectl get pods -n kube-system 명령으로 확인 가능

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl
쿠버네티스의 서비스 개념을 구현하기 위해 각 노드에서 실행됨
클러스터 내부에서 서비스 와 파드 간 트래픽을 적절히 분배
쿠버네티스 서비스가 가상 IP(Cluster IP)를 사용해도 실제 파드로 트래픽이 전달되도록 합니다.
iptables, ipvs, userspace 모드를 활용해서 트래픽을 제어
kubectl get pods -n kube-system 명령으로 확인 가능

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl
쿠버네티스의 서비스 개념을 구현하기 위해 각 노드에서 실행됨
클러스터 내부에서 서비스 와 파드 간 트래픽을 적절히 분배
쿠버네티스 서비스가 가상 IP(Cluster IP)를 사용해도 실제 파드로 트래픽이 전달되도록 합니다.
iptables, ipvs, userspace 모드를 활용해서 트래픽을 제어
kubectl get pods -n kube-system 명령으로 확인 가능

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl
쿠버네티스의 서비스 개념을 구현하기 위해 각 노드에서 실행됨
클러스터 내부에서 서비스 와 파드 간 트래픽을 적절히 분배
쿠버네티스 서비스가 가상 IP(Cluster IP)를 사용해도 실제 파드로 트래픽이 전달되도록 합니다.
iptables, ipvs, userspace 모드를 활용해서 트래픽을 제어
kubectl get pods -n kube-system 명령으로 확인 가능
kubernetes Cluster를 운영하다 보면 특정  Worker Node에는 특정 성격의 Pod만 배포하고 싶을 때가 있는데 특정 성격의 pod만 배치하는 것을 taint 라고 하고 toleration은 모든 모드는 전부 배치
Cluster를 구성을 할 때 비용 상의 문제로 GPU가 있는 Node 와 GPU가 없는 Node가 있을 때 GPU를 사용해야 하는 Pod는 GPU가 있는 노드에게만 배포하는 것이 taint 

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl
쿠버네티스의 서비스 개념을 구현하기 위해 각 노드에서 실행됨
클러스터 내부에서 서비스 와 파드 간 트래픽을 적절히 분배
쿠버네티스 서비스가 가상 IP(Cluster IP)를 사용해도 실제 파드로 트래픽이 전달되도록 합니다.
iptables, ipvs, userspace 모드를 활용해서 트래픽을 제어
kubectl get pods -n kube-system 명령으로 확인 가능
kubernetes Cluster를 운영하다 보면 특정  Worker Node에는 특정 성격의 Pod만 배포하고 싶을 때가 있는데 특정 성격의 pod만 배치하는 것을 taint 라고 하고 toleration은 모든 모드는 전부 배치
Cluster를 구성을 할 때 비용 상의 문제로 GPU가 있는 Node 와 GPU가 없는 Node가 있을 때 GPU를 사용해야 하는 Pod는 GPU가 있는 노드에게만 배포하는 것이 taint 

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl
쿠버네티스의 서비스 개념을 구현하기 위해 각 노드에서 실행됨
클러스터 내부에서 서비스 와 파드 간 트래픽을 적절히 분배
쿠버네티스 서비스가 가상 IP(Cluster IP)를 사용해도 실제 파드로 트래픽이 전달되도록 합니다.
iptables, ipvs, userspace 모드를 활용해서 트래픽을 제어
kubectl get pods -n kube-system 명령으로 확인 가능
kubernetes Cluster를 운영하다 보면 특정  Worker Node에는 특정 성격의 Pod만 배포하고 싶을 때가 있는데 특정 성격의 pod만 배치하는 것을 taint 라고 하고 toleration은 모든 모드는 전부 배치
Cluster를 구성을 할 때 비용 상의 문제로 GPU가 있는 Node 와 GPU가 없는 Node가 있을 때 GPU를 사용해야 하는 Pod는 GPU가 있는 노드에게만 배포하는 것이 taint 

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl
쿠버네티스의 서비스 개념을 구현하기 위해 각 노드에서 실행됨
클러스터 내부에서 서비스 와 파드 간 트래픽을 적절히 분배
쿠버네티스 서비스가 가상 IP(Cluster IP)를 사용해도 실제 파드로 트래픽이 전달되도록 합니다.
iptables, ipvs, userspace 모드를 활용해서 트래픽을 제어
kubectl get pods -n kube-system 명령으로 확인 가능
kubernetes Cluster를 운영하다 보면 특정  Worker Node에는 특정 성격의 Pod만 배포하고 싶을 때가 있는데 특정 성격의 pod만 배치하는 것을 taint 라고 하고 toleration은 모든 모드는 전부 배치
Cluster를 구성을 할 때 비용 상의 문제로 GPU가 있는 Node 와 GPU가 없는 Node가 있을 때 GPU를 사용해야 하는 Pod는 GPU가 있는 노드에게만 배포하는 것이 taint 

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl
쿠버네티스의 서비스 개념을 구현하기 위해 각 노드에서 실행됨
클러스터 내부에서 서비스 와 파드 간 트래픽을 적절히 분배
쿠버네티스 서비스가 가상 IP(Cluster IP)를 사용해도 실제 파드로 트래픽이 전달되도록 합니다.
iptables, ipvs, userspace 모드를 활용해서 트래픽을 제어
kubectl get pods -n kube-system 명령으로 확인 가능
kubernetes Cluster를 운영하다 보면 특정  Worker Node에는 특정 성격의 Pod만 배포하고 싶을 때가 있는데 특정 성격의 pod만 배치하는 것을 taint 라고 하고 toleration은 모든 모드는 전부 배치
Cluster를 구성을 할 때 비용 상의 문제로 GPU가 있는 Node 와 GPU가 없는 Node가 있을 때 GPU를 사용해야 하는 Pod는 GPU가 있는 노드에게만 배포하는 것이 taint 

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl
쿠버네티스의 서비스 개념을 구현하기 위해 각 노드에서 실행됨
클러스터 내부에서 서비스 와 파드 간 트래픽을 적절히 분배
쿠버네티스 서비스가 가상 IP(Cluster IP)를 사용해도 실제 파드로 트래픽이 전달되도록 합니다.
iptables, ipvs, userspace 모드를 활용해서 트래픽을 제어
kubectl get pods -n kube-system 명령으로 확인 가능
kubernetes Cluster를 운영하다 보면 특정  Worker Node에는 특정 성격의 Pod만 배포하고 싶을 때가 있는데 특정 성격의 pod만 배치하는 것을 taint 라고 하고 toleration은 모든 모드는 전부 배치
Cluster를 구성을 할 때 비용 상의 문제로 GPU가 있는 Node 와 GPU가 없는 Node가 있을 때 GPU를 사용해야 하는 Pod는 GPU가 있는 노드에게만 배포하는 것이 taint 

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl
쿠버네티스의 서비스 개념을 구현하기 위해 각 노드에서 실행됨
클러스터 내부에서 서비스 와 파드 간 트래픽을 적절히 분배
쿠버네티스 서비스가 가상 IP(Cluster IP)를 사용해도 실제 파드로 트래픽이 전달되도록 합니다.
iptables, ipvs, userspace 모드를 활용해서 트래픽을 제어
kubectl get pods -n kube-system 명령으로 확인 가능
kubernetes Cluster를 운영하다 보면 특정  Worker Node에는 특정 성격의 Pod만 배포하고 싶을 때가 있는데 특정 성격의 pod만 배치하는 것을 taint 라고 하고 toleration은 모든 모드는 전부 배치
Cluster를 구성을 할 때 비용 상의 문제로 GPU가 있는 Node 와 GPU가 없는 Node가 있을 때 GPU를 사용해야 하는 Pod는 GPU가 있는 노드에게만 배포하는 것이 taint 

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl
쿠버네티스의 서비스 개념을 구현하기 위해 각 노드에서 실행됨
클러스터 내부에서 서비스 와 파드 간 트래픽을 적절히 분배
쿠버네티스 서비스가 가상 IP(Cluster IP)를 사용해도 실제 파드로 트래픽이 전달되도록 합니다.
iptables, ipvs, userspace 모드를 활용해서 트래픽을 제어
kubectl get pods -n kube-system 명령으로 확인 가능
kubernetes Cluster를 운영하다 보면 특정  Worker Node에는 특정 성격의 Pod만 배포하고 싶을 때가 있는데 특정 성격의 pod만 배치하는 것을 taint 라고 하고 toleration은 모든 모드는 전부 배치
Cluster를 구성을 할 때 비용 상의 문제로 GPU가 있는 Node 와 GPU가 없는 Node가 있을 때 GPU를 사용해야 하는 Pod는 GPU가 있는 노드에게만 배포하는 것이 taint 

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl
쿠버네티스의 서비스 개념을 구현하기 위해 각 노드에서 실행됨
클러스터 내부에서 서비스 와 파드 간 트래픽을 적절히 분배
쿠버네티스 서비스가 가상 IP(Cluster IP)를 사용해도 실제 파드로 트래픽이 전달되도록 합니다.
iptables, ipvs, userspace 모드를 활용해서 트래픽을 제어
kubectl get pods -n kube-system 명령으로 확인 가능
kubernetes Cluster를 운영하다 보면 특정  Worker Node에는 특정 성격의 Pod만 배포하고 싶을 때가 있는데 특정 성격의 pod만 배치하는 것을 taint 라고 하고 toleration은 모든 모드는 전부 배치
Cluster를 구성을 할 때 비용 상의 문제로 GPU가 있는 Node 와 GPU가 없는 Node가 있을 때 GPU를 사용해야 하는 Pod는 GPU가 있는 노드에게만 배포하는 것이 taint 

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl
쿠버네티스의 서비스 개념을 구현하기 위해 각 노드에서 실행됨
클러스터 내부에서 서비스 와 파드 간 트래픽을 적절히 분배
쿠버네티스 서비스가 가상 IP(Cluster IP)를 사용해도 실제 파드로 트래픽이 전달되도록 합니다.
iptables, ipvs, userspace 모드를 활용해서 트래픽을 제어
kubectl get pods -n kube-system 명령으로 확인 가능
kubernetes Cluster를 운영하다 보면 특정  Worker Node에는 특정 성격의 Pod만 배포하고 싶을 때가 있는데 특정 성격의 pod만 배치하는 것을 taint 라고 하고 toleration은 모든 모드는 전부 배치
Cluster를 구성을 할 때 비용 상의 문제로 GPU가 있는 Node 와 GPU가 없는 Node가 있을 때 GPU를 사용해야 하는 Pod는 GPU가 있는 노드에게만 배포하는 것이 taint 

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl
쿠버네티스의 서비스 개념을 구현하기 위해 각 노드에서 실행됨
클러스터 내부에서 서비스 와 파드 간 트래픽을 적절히 분배
쿠버네티스 서비스가 가상 IP(Cluster IP)를 사용해도 실제 파드로 트래픽이 전달되도록 합니다.
iptables, ipvs, userspace 모드를 활용해서 트래픽을 제어
kubectl get pods -n kube-system 명령으로 확인 가능
kubernetes Cluster를 운영하다 보면 특정  Worker Node에는 특정 성격의 Pod만 배포하고 싶을 때가 있는데 특정 성격의 pod만 배치하는 것을 taint 라고 하고 toleration은 모든 모드는 전부 배치
Cluster를 구성을 할 때 비용 상의 문제로 GPU가 있는 Node 와 GPU가 없는 Node가 있을 때 GPU를 사용해야 하는 Pod는 GPU가 있는 노드에게만 배포하는 것이 taint 

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl
쿠버네티스의 서비스 개념을 구현하기 위해 각 노드에서 실행됨
클러스터 내부에서 서비스 와 파드 간 트래픽을 적절히 분배
쿠버네티스 서비스가 가상 IP(Cluster IP)를 사용해도 실제 파드로 트래픽이 전달되도록 합니다.
iptables, ipvs, userspace 모드를 활용해서 트래픽을 제어
kubectl get pods -n kube-system 명령으로 확인 가능
kubernetes Cluster를 운영하다 보면 특정  Worker Node에는 특정 성격의 Pod만 배포하고 싶을 때가 있는데 특정 성격의 pod만 배치하는 것을 taint 라고 하고 toleration은 모든 모드는 전부 배치
Cluster를 구성을 할 때 비용 상의 문제로 GPU가 있는 Node 와 GPU가 없는 Node가 있을 때 GPU를 사용해야 하는 Pod는 GPU가 있는 노드에게만 배포하는 것이 taint 

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl
쿠버네티스의 서비스 개념을 구현하기 위해 각 노드에서 실행됨
클러스터 내부에서 서비스 와 파드 간 트래픽을 적절히 분배
쿠버네티스 서비스가 가상 IP(Cluster IP)를 사용해도 실제 파드로 트래픽이 전달되도록 합니다.
iptables, ipvs, userspace 모드를 활용해서 트래픽을 제어
kubectl get pods -n kube-system 명령으로 확인 가능
kubernetes Cluster를 운영하다 보면 특정  Worker Node에는 특정 성격의 Pod만 배포하고 싶을 때가 있는데 특정 성격의 pod만 배치하는 것을 taint 라고 하고 toleration은 모든 모드는 전부 배치
Cluster를 구성을 할 때 비용 상의 문제로 GPU가 있는 Node 와 GPU가 없는 Node가 있을 때 GPU를 사용해야 하는 Pod는 GPU가 있는 노드에게만 배포하는 것이 taint 

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl
쿠버네티스의 서비스 개념을 구현하기 위해 각 노드에서 실행됨
클러스터 내부에서 서비스 와 파드 간 트래픽을 적절히 분배
쿠버네티스 서비스가 가상 IP(Cluster IP)를 사용해도 실제 파드로 트래픽이 전달되도록 합니다.
iptables, ipvs, userspace 모드를 활용해서 트래픽을 제어
kubectl get pods -n kube-system 명령으로 확인 가능
kubernetes Cluster를 운영하다 보면 특정  Worker Node에는 특정 성격의 Pod만 배포하고 싶을 때가 있는데 특정 성격의 pod만 배치하는 것을 taint 라고 하고 toleration은 모든 모드는 전부 배치
Cluster를 구성을 할 때 비용 상의 문제로 GPU가 있는 Node 와 GPU가 없는 Node가 있을 때 GPU를 사용해야 하는 Pod는 GPU가 있는 노드에게만 배포하는 것이 taint 

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl
쿠버네티스의 서비스 개념을 구현하기 위해 각 노드에서 실행됨
클러스터 내부에서 서비스 와 파드 간 트래픽을 적절히 분배
쿠버네티스 서비스가 가상 IP(Cluster IP)를 사용해도 실제 파드로 트래픽이 전달되도록 합니다.
iptables, ipvs, userspace 모드를 활용해서 트래픽을 제어
kubectl get pods -n kube-system 명령으로 확인 가능
kubernetes Cluster를 운영하다 보면 특정  Worker Node에는 특정 성격의 Pod만 배포하고 싶을 때가 있는데 특정 성격의 pod만 배치하는 것을 taint 라고 하고 toleration은 모든 모드는 전부 배치
Cluster를 구성을 할 때 비용 상의 문제로 GPU가 있는 Node 와 GPU가 없는 Node가 있을 때 GPU를 사용해야 하는 Pod는 GPU가 있는 노드에게만 배포하는 것이 taint 

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl
쿠버네티스의 서비스 개념을 구현하기 위해 각 노드에서 실행됨
클러스터 내부에서 서비스 와 파드 간 트래픽을 적절히 분배
쿠버네티스 서비스가 가상 IP(Cluster IP)를 사용해도 실제 파드로 트래픽이 전달되도록 합니다.
iptables, ipvs, userspace 모드를 활용해서 트래픽을 제어
kubectl get pods -n kube-system 명령으로 확인 가능
kubernetes Cluster를 운영하다 보면 특정  Worker Node에는 특정 성격의 Pod만 배포하고 싶을 때가 있는데 특정 성격의 pod만 배치하는 것을 taint 라고 하고 toleration은 모든 모드는 전부 배치
Cluster를 구성을 할 때 비용 상의 문제로 GPU가 있는 Node 와 GPU가 없는 Node가 있을 때 GPU를 사용해야 하는 Pod는 GPU가 있는 노드에게만 배포하는 것이 taint 

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl
쿠버네티스의 서비스 개념을 구현하기 위해 각 노드에서 실행됨
클러스터 내부에서 서비스 와 파드 간 트래픽을 적절히 분배
쿠버네티스 서비스가 가상 IP(Cluster IP)를 사용해도 실제 파드로 트래픽이 전달되도록 합니다.
iptables, ipvs, userspace 모드를 활용해서 트래픽을 제어
kubectl get pods -n kube-system 명령으로 확인 가능
kubernetes Cluster를 운영하다 보면 특정  Worker Node에는 특정 성격의 Pod만 배포하고 싶을 때가 있는데 특정 성격의 pod만 배치하는 것을 taint 라고 하고 toleration은 모든 모드는 전부 배치
Cluster를 구성을 할 때 비용 상의 문제로 GPU가 있는 Node 와 GPU가 없는 Node가 있을 때 GPU를 사용해야 하는 Pod는 GPU가 있는 노드에게만 배포하는 것이 taint 

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl
쿠버네티스의 서비스 개념을 구현하기 위해 각 노드에서 실행됨
클러스터 내부에서 서비스 와 파드 간 트래픽을 적절히 분배
쿠버네티스 서비스가 가상 IP(Cluster IP)를 사용해도 실제 파드로 트래픽이 전달되도록 합니다.
iptables, ipvs, userspace 모드를 활용해서 트래픽을 제어
kubectl get pods -n kube-system 명령으로 확인 가능
kubernetes Cluster를 운영하다 보면 특정  Worker Node에는 특정 성격의 Pod만 배포하고 싶을 때가 있는데 특정 성격의 pod만 배치하는 것을 taint 라고 하고 toleration은 모든 모드는 전부 배치
Cluster를 구성을 할 때 비용 상의 문제로 GPU가 있는 Node 와 GPU가 없는 Node가 있을 때 GPU를 사용해야 하는 Pod는 GPU가 있는 노드에게만 배포하는 것이 taint 

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl
쿠버네티스의 서비스 개념을 구현하기 위해 각 노드에서 실행됨
클러스터 내부에서 서비스 와 파드 간 트래픽을 적절히 분배
쿠버네티스 서비스가 가상 IP(Cluster IP)를 사용해도 실제 파드로 트래픽이 전달되도록 합니다.
iptables, ipvs, userspace 모드를 활용해서 트래픽을 제어
kubectl get pods -n kube-system 명령으로 확인 가능
kubernetes Cluster를 운영하다 보면 특정  Worker Node에는 특정 성격의 Pod만 배포하고 싶을 때가 있는데 특정 성격의 pod만 배치하는 것을 taint 라고 하고 toleration은 모든 모드는 전부 배치
Cluster를 구성을 할 때 비용 상의 문제로 GPU가 있는 Node 와 GPU가 없는 Node가 있을 때 GPU를 사용해야 하는 Pod는 GPU가 있는 노드에게만 배포하는 것이 taint 

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl
쿠버네티스의 서비스 개념을 구현하기 위해 각 노드에서 실행됨
클러스터 내부에서 서비스 와 파드 간 트래픽을 적절히 분배
쿠버네티스 서비스가 가상 IP(Cluster IP)를 사용해도 실제 파드로 트래픽이 전달되도록 합니다.
iptables, ipvs, userspace 모드를 활용해서 트래픽을 제어
kubectl get pods -n kube-system 명령으로 확인 가능
kubernetes Cluster를 운영하다 보면 특정  Worker Node에는 특정 성격의 Pod만 배포하고 싶을 때가 있는데 특정 성격의 pod만 배치하는 것을 taint 라고 하고 toleration은 모든 모드는 전부 배치
Cluster를 구성을 할 때 비용 상의 문제로 GPU가 있는 Node 와 GPU가 없는 Node가 있을 때 GPU를 사용해야 하는 Pod는 GPU가 있는 노드에게만 배포하는 것이 taint 

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl
쿠버네티스의 서비스 개념을 구현하기 위해 각 노드에서 실행됨
클러스터 내부에서 서비스 와 파드 간 트래픽을 적절히 분배
쿠버네티스 서비스가 가상 IP(Cluster IP)를 사용해도 실제 파드로 트래픽이 전달되도록 합니다.
iptables, ipvs, userspace 모드를 활용해서 트래픽을 제어
kubectl get pods -n kube-system 명령으로 확인 가능
kubernetes Cluster를 운영하다 보면 특정  Worker Node에는 특정 성격의 Pod만 배포하고 싶을 때가 있는데 특정 성격의 pod만 배치하는 것을 taint 라고 하고 toleration은 모든 모드는 전부 배치
Cluster를 구성을 할 때 비용 상의 문제로 GPU가 있는 Node 와 GPU가 없는 Node가 있을 때 GPU를 사용해야 하는 Pod는 GPU가 있는 노드에게만 배포하는 것이 taint 

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl
쿠버네티스의 서비스 개념을 구현하기 위해 각 노드에서 실행됨
클러스터 내부에서 서비스 와 파드 간 트래픽을 적절히 분배
쿠버네티스 서비스가 가상 IP(Cluster IP)를 사용해도 실제 파드로 트래픽이 전달되도록 합니다.
iptables, ipvs, userspace 모드를 활용해서 트래픽을 제어
kubectl get pods -n kube-system 명령으로 확인 가능
kubernetes Cluster를 운영하다 보면 특정  Worker Node에는 특정 성격의 Pod만 배포하고 싶을 때가 있는데 특정 성격의 pod만 배치하는 것을 taint 라고 하고 toleration은 모든 모드는 전부 배치
Cluster를 구성을 할 때 비용 상의 문제로 GPU가 있는 Node 와 GPU가 없는 Node가 있을 때 GPU를 사용해야 하는 Pod는 GPU가 있는 노드에게만 배포하는 것이 taint 

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl
쿠버네티스의 서비스 개념을 구현하기 위해 각 노드에서 실행됨
클러스터 내부에서 서비스 와 파드 간 트래픽을 적절히 분배
쿠버네티스 서비스가 가상 IP(Cluster IP)를 사용해도 실제 파드로 트래픽이 전달되도록 합니다.
iptables, ipvs, userspace 모드를 활용해서 트래픽을 제어
kubectl get pods -n kube-system 명령으로 확인 가능
kubernetes Cluster를 운영하다 보면 특정  Worker Node에는 특정 성격의 Pod만 배포하고 싶을 때가 있는데 특정 성격의 pod만 배치하는 것을 taint 라고 하고 toleration은 모든 모드는 전부 배치
Cluster를 구성을 할 때 비용 상의 문제로 GPU가 있는 Node 와 GPU가 없는 Node가 있을 때 GPU를 사용해야 하는 Pod는 GPU가 있는 노드에게만 배포하는 것이 taint 

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl
쿠버네티스의 서비스 개념을 구현하기 위해 각 노드에서 실행됨
클러스터 내부에서 서비스 와 파드 간 트래픽을 적절히 분배
쿠버네티스 서비스가 가상 IP(Cluster IP)를 사용해도 실제 파드로 트래픽이 전달되도록 합니다.
iptables, ipvs, userspace 모드를 활용해서 트래픽을 제어
kubectl get pods -n kube-system 명령으로 확인 가능
kubernetes Cluster를 운영하다 보면 특정  Worker Node에는 특정 성격의 Pod만 배포하고 싶을 때가 있는데 특정 성격의 pod만 배치하는 것을 taint 라고 하고 toleration은 모든 모드는 전부 배치
Cluster를 구성을 할 때 비용 상의 문제로 GPU가 있는 Node 와 GPU가 없는 Node가 있을 때 GPU를 사용해야 하는 Pod는 GPU가 있는 노드에게만 배포하는 것이 taint 

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl
쿠버네티스의 서비스 개념을 구현하기 위해 각 노드에서 실행됨
클러스터 내부에서 서비스 와 파드 간 트래픽을 적절히 분배
쿠버네티스 서비스가 가상 IP(Cluster IP)를 사용해도 실제 파드로 트래픽이 전달되도록 합니다.
iptables, ipvs, userspace 모드를 활용해서 트래픽을 제어
kubectl get pods -n kube-system 명령으로 확인 가능
kubernetes Cluster를 운영하다 보면 특정  Worker Node에는 특정 성격의 Pod만 배포하고 싶을 때가 있는데 특정 성격의 pod만 배치하는 것을 taint 라고 하고 toleration은 모든 모드는 전부 배치
Cluster를 구성을 할 때 비용 상의 문제로 GPU가 있는 Node 와 GPU가 없는 Node가 있을 때 GPU를 사용해야 하는 Pod는 GPU가 있는 노드에게만 배포하는 것이 taint 

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl
쿠버네티스의 서비스 개념을 구현하기 위해 각 노드에서 실행됨
클러스터 내부에서 서비스 와 파드 간 트래픽을 적절히 분배
쿠버네티스 서비스가 가상 IP(Cluster IP)를 사용해도 실제 파드로 트래픽이 전달되도록 합니다.
iptables, ipvs, userspace 모드를 활용해서 트래픽을 제어
kubectl get pods -n kube-system 명령으로 확인 가능
kubernetes Cluster를 운영하다 보면 특정  Worker Node에는 특정 성격의 Pod만 배포하고 싶을 때가 있는데 특정 성격의 pod만 배치하는 것을 taint 라고 하고 toleration은 모든 모드는 전부 배치
Cluster를 구성을 할 때 비용 상의 문제로 GPU가 있는 Node 와 GPU가 없는 Node가 있을 때 GPU를 사용해야 하는 Pod는 GPU가 있는 노드에게만 배포하는 것이 taint 

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl
쿠버네티스의 서비스 개념을 구현하기 위해 각 노드에서 실행됨
클러스터 내부에서 서비스 와 파드 간 트래픽을 적절히 분배
쿠버네티스 서비스가 가상 IP(Cluster IP)를 사용해도 실제 파드로 트래픽이 전달되도록 합니다.
iptables, ipvs, userspace 모드를 활용해서 트래픽을 제어
kubectl get pods -n kube-system 명령으로 확인 가능
kubernetes Cluster를 운영하다 보면 특정  Worker Node에는 특정 성격의 Pod만 배포하고 싶을 때가 있는데 특정 성격의 pod만 배치하는 것을 taint 라고 하고 toleration은 모든 모드는 전부 배치
Cluster를 구성을 할 때 비용 상의 문제로 GPU가 있는 Node 와 GPU가 없는 Node가 있을 때 GPU를 사용해야 하는 Pod는 GPU가 있는 노드에게만 배포하는 것이 taint 

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl
쿠버네티스의 서비스 개념을 구현하기 위해 각 노드에서 실행됨
클러스터 내부에서 서비스 와 파드 간 트래픽을 적절히 분배
쿠버네티스 서비스가 가상 IP(Cluster IP)를 사용해도 실제 파드로 트래픽이 전달되도록 합니다.
iptables, ipvs, userspace 모드를 활용해서 트래픽을 제어
kubectl get pods -n kube-system 명령으로 확인 가능
kubernetes Cluster를 운영하다 보면 특정  Worker Node에는 특정 성격의 Pod만 배포하고 싶을 때가 있는데 특정 성격의 pod만 배치하는 것을 taint 라고 하고 toleration은 모든 모드는 전부 배치
Cluster를 구성을 할 때 비용 상의 문제로 GPU가 있는 Node 와 GPU가 없는 Node가 있을 때 GPU를 사용해야 하는 Pod는 GPU가 있는 노드에게만 배포하는 것이 taint 

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl
쿠버네티스의 서비스 개념을 구현하기 위해 각 노드에서 실행됨
클러스터 내부에서 서비스 와 파드 간 트래픽을 적절히 분배
쿠버네티스 서비스가 가상 IP(Cluster IP)를 사용해도 실제 파드로 트래픽이 전달되도록 합니다.
iptables, ipvs, userspace 모드를 활용해서 트래픽을 제어
kubectl get pods -n kube-system 명령으로 확인 가능
kubernetes Cluster를 운영하다 보면 특정  Worker Node에는 특정 성격의 Pod만 배포하고 싶을 때가 있는데 특정 성격의 pod만 배치하는 것을 taint 라고 하고 toleration은 모든 모드는 전부 배치
Cluster를 구성을 할 때 비용 상의 문제로 GPU가 있는 Node 와 GPU가 없는 Node가 있을 때 GPU를 사용해야 하는 Pod는 GPU가 있는 노드에게만 배포하는 것이 taint 

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl
쿠버네티스의 서비스 개념을 구현하기 위해 각 노드에서 실행됨
클러스터 내부에서 서비스 와 파드 간 트래픽을 적절히 분배
쿠버네티스 서비스가 가상 IP(Cluster IP)를 사용해도 실제 파드로 트래픽이 전달되도록 합니다.
iptables, ipvs, userspace 모드를 활용해서 트래픽을 제어
kubectl get pods -n kube-system 명령으로 확인 가능
kubernetes Cluster를 운영하다 보면 특정  Worker Node에는 특정 성격의 Pod만 배포하고 싶을 때가 있는데 특정 성격의 pod만 배치하는 것을 taint 라고 하고 toleration은 모든 모드는 전부 배치
Cluster를 구성을 할 때 비용 상의 문제로 GPU가 있는 Node 와 GPU가 없는 Node가 있을 때 GPU를 사용해야 하는 Pod는 GPU가 있는 노드에게만 배포하는 것이 taint 

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl
쿠버네티스의 서비스 개념을 구현하기 위해 각 노드에서 실행됨
클러스터 내부에서 서비스 와 파드 간 트래픽을 적절히 분배
쿠버네티스 서비스가 가상 IP(Cluster IP)를 사용해도 실제 파드로 트래픽이 전달되도록 합니다.
iptables, ipvs, userspace 모드를 활용해서 트래픽을 제어
kubectl get pods -n kube-system 명령으로 확인 가능
kubernetes Cluster를 운영하다 보면 특정  Worker Node에는 특정 성격의 Pod만 배포하고 싶을 때가 있는데 특정 성격의 pod만 배치하는 것을 taint 라고 하고 toleration은 모든 모드는 전부 배치
Cluster를 구성을 할 때 비용 상의 문제로 GPU가 있는 Node 와 GPU가 없는 Node가 있을 때 GPU를 사용해야 하는 Pod는 GPU가 있는 노드에게만 배포하는 것이 taint 

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl
쿠버네티스의 서비스 개념을 구현하기 위해 각 노드에서 실행됨
클러스터 내부에서 서비스 와 파드 간 트래픽을 적절히 분배
쿠버네티스 서비스가 가상 IP(Cluster IP)를 사용해도 실제 파드로 트래픽이 전달되도록 합니다.
iptables, ipvs, userspace 모드를 활용해서 트래픽을 제어
kubectl get pods -n kube-system 명령으로 확인 가능
kubernetes Cluster를 운영하다 보면 특정  Worker Node에는 특정 성격의 Pod만 배포하고 싶을 때가 있는데 특정 성격의 pod만 배치하는 것을 taint 라고 하고 toleration은 모든 모드는 전부 배치
Cluster를 구성을 할 때 비용 상의 문제로 GPU가 있는 Node 와 GPU가 없는 Node가 있을 때 GPU를 사용해야 하는 Pod는 GPU가 있는 노드에게만 배포하는 것이 taint 

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl
쿠버네티스의 서비스 개념을 구현하기 위해 각 노드에서 실행됨
클러스터 내부에서 서비스 와 파드 간 트래픽을 적절히 분배
쿠버네티스 서비스가 가상 IP(Cluster IP)를 사용해도 실제 파드로 트래픽이 전달되도록 합니다.
iptables, ipvs, userspace 모드를 활용해서 트래픽을 제어
kubectl get pods -n kube-system 명령으로 확인 가능
kubernetes Cluster를 운영하다 보면 특정  Worker Node에는 특정 성격의 Pod만 배포하고 싶을 때가 있는데 특정 성격의 pod만 배치하는 것을 taint 라고 하고 toleration은 모든 모드는 전부 배치
Cluster를 구성을 할 때 비용 상의 문제로 GPU가 있는 Node 와 GPU가 없는 Node가 있을 때 GPU를 사용해야 하는 Pod는 GPU가 있는 노드에게만 배포하는 것이 taint 

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl
쿠버네티스의 서비스 개념을 구현하기 위해 각 노드에서 실행됨
클러스터 내부에서 서비스 와 파드 간 트래픽을 적절히 분배
쿠버네티스 서비스가 가상 IP(Cluster IP)를 사용해도 실제 파드로 트래픽이 전달되도록 합니다.
iptables, ipvs, userspace 모드를 활용해서 트래픽을 제어
kubectl get pods -n kube-system 명령으로 확인 가능
kubernetes Cluster를 운영하다 보면 특정  Worker Node에는 특정 성격의 Pod만 배포하고 싶을 때가 있는데 특정 성격의 pod만 배치하는 것을 taint 라고 하고 toleration은 모든 모드는 전부 배치
Cluster를 구성을 할 때 비용 상의 문제로 GPU가 있는 Node 와 GPU가 없는 Node가 있을 때 GPU를 사용해야 하는 Pod는 GPU가 있는 노드에게만 배포하는 것이 taint 

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl
쿠버네티스의 서비스 개념을 구현하기 위해 각 노드에서 실행됨
클러스터 내부에서 서비스 와 파드 간 트래픽을 적절히 분배
쿠버네티스 서비스가 가상 IP(Cluster IP)를 사용해도 실제 파드로 트래픽이 전달되도록 합니다.
iptables, ipvs, userspace 모드를 활용해서 트래픽을 제어
kubectl get pods -n kube-system 명령으로 확인 가능
kubernetes Cluster를 운영하다 보면 특정  Worker Node에는 특정 성격의 Pod만 배포하고 싶을 때가 있는데 특정 성격의 pod만 배치하는 것을 taint 라고 하고 toleration은 모든 모드는 전부 배치
Cluster를 구성을 할 때 비용 상의 문제로 GPU가 있는 Node 와 GPU가 없는 Node가 있을 때 GPU를 사용해야 하는 Pod는 GPU가 있는 노드에게만 배포하는 것이 taint 

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl
쿠버네티스의 서비스 개념을 구현하기 위해 각 노드에서 실행됨
클러스터 내부에서 서비스 와 파드 간 트래픽을 적절히 분배
쿠버네티스 서비스가 가상 IP(Cluster IP)를 사용해도 실제 파드로 트래픽이 전달되도록 합니다.
iptables, ipvs, userspace 모드를 활용해서 트래픽을 제어
kubectl get pods -n kube-system 명령으로 확인 가능
kubernetes Cluster를 운영하다 보면 특정  Worker Node에는 특정 성격의 Pod만 배포하고 싶을 때가 있는데 특정 성격의 pod만 배치하는 것을 taint 라고 하고 toleration은 모든 모드는 전부 배치
Cluster를 구성을 할 때 비용 상의 문제로 GPU가 있는 Node 와 GPU가 없는 Node가 있을 때 GPU를 사용해야 하는 Pod는 GPU가 있는 노드에게만 배포하는 것이 taint 

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl
쿠버네티스의 서비스 개념을 구현하기 위해 각 노드에서 실행됨
클러스터 내부에서 서비스 와 파드 간 트래픽을 적절히 분배
쿠버네티스 서비스가 가상 IP(Cluster IP)를 사용해도 실제 파드로 트래픽이 전달되도록 합니다.
iptables, ipvs, userspace 모드를 활용해서 트래픽을 제어
kubectl get pods -n kube-system 명령으로 확인 가능
kubernetes Cluster를 운영하다 보면 특정  Worker Node에는 특정 성격의 Pod만 배포하고 싶을 때가 있는데 특정 성격의 pod만 배치하는 것을 taint 라고 하고 toleration은 모든 모드는 전부 배치
Cluster를 구성을 할 때 비용 상의 문제로 GPU가 있는 Node 와 GPU가 없는 Node가 있을 때 GPU를 사용해야 하는 Pod는 GPU가 있는 노드에게만 배포하는 것이 taint 

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl
쿠버네티스의 서비스 개념을 구현하기 위해 각 노드에서 실행됨
클러스터 내부에서 서비스 와 파드 간 트래픽을 적절히 분배
쿠버네티스 서비스가 가상 IP(Cluster IP)를 사용해도 실제 파드로 트래픽이 전달되도록 합니다.
iptables, ipvs, userspace 모드를 활용해서 트래픽을 제어
kubectl get pods -n kube-system 명령으로 확인 가능
kubernetes Cluster를 운영하다 보면 특정  Worker Node에는 특정 성격의 Pod만 배포하고 싶을 때가 있는데 특정 성격의 pod만 배치하는 것을 taint 라고 하고 toleration은 모든 모드는 전부 배치
Cluster를 구성을 할 때 비용 상의 문제로 GPU가 있는 Node 와 GPU가 없는 Node가 있을 때 GPU를 사용해야 하는 Pod는 GPU가 있는 노드에게만 배포하는 것이 taint 

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl
쿠버네티스의 서비스 개념을 구현하기 위해 각 노드에서 실행됨
클러스터 내부에서 서비스 와 파드 간 트래픽을 적절히 분배
쿠버네티스 서비스가 가상 IP(Cluster IP)를 사용해도 실제 파드로 트래픽이 전달되도록 합니다.
iptables, ipvs, userspace 모드를 활용해서 트래픽을 제어
kubectl get pods -n kube-system 명령으로 확인 가능
kubernetes Cluster를 운영하다 보면 특정  Worker Node에는 특정 성격의 Pod만 배포하고 싶을 때가 있는데 특정 성격의 pod만 배치하는 것을 taint 라고 하고 toleration은 모든 모드는 전부 배치
Cluster를 구성을 할 때 비용 상의 문제로 GPU가 있는 Node 와 GPU가 없는 Node가 있을 때 GPU를 사용해야 하는 Pod는 GPU가 있는 노드에게만 배포하는 것이 taint 

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl
쿠버네티스의 서비스 개념을 구현하기 위해 각 노드에서 실행됨
클러스터 내부에서 서비스 와 파드 간 트래픽을 적절히 분배
쿠버네티스 서비스가 가상 IP(Cluster IP)를 사용해도 실제 파드로 트래픽이 전달되도록 합니다.
iptables, ipvs, userspace 모드를 활용해서 트래픽을 제어
kubectl get pods -n kube-system 명령으로 확인 가능
kubernetes Cluster를 운영하다 보면 특정  Worker Node에는 특정 성격의 Pod만 배포하고 싶을 때가 있는데 특정 성격의 pod만 배치하는 것을 taint 라고 하고 toleration은 모든 모드는 전부 배치
Cluster를 구성을 할 때 비용 상의 문제로 GPU가 있는 Node 와 GPU가 없는 Node가 있을 때 GPU를 사용해야 하는 Pod는 GPU가 있는 노드에게만 배포하는 것이 taint 

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl
쿠버네티스의 서비스 개념을 구현하기 위해 각 노드에서 실행됨
클러스터 내부에서 서비스 와 파드 간 트래픽을 적절히 분배
쿠버네티스 서비스가 가상 IP(Cluster IP)를 사용해도 실제 파드로 트래픽이 전달되도록 합니다.
iptables, ipvs, userspace 모드를 활용해서 트래픽을 제어
kubectl get pods -n kube-system 명령으로 확인 가능
kubernetes Cluster를 운영하다 보면 특정  Worker Node에는 특정 성격의 Pod만 배포하고 싶을 때가 있는데 특정 성격의 pod만 배치하는 것을 taint 라고 하고 toleration은 모든 모드는 전부 배치
Cluster를 구성을 할 때 비용 상의 문제로 GPU가 있는 Node 와 GPU가 없는 Node가 있을 때 GPU를 사용해야 하는 Pod는 GPU가 있는 노드에게만 배포하는 것이 taint 

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl
쿠버네티스의 서비스 개념을 구현하기 위해 각 노드에서 실행됨
클러스터 내부에서 서비스 와 파드 간 트래픽을 적절히 분배
쿠버네티스 서비스가 가상 IP(Cluster IP)를 사용해도 실제 파드로 트래픽이 전달되도록 합니다.
iptables, ipvs, userspace 모드를 활용해서 트래픽을 제어
kubectl get pods -n kube-system 명령으로 확인 가능
kubernetes Cluster를 운영하다 보면 특정  Worker Node에는 특정 성격의 Pod만 배포하고 싶을 때가 있는데 특정 성격의 pod만 배치하는 것을 taint 라고 하고 toleration은 모든 모드는 전부 배치
Cluster를 구성을 할 때 비용 상의 문제로 GPU가 있는 Node 와 GPU가 없는 Node가 있을 때 GPU를 사용해야 하는 Pod는 GPU가 있는 노드에게만 배포하는 것이 taint 

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl
쿠버네티스의 서비스 개념을 구현하기 위해 각 노드에서 실행됨
클러스터 내부에서 서비스 와 파드 간 트래픽을 적절히 분배
쿠버네티스 서비스가 가상 IP(Cluster IP)를 사용해도 실제 파드로 트래픽이 전달되도록 합니다.
iptables, ipvs, userspace 모드를 활용해서 트래픽을 제어
kubectl get pods -n kube-system 명령으로 확인 가능
kubernetes Cluster를 운영하다 보면 특정  Worker Node에는 특정 성격의 Pod만 배포하고 싶을 때가 있는데 특정 성격의 pod만 배치하는 것을 taint 라고 하고 toleration은 모든 모드는 전부 배치
Cluster를 구성을 할 때 비용 상의 문제로 GPU가 있는 Node 와 GPU가 없는 Node가 있을 때 GPU를 사용해야 하는 Pod는 GPU가 있는 노드에게만 배포하는 것이 taint 

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl
쿠버네티스의 서비스 개념을 구현하기 위해 각 노드에서 실행됨
클러스터 내부에서 서비스 와 파드 간 트래픽을 적절히 분배
쿠버네티스 서비스가 가상 IP(Cluster IP)를 사용해도 실제 파드로 트래픽이 전달되도록 합니다.
iptables, ipvs, userspace 모드를 활용해서 트래픽을 제어
kubectl get pods -n kube-system 명령으로 확인 가능
kubernetes Cluster를 운영하다 보면 특정  Worker Node에는 특정 성격의 Pod만 배포하고 싶을 때가 있는데 특정 성격의 pod만 배치하는 것을 taint 라고 하고 toleration은 모든 모드는 전부 배치
Cluster를 구성을 할 때 비용 상의 문제로 GPU가 있는 Node 와 GPU가 없는 Node가 있을 때 GPU를 사용해야 하는 Pod는 GPU가 있는 노드에게만 배포하는 것이 taint 

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl
쿠버네티스의 서비스 개념을 구현하기 위해 각 노드에서 실행됨
클러스터 내부에서 서비스 와 파드 간 트래픽을 적절히 분배
쿠버네티스 서비스가 가상 IP(Cluster IP)를 사용해도 실제 파드로 트래픽이 전달되도록 합니다.
iptables, ipvs, userspace 모드를 활용해서 트래픽을 제어
kubectl get pods -n kube-system 명령으로 확인 가능
kubernetes Cluster를 운영하다 보면 특정  Worker Node에는 특정 성격의 Pod만 배포하고 싶을 때가 있는데 특정 성격의 pod만 배치하는 것을 taint 라고 하고 toleration은 모든 모드는 전부 배치
Cluster를 구성을 할 때 비용 상의 문제로 GPU가 있는 Node 와 GPU가 없는 Node가 있을 때 GPU를 사용해야 하는 Pod는 GPU가 있는 노드에게만 배포하는 것이 taint 

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl
쿠버네티스의 서비스 개념을 구현하기 위해 각 노드에서 실행됨
클러스터 내부에서 서비스 와 파드 간 트래픽을 적절히 분배
쿠버네티스 서비스가 가상 IP(Cluster IP)를 사용해도 실제 파드로 트래픽이 전달되도록 합니다.
iptables, ipvs, userspace 모드를 활용해서 트래픽을 제어
kubectl get pods -n kube-system 명령으로 확인 가능
kubernetes Cluster를 운영하다 보면 특정  Worker Node에는 특정 성격의 Pod만 배포하고 싶을 때가 있는데 특정 성격의 pod만 배치하는 것을 taint 라고 하고 toleration은 모든 모드는 전부 배치
Cluster를 구성을 할 때 비용 상의 문제로 GPU가 있는 Node 와 GPU가 없는 Node가 있을 때 GPU를 사용해야 하는 Pod는 GPU가 있는 노드에게만 배포하는 것이 taint 

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl
쿠버네티스의 서비스 개념을 구현하기 위해 각 노드에서 실행됨
클러스터 내부에서 서비스 와 파드 간 트래픽을 적절히 분배
쿠버네티스 서비스가 가상 IP(Cluster IP)를 사용해도 실제 파드로 트래픽이 전달되도록 합니다.
iptables, ipvs, userspace 모드를 활용해서 트래픽을 제어
kubectl get pods -n kube-system 명령으로 확인 가능
kubernetes Cluster를 운영하다 보면 특정  Worker Node에는 특정 성격의 Pod만 배포하고 싶을 때가 있는데 특정 성격의 pod만 배치하는 것을 taint 라고 하고 toleration은 모든 모드는 전부 배치
Cluster를 구성을 할 때 비용 상의 문제로 GPU가 있는 Node 와 GPU가 없는 Node가 있을 때 GPU를 사용해야 하는 Pod는 GPU가 있는 노드에게만 배포하는 것이 taint 

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl
쿠버네티스의 서비스 개념을 구현하기 위해 각 노드에서 실행됨
클러스터 내부에서 서비스 와 파드 간 트래픽을 적절히 분배
쿠버네티스 서비스가 가상 IP(Cluster IP)를 사용해도 실제 파드로 트래픽이 전달되도록 합니다.
iptables, ipvs, userspace 모드를 활용해서 트래픽을 제어
kubectl get pods -n kube-system 명령으로 확인 가능
kubernetes Cluster를 운영하다 보면 특정  Worker Node에는 특정 성격의 Pod만 배포하고 싶을 때가 있는데 특정 성격의 pod만 배치하는 것을 taint 라고 하고 toleration은 모든 모드는 전부 배치
Cluster를 구성을 할 때 비용 상의 문제로 GPU가 있는 Node 와 GPU가 없는 Node가 있을 때 GPU를 사용해야 하는 Pod는 GPU가 있는 노드에게만 배포하는 것이 taint 

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl
쿠버네티스의 서비스 개념을 구현하기 위해 각 노드에서 실행됨
클러스터 내부에서 서비스 와 파드 간 트래픽을 적절히 분배
쿠버네티스 서비스가 가상 IP(Cluster IP)를 사용해도 실제 파드로 트래픽이 전달되도록 합니다.
iptables, ipvs, userspace 모드를 활용해서 트래픽을 제어
kubectl get pods -n kube-system 명령으로 확인 가능
kubernetes Cluster를 운영하다 보면 특정  Worker Node에는 특정 성격의 Pod만 배포하고 싶을 때가 있는데 특정 성격의 pod만 배치하는 것을 taint 라고 하고 toleration은 모든 모드는 전부 배치
Cluster를 구성을 할 때 비용 상의 문제로 GPU가 있는 Node 와 GPU가 없는 Node가 있을 때 GPU를 사용해야 하는 Pod는 GPU가 있는 노드에게만 배포하는 것이 taint 

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl
쿠버네티스의 서비스 개념을 구현하기 위해 각 노드에서 실행됨
클러스터 내부에서 서비스 와 파드 간 트래픽을 적절히 분배
쿠버네티스 서비스가 가상 IP(Cluster IP)를 사용해도 실제 파드로 트래픽이 전달되도록 합니다.
iptables, ipvs, userspace 모드를 활용해서 트래픽을 제어
kubectl get pods -n kube-system 명령으로 확인 가능
kubernetes Cluster를 운영하다 보면 특정  Worker Node에는 특정 성격의 Pod만 배포하고 싶을 때가 있는데 특정 성격의 pod만 배치하는 것을 taint 라고 하고 toleration은 모든 모드는 전부 배치
Cluster를 구성을 할 때 비용 상의 문제로 GPU가 있는 Node 와 GPU가 없는 Node가 있을 때 GPU를 사용해야 하는 Pod는 GPU가 있는 노드에게만 배포하는 것이 taint 

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl
쿠버네티스의 서비스 개념을 구현하기 위해 각 노드에서 실행됨
클러스터 내부에서 서비스 와 파드 간 트래픽을 적절히 분배
쿠버네티스 서비스가 가상 IP(Cluster IP)를 사용해도 실제 파드로 트래픽이 전달되도록 합니다.
iptables, ipvs, userspace 모드를 활용해서 트래픽을 제어
kubectl get pods -n kube-system 명령으로 확인 가능
kubernetes Cluster를 운영하다 보면 특정  Worker Node에는 특정 성격의 Pod만 배포하고 싶을 때가 있는데 특정 성격의 pod만 배치하는 것을 taint 라고 하고 toleration은 모든 모드는 전부 배치
Cluster를 구성을 할 때 비용 상의 문제로 GPU가 있는 Node 와 GPU가 없는 Node가 있을 때 GPU를 사용해야 하는 Pod는 GPU가 있는 노드에게만 배포하는 것이 taint 

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl
쿠버네티스의 서비스 개념을 구현하기 위해 각 노드에서 실행됨
클러스터 내부에서 서비스 와 파드 간 트래픽을 적절히 분배
쿠버네티스 서비스가 가상 IP(Cluster IP)를 사용해도 실제 파드로 트래픽이 전달되도록 합니다.
iptables, ipvs, userspace 모드를 활용해서 트래픽을 제어
kubectl get pods -n kube-system 명령으로 확인 가능
kubernetes Cluster를 운영하다 보면 특정  Worker Node에는 특정 성격의 Pod만 배포하고 싶을 때가 있는데 특정 성격의 pod만 배치하는 것을 taint 라고 하고 toleration은 모든 모드는 전부 배치
Cluster를 구성을 할 때 비용 상의 문제로 GPU가 있는 Node 와 GPU가 없는 Node가 있을 때 GPU를 사용해야 하는 Pod는 GPU가 있는 노드에게만 배포하는 것이 taint 

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl
쿠버네티스의 서비스 개념을 구현하기 위해 각 노드에서 실행됨
클러스터 내부에서 서비스 와 파드 간 트래픽을 적절히 분배
쿠버네티스 서비스가 가상 IP(Cluster IP)를 사용해도 실제 파드로 트래픽이 전달되도록 합니다.
iptables, ipvs, userspace 모드를 활용해서 트래픽을 제어
kubectl get pods -n kube-system 명령으로 확인 가능
kubernetes Cluster를 운영하다 보면 특정  Worker Node에는 특정 성격의 Pod만 배포하고 싶을 때가 있는데 특정 성격의 pod만 배치하는 것을 taint 라고 하고 toleration은 모든 모드는 전부 배치
Cluster를 구성을 할 때 비용 상의 문제로 GPU가 있는 Node 와 GPU가 없는 Node가 있을 때 GPU를 사용해야 하는 Pod는 GPU가 있는 노드에게만 배포하는 것이 taint 

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl
쿠버네티스의 서비스 개념을 구현하기 위해 각 노드에서 실행됨
클러스터 내부에서 서비스 와 파드 간 트래픽을 적절히 분배
쿠버네티스 서비스가 가상 IP(Cluster IP)를 사용해도 실제 파드로 트래픽이 전달되도록 합니다.
iptables, ipvs, userspace 모드를 활용해서 트래픽을 제어
kubectl get pods -n kube-system 명령으로 확인 가능
kubernetes Cluster를 운영하다 보면 특정  Worker Node에는 특정 성격의 Pod만 배포하고 싶을 때가 있는데 특정 성격의 pod만 배치하는 것을 taint 라고 하고 toleration은 모든 모드는 전부 배치
Cluster를 구성을 할 때 비용 상의 문제로 GPU가 있는 Node 와 GPU가 없는 Node가 있을 때 GPU를 사용해야 하는 Pod는 GPU가 있는 노드에게만 배포하는 것이 taint 

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl
쿠버네티스의 서비스 개념을 구현하기 위해 각 노드에서 실행됨
클러스터 내부에서 서비스 와 파드 간 트래픽을 적절히 분배
쿠버네티스 서비스가 가상 IP(Cluster IP)를 사용해도 실제 파드로 트래픽이 전달되도록 합니다.
iptables, ipvs, userspace 모드를 활용해서 트래픽을 제어
kubectl get pods -n kube-system 명령으로 확인 가능
kubernetes Cluster를 운영하다 보면 특정  Worker Node에는 특정 성격의 Pod만 배포하고 싶을 때가 있는데 특정 성격의 pod만 배치하는 것을 taint 라고 하고 toleration은 모든 모드는 전부 배치
Cluster를 구성을 할 때 비용 상의 문제로 GPU가 있는 Node 와 GPU가 없는 Node가 있을 때 GPU를 사용해야 하는 Pod는 GPU가 있는 노드에게만 배포하는 것이 taint 

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl
쿠버네티스의 서비스 개념을 구현하기 위해 각 노드에서 실행됨
클러스터 내부에서 서비스 와 파드 간 트래픽을 적절히 분배
쿠버네티스 서비스가 가상 IP(Cluster IP)를 사용해도 실제 파드로 트래픽이 전달되도록 합니다.
iptables, ipvs, userspace 모드를 활용해서 트래픽을 제어
kubectl get pods -n kube-system 명령으로 확인 가능
kubernetes Cluster를 운영하다 보면 특정  Worker Node에는 특정 성격의 Pod만 배포하고 싶을 때가 있는데 특정 성격의 pod만 배치하는 것을 taint 라고 하고 toleration은 모든 모드는 전부 배치
Cluster를 구성을 할 때 비용 상의 문제로 GPU가 있는 Node 와 GPU가 없는 Node가 있을 때 GPU를 사용해야 하는 Pod는 GPU가 있는 노드에게만 배포하는 것이 taint 

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl
쿠버네티스의 서비스 개념을 구현하기 위해 각 노드에서 실행됨
클러스터 내부에서 서비스 와 파드 간 트래픽을 적절히 분배
쿠버네티스 서비스가 가상 IP(Cluster IP)를 사용해도 실제 파드로 트래픽이 전달되도록 합니다.
iptables, ipvs, userspace 모드를 활용해서 트래픽을 제어
kubectl get pods -n kube-system 명령으로 확인 가능
kubernetes Cluster를 운영하다 보면 특정  Worker Node에는 특정 성격의 Pod만 배포하고 싶을 때가 있는데 특정 성격의 pod만 배치하는 것을 taint 라고 하고 toleration은 모든 모드는 전부 배치
Cluster를 구성을 할 때 비용 상의 문제로 GPU가 있는 Node 와 GPU가 없는 Node가 있을 때 GPU를 사용해야 하는 Pod는 GPU가 있는 노드에게만 배포하는 것이 taint 

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl
쿠버네티스의 서비스 개념을 구현하기 위해 각 노드에서 실행됨
클러스터 내부에서 서비스 와 파드 간 트래픽을 적절히 분배
쿠버네티스 서비스가 가상 IP(Cluster IP)를 사용해도 실제 파드로 트래픽이 전달되도록 합니다.
iptables, ipvs, userspace 모드를 활용해서 트래픽을 제어
kubectl get pods -n kube-system 명령으로 확인 가능
kubernetes Cluster를 운영하다 보면 특정  Worker Node에는 특정 성격의 Pod만 배포하고 싶을 때가 있는데 특정 성격의 pod만 배치하는 것을 taint 라고 하고 toleration은 모든 모드는 전부 배치
Cluster를 구성을 할 때 비용 상의 문제로 GPU가 있는 Node 와 GPU가 없는 Node가 있을 때 GPU를 사용해야 하는 Pod는 GPU가 있는 노드에게만 배포하는 것이 taint 

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl
쿠버네티스의 서비스 개념을 구현하기 위해 각 노드에서 실행됨
클러스터 내부에서 서비스 와 파드 간 트래픽을 적절히 분배
쿠버네티스 서비스가 가상 IP(Cluster IP)를 사용해도 실제 파드로 트래픽이 전달되도록 합니다.
iptables, ipvs, userspace 모드를 활용해서 트래픽을 제어
kubectl get pods -n kube-system 명령으로 확인 가능
kubernetes Cluster를 운영하다 보면 특정  Worker Node에는 특정 성격의 Pod만 배포하고 싶을 때가 있는데 특정 성격의 pod만 배치하는 것을 taint 라고 하고 toleration은 모든 모드는 전부 배치
Cluster를 구성을 할 때 비용 상의 문제로 GPU가 있는 Node 와 GPU가 없는 Node가 있을 때 GPU를 사용해야 하는 Pod는 GPU가 있는 노드에게만 배포하는 것이 taint 

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl
쿠버네티스의 서비스 개념을 구현하기 위해 각 노드에서 실행됨
클러스터 내부에서 서비스 와 파드 간 트래픽을 적절히 분배
쿠버네티스 서비스가 가상 IP(Cluster IP)를 사용해도 실제 파드로 트래픽이 전달되도록 합니다.
iptables, ipvs, userspace 모드를 활용해서 트래픽을 제어
kubectl get pods -n kube-system 명령으로 확인 가능
kubernetes Cluster를 운영하다 보면 특정  Worker Node에는 특정 성격의 Pod만 배포하고 싶을 때가 있는데 특정 성격의 pod만 배치하는 것을 taint 라고 하고 toleration은 모든 모드는 전부 배치
Cluster를 구성을 할 때 비용 상의 문제로 GPU가 있는 Node 와 GPU가 없는 Node가 있을 때 GPU를 사용해야 하는 Pod는 GPU가 있는 노드에게만 배포하는 것이 taint 

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl
쿠버네티스의 서비스 개념을 구현하기 위해 각 노드에서 실행됨
클러스터 내부에서 서비스 와 파드 간 트래픽을 적절히 분배
쿠버네티스 서비스가 가상 IP(Cluster IP)를 사용해도 실제 파드로 트래픽이 전달되도록 합니다.
iptables, ipvs, userspace 모드를 활용해서 트래픽을 제어
kubectl get pods -n kube-system 명령으로 확인 가능
kubernetes Cluster를 운영하다 보면 특정  Worker Node에는 특정 성격의 Pod만 배포하고 싶을 때가 있는데 특정 성격의 pod만 배치하는 것을 taint 라고 하고 toleration은 모든 모드는 전부 배치
Cluster를 구성을 할 때 비용 상의 문제로 GPU가 있는 Node 와 GPU가 없는 Node가 있을 때 GPU를 사용해야 하는 Pod는 GPU가 있는 노드에게만 배포하는 것이 taint 

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl
쿠버네티스의 서비스 개념을 구현하기 위해 각 노드에서 실행됨
클러스터 내부에서 서비스 와 파드 간 트래픽을 적절히 분배
쿠버네티스 서비스가 가상 IP(Cluster IP)를 사용해도 실제 파드로 트래픽이 전달되도록 합니다.
iptables, ipvs, userspace 모드를 활용해서 트래픽을 제어
kubectl get pods -n kube-system 명령으로 확인 가능
kubernetes Cluster를 운영하다 보면 특정  Worker Node에는 특정 성격의 Pod만 배포하고 싶을 때가 있는데 특정 성격의 pod만 배치하는 것을 taint 라고 하고 toleration은 모든 모드는 전부 배치
Cluster를 구성을 할 때 비용 상의 문제로 GPU가 있는 Node 와 GPU가 없는 Node가 있을 때 GPU를 사용해야 하는 Pod는 GPU가 있는 노드에게만 배포하는 것이 taint 

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl
쿠버네티스의 서비스 개념을 구현하기 위해 각 노드에서 실행됨
클러스터 내부에서 서비스 와 파드 간 트래픽을 적절히 분배
쿠버네티스 서비스가 가상 IP(Cluster IP)를 사용해도 실제 파드로 트래픽이 전달되도록 합니다.
iptables, ipvs, userspace 모드를 활용해서 트래픽을 제어
kubectl get pods -n kube-system 명령으로 확인 가능
kubernetes Cluster를 운영하다 보면 특정  Worker Node에는 특정 성격의 Pod만 배포하고 싶을 때가 있는데 특정 성격의 pod만 배치하는 것을 taint 라고 하고 toleration은 모든 모드는 전부 배치
Cluster를 구성을 할 때 비용 상의 문제로 GPU가 있는 Node 와 GPU가 없는 Node가 있을 때 GPU를 사용해야 하는 Pod는 GPU가 있는 노드에게만 배포하는 것이 taint 

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl
쿠버네티스의 서비스 개념을 구현하기 위해 각 노드에서 실행됨
클러스터 내부에서 서비스 와 파드 간 트래픽을 적절히 분배
쿠버네티스 서비스가 가상 IP(Cluster IP)를 사용해도 실제 파드로 트래픽이 전달되도록 합니다.
iptables, ipvs, userspace 모드를 활용해서 트래픽을 제어
kubectl get pods -n kube-system 명령으로 확인 가능
kubernetes Cluster를 운영하다 보면 특정  Worker Node에는 특정 성격의 Pod만 배포하고 싶을 때가 있는데 특정 성격의 pod만 배치하는 것을 taint 라고 하고 toleration은 모든 모드는 전부 배치
Cluster를 구성을 할 때 비용 상의 문제로 GPU가 있는 Node 와 GPU가 없는 Node가 있을 때 GPU를 사용해야 하는 Pod는 GPU가 있는 노드에게만 배포하는 것이 taint 

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl
쿠버네티스의 서비스 개념을 구현하기 위해 각 노드에서 실행됨
클러스터 내부에서 서비스 와 파드 간 트래픽을 적절히 분배
쿠버네티스 서비스가 가상 IP(Cluster IP)를 사용해도 실제 파드로 트래픽이 전달되도록 합니다.
iptables, ipvs, userspace 모드를 활용해서 트래픽을 제어
kubectl get pods -n kube-system 명령으로 확인 가능
kubernetes Cluster를 운영하다 보면 특정  Worker Node에는 특정 성격의 Pod만 배포하고 싶을 때가 있는데 특정 성격의 pod만 배치하는 것을 taint 라고 하고 toleration은 모든 모드는 전부 배치
Cluster를 구성을 할 때 비용 상의 문제로 GPU가 있는 Node 와 GPU가 없는 Node가 있을 때 GPU를 사용해야 하는 Pod는 GPU가 있는 노드에게만 배포하는 것이 taint 

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl
쿠버네티스의 서비스 개념을 구현하기 위해 각 노드에서 실행됨
클러스터 내부에서 서비스 와 파드 간 트래픽을 적절히 분배
쿠버네티스 서비스가 가상 IP(Cluster IP)를 사용해도 실제 파드로 트래픽이 전달되도록 합니다.
iptables, ipvs, userspace 모드를 활용해서 트래픽을 제어
kubectl get pods -n kube-system 명령으로 확인 가능
kubernetes Cluster를 운영하다 보면 특정  Worker Node에는 특정 성격의 Pod만 배포하고 싶을 때가 있는데 특정 성격의 pod만 배치하는 것을 taint 라고 하고 toleration은 모든 모드는 전부 배치
Cluster를 구성을 할 때 비용 상의 문제로 GPU가 있는 Node 와 GPU가 없는 Node가 있을 때 GPU를 사용해야 하는 Pod는 GPU가 있는 노드에게만 배포하는 것이 taint 

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl
쿠버네티스의 서비스 개념을 구현하기 위해 각 노드에서 실행됨
클러스터 내부에서 서비스 와 파드 간 트래픽을 적절히 분배
쿠버네티스 서비스가 가상 IP(Cluster IP)를 사용해도 실제 파드로 트래픽이 전달되도록 합니다.
iptables, ipvs, userspace 모드를 활용해서 트래픽을 제어
kubectl get pods -n kube-system 명령으로 확인 가능
kubernetes Cluster를 운영하다 보면 특정  Worker Node에는 특정 성격의 Pod만 배포하고 싶을 때가 있는데 특정 성격의 pod만 배치하는 것을 taint 라고 하고 toleration은 모든 모드는 전부 배치
Cluster를 구성을 할 때 비용 상의 문제로 GPU가 있는 Node 와 GPU가 없는 Node가 있을 때 GPU를 사용해야 하는 Pod는 GPU가 있는 노드에게만 배포하는 것이 taint 

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl
쿠버네티스의 서비스 개념을 구현하기 위해 각 노드에서 실행됨
클러스터 내부에서 서비스 와 파드 간 트래픽을 적절히 분배
쿠버네티스 서비스가 가상 IP(Cluster IP)를 사용해도 실제 파드로 트래픽이 전달되도록 합니다.
iptables, ipvs, userspace 모드를 활용해서 트래픽을 제어
kubectl get pods -n kube-system 명령으로 확인 가능
kubernetes Cluster를 운영하다 보면 특정  Worker Node에는 특정 성격의 Pod만 배포하고 싶을 때가 있는데 특정 성격의 pod만 배치하는 것을 taint 라고 하고 toleration은 모든 모드는 전부 배치
Cluster를 구성을 할 때 비용 상의 문제로 GPU가 있는 Node 와 GPU가 없는 Node가 있을 때 GPU를 사용해야 하는 Pod는 GPU가 있는 노드에게만 배포하는 것이 taint 

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl
쿠버네티스의 서비스 개념을 구현하기 위해 각 노드에서 실행됨
클러스터 내부에서 서비스 와 파드 간 트래픽을 적절히 분배
쿠버네티스 서비스가 가상 IP(Cluster IP)를 사용해도 실제 파드로 트래픽이 전달되도록 합니다.
iptables, ipvs, userspace 모드를 활용해서 트래픽을 제어
kubectl get pods -n kube-system 명령으로 확인 가능
kubernetes Cluster를 운영하다 보면 특정  Worker Node에는 특정 성격의 Pod만 배포하고 싶을 때가 있는데 특정 성격의 pod만 배치하는 것을 taint 라고 하고 toleration은 모든 모드는 전부 배치
Cluster를 구성을 할 때 비용 상의 문제로 GPU가 있는 Node 와 GPU가 없는 Node가 있을 때 GPU를 사용해야 하는 Pod는 GPU가 있는 노드에게만 배포하는 것이 taint 

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl
쿠버네티스의 서비스 개념을 구현하기 위해 각 노드에서 실행됨
클러스터 내부에서 서비스 와 파드 간 트래픽을 적절히 분배
쿠버네티스 서비스가 가상 IP(Cluster IP)를 사용해도 실제 파드로 트래픽이 전달되도록 합니다.
iptables, ipvs, userspace 모드를 활용해서 트래픽을 제어
kubectl get pods -n kube-system 명령으로 확인 가능
kubernetes Cluster를 운영하다 보면 특정  Worker Node에는 특정 성격의 Pod만 배포하고 싶을 때가 있는데 특정 성격의 pod만 배치하는 것을 taint 라고 하고 toleration은 모든 모드는 전부 배치
Cluster를 구성을 할 때 비용 상의 문제로 GPU가 있는 Node 와 GPU가 없는 Node가 있을 때 GPU를 사용해야 하는 Pod는 GPU가 있는 노드에게만 배포하는 것이 taint 

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl
쿠버네티스의 서비스 개념을 구현하기 위해 각 노드에서 실행됨
클러스터 내부에서 서비스 와 파드 간 트래픽을 적절히 분배
쿠버네티스 서비스가 가상 IP(Cluster IP)를 사용해도 실제 파드로 트래픽이 전달되도록 합니다.
iptables, ipvs, userspace 모드를 활용해서 트래픽을 제어
kubectl get pods -n kube-system 명령으로 확인 가능
kubernetes Cluster를 운영하다 보면 특정  Worker Node에는 특정 성격의 Pod만 배포하고 싶을 때가 있는데 특정 성격의 pod만 배치하는 것을 taint 라고 하고 toleration은 모든 모드는 전부 배치
Cluster를 구성을 할 때 비용 상의 문제로 GPU가 있는 Node 와 GPU가 없는 Node가 있을 때 GPU를 사용해야 하는 Pod는 GPU가 있는 노드에게만 배포하는 것이 taint 

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl
쿠버네티스의 서비스 개념을 구현하기 위해 각 노드에서 실행됨
클러스터 내부에서 서비스 와 파드 간 트래픽을 적절히 분배
쿠버네티스 서비스가 가상 IP(Cluster IP)를 사용해도 실제 파드로 트래픽이 전달되도록 합니다.
iptables, ipvs, userspace 모드를 활용해서 트래픽을 제어
kubectl get pods -n kube-system 명령으로 확인 가능
kubernetes Cluster를 운영하다 보면 특정  Worker Node에는 특정 성격의 Pod만 배포하고 싶을 때가 있는데 특정 성격의 pod만 배치하는 것을 taint 라고 하고 toleration은 모든 모드는 전부 배치
Cluster를 구성을 할 때 비용 상의 문제로 GPU가 있는 Node 와 GPU가 없는 Node가 있을 때 GPU를 사용해야 하는 Pod는 GPU가 있는 노드에게만 배포하는 것이 taint 

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl
쿠버네티스의 서비스 개념을 구현하기 위해 각 노드에서 실행됨
클러스터 내부에서 서비스 와 파드 간 트래픽을 적절히 분배
쿠버네티스 서비스가 가상 IP(Cluster IP)를 사용해도 실제 파드로 트래픽이 전달되도록 합니다.
iptables, ipvs, userspace 모드를 활용해서 트래픽을 제어
kubectl get pods -n kube-system 명령으로 확인 가능
kubernetes Cluster를 운영하다 보면 특정  Worker Node에는 특정 성격의 Pod만 배포하고 싶을 때가 있는데 특정 성격의 pod만 배치하는 것을 taint 라고 하고 toleration은 모든 모드는 전부 배치
Cluster를 구성을 할 때 비용 상의 문제로 GPU가 있는 Node 와 GPU가 없는 Node가 있을 때 GPU를 사용해야 하는 Pod는 GPU가 있는 노드에게만 배포하는 것이 taint 

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl
쿠버네티스의 서비스 개념을 구현하기 위해 각 노드에서 실행됨
클러스터 내부에서 서비스 와 파드 간 트래픽을 적절히 분배
쿠버네티스 서비스가 가상 IP(Cluster IP)를 사용해도 실제 파드로 트래픽이 전달되도록 합니다.
iptables, ipvs, userspace 모드를 활용해서 트래픽을 제어
kubectl get pods -n kube-system 명령으로 확인 가능
kubernetes Cluster를 운영하다 보면 특정  Worker Node에는 특정 성격의 Pod만 배포하고 싶을 때가 있는데 특정 성격의 pod만 배치하는 것을 taint 라고 하고 toleration은 모든 모드는 전부 배치
Cluster를 구성을 할 때 비용 상의 문제로 GPU가 있는 Node 와 GPU가 없는 Node가 있을 때 GPU를 사용해야 하는 Pod는 GPU가 있는 노드에게만 배포하는 것이 taint 

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl
쿠버네티스의 서비스 개념을 구현하기 위해 각 노드에서 실행됨
클러스터 내부에서 서비스 와 파드 간 트래픽을 적절히 분배
쿠버네티스 서비스가 가상 IP(Cluster IP)를 사용해도 실제 파드로 트래픽이 전달되도록 합니다.
iptables, ipvs, userspace 모드를 활용해서 트래픽을 제어
kubectl get pods -n kube-system 명령으로 확인 가능
kubernetes Cluster를 운영하다 보면 특정  Worker Node에는 특정 성격의 Pod만 배포하고 싶을 때가 있는데 특정 성격의 pod만 배치하는 것을 taint 라고 하고 toleration은 모든 모드는 전부 배치
Cluster를 구성을 할 때 비용 상의 문제로 GPU가 있는 Node 와 GPU가 없는 Node가 있을 때 GPU를 사용해야 하는 Pod는 GPU가 있는 노드에게만 배포하는 것이 taint 

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl
쿠버네티스의 서비스 개념을 구현하기 위해 각 노드에서 실행됨
클러스터 내부에서 서비스 와 파드 간 트래픽을 적절히 분배
쿠버네티스 서비스가 가상 IP(Cluster IP)를 사용해도 실제 파드로 트래픽이 전달되도록 합니다.
iptables, ipvs, userspace 모드를 활용해서 트래픽을 제어
kubectl get pods -n kube-system 명령으로 확인 가능
kubernetes Cluster를 운영하다 보면 특정  Worker Node에는 특정 성격의 Pod만 배포하고 싶을 때가 있는데 특정 성격의 pod만 배치하는 것을 taint 라고 하고 toleration은 모든 모드는 전부 배치
Cluster를 구성을 할 때 비용 상의 문제로 GPU가 있는 Node 와 GPU가 없는 Node가 있을 때 GPU를 사용해야 하는 Pod는 GPU가 있는 노드에게만 배포하는 것이 taint 

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl
쿠버네티스의 서비스 개념을 구현하기 위해 각 노드에서 실행됨
클러스터 내부에서 서비스 와 파드 간 트래픽을 적절히 분배
쿠버네티스 서비스가 가상 IP(Cluster IP)를 사용해도 실제 파드로 트래픽이 전달되도록 합니다.
iptables, ipvs, userspace 모드를 활용해서 트래픽을 제어
kubectl get pods -n kube-system 명령으로 확인 가능
kubernetes Cluster를 운영하다 보면 특정  Worker Node에는 특정 성격의 Pod만 배포하고 싶을 때가 있는데 특정 성격의 pod만 배치하는 것을 taint 라고 하고 toleration은 모든 모드는 전부 배치
Cluster를 구성을 할 때 비용 상의 문제로 GPU가 있는 Node 와 GPU가 없는 Node가 있을 때 GPU를 사용해야 하는 Pod는 GPU가 있는 노드에게만 배포하는 것이 taint 

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl
쿠버네티스의 서비스 개념을 구현하기 위해 각 노드에서 실행됨
클러스터 내부에서 서비스 와 파드 간 트래픽을 적절히 분배
쿠버네티스 서비스가 가상 IP(Cluster IP)를 사용해도 실제 파드로 트래픽이 전달되도록 합니다.
iptables, ipvs, userspace 모드를 활용해서 트래픽을 제어
kubectl get pods -n kube-system 명령으로 확인 가능
kubernetes Cluster를 운영하다 보면 특정  Worker Node에는 특정 성격의 Pod만 배포하고 싶을 때가 있는데 특정 성격의 pod만 배치하는 것을 taint 라고 하고 toleration은 모든 모드는 전부 배치
Cluster를 구성을 할 때 비용 상의 문제로 GPU가 있는 Node 와 GPU가 없는 Node가 있을 때 GPU를 사용해야 하는 Pod는 GPU가 있는 노드에게만 배포하는 것이 taint 

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl
쿠버네티스의 서비스 개념을 구현하기 위해 각 노드에서 실행됨
클러스터 내부에서 서비스 와 파드 간 트래픽을 적절히 분배
쿠버네티스 서비스가 가상 IP(Cluster IP)를 사용해도 실제 파드로 트래픽이 전달되도록 합니다.
iptables, ipvs, userspace 모드를 활용해서 트래픽을 제어
kubectl get pods -n kube-system 명령으로 확인 가능
kubernetes Cluster를 운영하다 보면 특정  Worker Node에는 특정 성격의 Pod만 배포하고 싶을 때가 있는데 특정 성격의 pod만 배치하는 것을 taint 라고 하고 toleration은 모든 모드는 전부 배치
Cluster를 구성을 할 때 비용 상의 문제로 GPU가 있는 Node 와 GPU가 없는 Node가 있을 때 GPU를 사용해야 하는 Pod는 GPU가 있는 노드에게만 배포하는 것이 taint 

- Docker 만을 이용해서 20개의 Container를 만들려면 docker run 명령을 20번 실행해야 하고 서로 다른 컴퓨터에서 작업을 수행해야 함
- docker-compose는 옵션을 이용해서 수동으로 Container의 수를 바꿀 수는 있지만 모니터링 기능이 없어서 Container를 만들 때 외에는 관여하지 않지만 이에 비해 kubernetes는 이 상태를 유지하는 기능이 있음
ReplicationController
API Server			kubelet
Scheduler				proxy
Controller Manager			container runtime
pod
container
etcd

Master Node를 설정하는 관리자의 컴퓨터에는 kubectl을 설치하는데 kubectl을 설치해야 Master Node에 로그인을 해서 초기 설정을 진행하거나 추후 조정 가능
kubectl 설치: https://kubernetes.io/ko/docs/tasks/tools/install-kubectl-linux/
Core가 2개 이상이어야 합니다.
포트 개방
API Server: 6443
etcd Server: 2379, 2380
kubelet API: 10250
kube scheduler: 10251
kube controller manager: 10252
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
하드웨어 제약은 없음
포트 개방
API Server: 6443
kube-proxy가 서비스를 Load Balancing 하기 위해서 사용하는 포트: 26443
Flannel CNI 플러그인: 8285, 8472(CNI 플러그 인에 따라 다르게 설정)
NodePort로 사용할 포트: 30000~32767
패키지 정보 업데이트: sudo apt update && sudo apt upgrade -y
인증서 패키지 설치: sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg
sudo swapoff -a
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
확인: sudo free -m 
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay
sudo modprobe br_netfilter
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
sudo sysctl --system
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

sudo systemctl restart containerd
sudo systemctl enable containerd
sudo curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update
sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl #패키지를 업그레드하더라도 3가지는 그대로 유지(쿠버네티스 버전 유지)
Master에서 수행하는 작업
클러스터 초기화
sudo kubeadm init --pod-network-cidr=10.244.0.0/16
#설치되는 CNI에 따라서 cidr 값은 변경됩니다.
#이 값은 flannel을 사용할 때 값입니다.
#결과로 kubeadm join 문구가 출력되는데 Worker에서는 이 구문을 수행해야 클러스터에 참여됩니다.
mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

CNI 설치
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
Worker 수행해야 하는 작업 - Master 작업이 끝나야 가능
sudo kubeadm join 10.0.2.101:6443 --token 58ds6a.jiiotw0j4j1ew9td \
--discovery-token-ca-cert-hash sha256:c160d8f52de3dbca2382cb7099ed137b11ff4eddaf0ad8501c6c5692e0774958

확인: master에서 kubectl get nodes 를 확인해서 노드들이 참여했는지 확인
#### kubectl
쿠버네티스의 서비스 개념을 구현하기 위해 각 노드에서 실행됨
클러스터 내부에서 서비스 와 파드 간 트래픽을 적절히 분배
쿠버네티스 서비스가 가상 IP(Cluster IP)를 사용해도 실제 파드로 트래픽이 전달되도록 합니다.
iptables, ipvs, userspace 모드를 활용해서 트래픽을 제어
kubectl get pods -n kube-system 명령으로 확인 가능
kubernetes Cluster를 운영하다 보면 특정  Worker Node에는 특정 성격의 Pod만 배포하고 싶을 때가 있는데 특정 성격의 pod만 배치하는 것을 taint 라고 하고 toleration은 모든 모드는 전부 배치
Cluster를 구성을 할 때 비용 상의 문제로 GPU가 있는 Node 와 GPU가 없는 Node가 있을 때 GPU를 사용해야 하는 Pod는 GPU가 있는 노드에게만 배포하는 것이 taint 
