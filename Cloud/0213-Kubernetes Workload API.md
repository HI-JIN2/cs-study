yaml
파드 <-> ReplicationControler

파드 <-> ReplicationControler

파드 <-> ReplicationControler

파드 <-> ReplicationControler

파드 <-> ReplicationControler

파드 <-> ReplicationControler

파드 <-> ReplicationControler

파드 <-> ReplicationControler

파드 <-> ReplicationControler

파드 <-> ReplicationControler

파드 <-> ReplicationControler

파드 <-> ReplicationControler

파드 <-> ReplicationControler

파드 <-> ReplicationControler

파드 <-> ReplicationControler

파드 <-> ReplicationControler

파드 <-> ReplicationControler

파드 <-> ReplicationControler

파드 <-> ReplicationControler

파드 <-> ReplicationControler

파드 <-> ReplicationControler

파드 <-> ReplicationControler

파드 <-> ReplicationControler

파드 <-> ReplicationControler

파드 <-> ReplicationControler

파드 <-> ReplicationControler

파드 <-> ReplicationControler

파드 <-> ReplicationControler

파드 <-> ReplicationControler

파드 <-> ReplicationControler

파드 <-> ReplicationControler

파드 <-> ReplicationControler

파드 <-> ReplicationControler

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml
임의의 시점에 파드를 업데이트하는 경우는 데못셋과 연결된 파드를 kubectl delete pod 명령으로 수동으로 정지시키고 자동화된 복구 기능으로 새로운 파드를 생성해야 함

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml
임의의 시점에 파드를 업데이트하는 경우는 데못셋과 연결된 파드를 kubectl delete pod 명령으로 수동으로 정지시키고 자동화된 복구 기능으로 새로운 파드를 생성해야 함

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml
임의의 시점에 파드를 업데이트하는 경우는 데못셋과 연결된 파드를 kubectl delete pod 명령으로 수동으로 정지시키고 자동화된 복구 기능으로 새로운 파드를 생성해야 함

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml
임의의 시점에 파드를 업데이트하는 경우는 데못셋과 연결된 파드를 kubectl delete pod 명령으로 수동으로 정지시키고 자동화된 복구 기능으로 새로운 파드를 생성해야 함

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml
임의의 시점에 파드를 업데이트하는 경우는 데못셋과 연결된 파드를 kubectl delete pod 명령으로 수동으로 정지시키고 자동화된 복구 기능으로 새로운 파드를 생성해야 함

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml
임의의 시점에 파드를 업데이트하는 경우는 데못셋과 연결된 파드를 kubectl delete pod 명령으로 수동으로 정지시키고 자동화된 복구 기능으로 새로운 파드를 생성해야 함

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml
임의의 시점에 파드를 업데이트하는 경우는 데못셋과 연결된 파드를 kubectl delete pod 명령으로 수동으로 정지시키고 자동화된 복구 기능으로 새로운 파드를 생성해야 함

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml
임의의 시점에 파드를 업데이트하는 경우는 데못셋과 연결된 파드를 kubectl delete pod 명령으로 수동으로 정지시키고 자동화된 복구 기능으로 새로운 파드를 생성해야 함

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml
임의의 시점에 파드를 업데이트하는 경우는 데못셋과 연결된 파드를 kubectl delete pod 명령으로 수동으로 정지시키고 자동화된 복구 기능으로 새로운 파드를 생성해야 함

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml
임의의 시점에 파드를 업데이트하는 경우는 데못셋과 연결된 파드를 kubectl delete pod 명령으로 수동으로 정지시키고 자동화된 복구 기능으로 새로운 파드를 생성해야 함

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml
임의의 시점에 파드를 업데이트하는 경우는 데못셋과 연결된 파드를 kubectl delete pod 명령으로 수동으로 정지시키고 자동화된 복구 기능으로 새로운 파드를 생성해야 함

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml
임의의 시점에 파드를 업데이트하는 경우는 데못셋과 연결된 파드를 kubectl delete pod 명령으로 수동으로 정지시키고 자동화된 복구 기능으로 새로운 파드를 생성해야 함

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml
임의의 시점에 파드를 업데이트하는 경우는 데못셋과 연결된 파드를 kubectl delete pod 명령으로 수동으로 정지시키고 자동화된 복구 기능으로 새로운 파드를 생성해야 함

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml
임의의 시점에 파드를 업데이트하는 경우는 데못셋과 연결된 파드를 kubectl delete pod 명령으로 수동으로 정지시키고 자동화된 복구 기능으로 새로운 파드를 생성해야 함

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml
임의의 시점에 파드를 업데이트하는 경우는 데못셋과 연결된 파드를 kubectl delete pod 명령으로 수동으로 정지시키고 자동화된 복구 기능으로 새로운 파드를 생성해야 함

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml
임의의 시점에 파드를 업데이트하는 경우는 데못셋과 연결된 파드를 kubectl delete pod 명령으로 수동으로 정지시키고 자동화된 복구 기능으로 새로운 파드를 생성해야 함

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml
임의의 시점에 파드를 업데이트하는 경우는 데못셋과 연결된 파드를 kubectl delete pod 명령으로 수동으로 정지시키고 자동화된 복구 기능으로 새로운 파드를 생성해야 함

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml
임의의 시점에 파드를 업데이트하는 경우는 데못셋과 연결된 파드를 kubectl delete pod 명령으로 수동으로 정지시키고 자동화된 복구 기능으로 새로운 파드를 생성해야 함

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml
임의의 시점에 파드를 업데이트하는 경우는 데못셋과 연결된 파드를 kubectl delete pod 명령으로 수동으로 정지시키고 자동화된 복구 기능으로 새로운 파드를 생성해야 함

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml
임의의 시점에 파드를 업데이트하는 경우는 데못셋과 연결된 파드를 kubectl delete pod 명령으로 수동으로 정지시키고 자동화된 복구 기능으로 새로운 파드를 생성해야 함

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml
임의의 시점에 파드를 업데이트하는 경우는 데못셋과 연결된 파드를 kubectl delete pod 명령으로 수동으로 정지시키고 자동화된 복구 기능으로 새로운 파드를 생성해야 함

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml
임의의 시점에 파드를 업데이트하는 경우는 데못셋과 연결된 파드를 kubectl delete pod 명령으로 수동으로 정지시키고 자동화된 복구 기능으로 새로운 파드를 생성해야 함

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml
임의의 시점에 파드를 업데이트하는 경우는 데못셋과 연결된 파드를 kubectl delete pod 명령으로 수동으로 정지시키고 자동화된 복구 기능으로 새로운 파드를 생성해야 함
- 리소스를 위한 매니페스트 작성: sample-statefulset.yaml
apiVersion: apps/v1

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml
임의의 시점에 파드를 업데이트하는 경우는 데못셋과 연결된 파드를 kubectl delete pod 명령으로 수동으로 정지시키고 자동화된 복구 기능으로 새로운 파드를 생성해야 함
- 리소스를 위한 매니페스트 작성: sample-statefulset.yaml
apiVersion: apps/v1

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml
임의의 시점에 파드를 업데이트하는 경우는 데못셋과 연결된 파드를 kubectl delete pod 명령으로 수동으로 정지시키고 자동화된 복구 기능으로 새로운 파드를 생성해야 함
- 리소스를 위한 매니페스트 작성: sample-statefulset.yaml
apiVersion: apps/v1
name: sample-statefulset

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml
임의의 시점에 파드를 업데이트하는 경우는 데못셋과 연결된 파드를 kubectl delete pod 명령으로 수동으로 정지시키고 자동화된 복구 기능으로 새로운 파드를 생성해야 함
- 리소스를 위한 매니페스트 작성: sample-statefulset.yaml
apiVersion: apps/v1
name: sample-statefulset
serviceName: sample-statefulset

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml
임의의 시점에 파드를 업데이트하는 경우는 데못셋과 연결된 파드를 kubectl delete pod 명령으로 수동으로 정지시키고 자동화된 복구 기능으로 새로운 파드를 생성해야 함
- 리소스를 위한 매니페스트 작성: sample-statefulset.yaml
apiVersion: apps/v1
name: sample-statefulset
serviceName: sample-statefulset

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml
임의의 시점에 파드를 업데이트하는 경우는 데못셋과 연결된 파드를 kubectl delete pod 명령으로 수동으로 정지시키고 자동화된 복구 기능으로 새로운 파드를 생성해야 함
- 리소스를 위한 매니페스트 작성: sample-statefulset.yaml
apiVersion: apps/v1
name: sample-statefulset
serviceName: sample-statefulset
matchLabels:
app: sample-app

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml
임의의 시점에 파드를 업데이트하는 경우는 데못셋과 연결된 파드를 kubectl delete pod 명령으로 수동으로 정지시키고 자동화된 복구 기능으로 새로운 파드를 생성해야 함
- 리소스를 위한 매니페스트 작성: sample-statefulset.yaml
apiVersion: apps/v1
name: sample-statefulset
serviceName: sample-statefulset
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
volumeMounts:
- name: www
mountPath: /usr/share/nginx/html

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml
임의의 시점에 파드를 업데이트하는 경우는 데못셋과 연결된 파드를 kubectl delete pod 명령으로 수동으로 정지시키고 자동화된 복구 기능으로 새로운 파드를 생성해야 함
- 리소스를 위한 매니페스트 작성: sample-statefulset.yaml
apiVersion: apps/v1
name: sample-statefulset
serviceName: sample-statefulset
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
volumeMounts:
- name: www
mountPath: /usr/share/nginx/html

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml
임의의 시점에 파드를 업데이트하는 경우는 데못셋과 연결된 파드를 kubectl delete pod 명령으로 수동으로 정지시키고 자동화된 복구 기능으로 새로운 파드를 생성해야 함
- 리소스를 위한 매니페스트 작성: sample-statefulset.yaml
apiVersion: apps/v1
name: sample-statefulset
serviceName: sample-statefulset
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
volumeMounts:
- name: www
mountPath: /usr/share/nginx/html

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml
임의의 시점에 파드를 업데이트하는 경우는 데못셋과 연결된 파드를 kubectl delete pod 명령으로 수동으로 정지시키고 자동화된 복구 기능으로 새로운 파드를 생성해야 함
- 리소스를 위한 매니페스트 작성: sample-statefulset.yaml
apiVersion: apps/v1
name: sample-statefulset
serviceName: sample-statefulset
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
volumeMounts:
- name: www
mountPath: /usr/share/nginx/html
name: www
spec:
accessModes:
- ReadWriteOnce
resources:
requests:
storage: 1G

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml
임의의 시점에 파드를 업데이트하는 경우는 데못셋과 연결된 파드를 kubectl delete pod 명령으로 수동으로 정지시키고 자동화된 복구 기능으로 새로운 파드를 생성해야 함
- 리소스를 위한 매니페스트 작성: sample-statefulset.yaml
apiVersion: apps/v1
name: sample-statefulset
serviceName: sample-statefulset
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
volumeMounts:
- name: www
mountPath: /usr/share/nginx/html
name: www
spec:
accessModes:
- ReadWriteOnce
resources:
requests:
storage: 1G
- 리소스 생성: kubectl apply -f sample-statefulset.yaml

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml
임의의 시점에 파드를 업데이트하는 경우는 데못셋과 연결된 파드를 kubectl delete pod 명령으로 수동으로 정지시키고 자동화된 복구 기능으로 새로운 파드를 생성해야 함
- 리소스를 위한 매니페스트 작성: sample-statefulset.yaml
apiVersion: apps/v1
name: sample-statefulset
serviceName: sample-statefulset
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
volumeMounts:
- name: www
mountPath: /usr/share/nginx/html
name: www
spec:
accessModes:
- ReadWriteOnce
resources:
requests:
storage: 1G
- 리소스 생성: kubectl apply -f sample-statefulset.yaml
- 확인: kubectl get pods

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml
임의의 시점에 파드를 업데이트하는 경우는 데못셋과 연결된 파드를 kubectl delete pod 명령으로 수동으로 정지시키고 자동화된 복구 기능으로 새로운 파드를 생성해야 함
- 리소스를 위한 매니페스트 작성: sample-statefulset.yaml
apiVersion: apps/v1
name: sample-statefulset
serviceName: sample-statefulset
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
volumeMounts:
- name: www
mountPath: /usr/share/nginx/html
name: www
spec:
accessModes:
- ReadWriteOnce
resources:
requests:
storage: 1G
- 리소스 생성: kubectl apply -f sample-statefulset.yaml
- 확인: kubectl get pods
- 볼륨 클레임 확인: kubectl get persistentvolumeclaims

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml
임의의 시점에 파드를 업데이트하는 경우는 데못셋과 연결된 파드를 kubectl delete pod 명령으로 수동으로 정지시키고 자동화된 복구 기능으로 새로운 파드를 생성해야 함
- 리소스를 위한 매니페스트 작성: sample-statefulset.yaml
apiVersion: apps/v1
name: sample-statefulset
serviceName: sample-statefulset
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
volumeMounts:
- name: www
mountPath: /usr/share/nginx/html
name: www
spec:
accessModes:
- ReadWriteOnce
resources:
requests:
storage: 1G
- 리소스 생성: kubectl apply -f sample-statefulset.yaml
- 확인: kubectl get pods
- 볼륨 클레임 확인: kubectl get persistentvolumeclaims
- persistemvolume 확인: kubectl get persistentvolumes

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml
임의의 시점에 파드를 업데이트하는 경우는 데못셋과 연결된 파드를 kubectl delete pod 명령으로 수동으로 정지시키고 자동화된 복구 기능으로 새로운 파드를 생성해야 함
- 리소스를 위한 매니페스트 작성: sample-statefulset.yaml
apiVersion: apps/v1
name: sample-statefulset
serviceName: sample-statefulset
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
volumeMounts:
- name: www
mountPath: /usr/share/nginx/html
name: www
spec:
accessModes:
- ReadWriteOnce
resources:
requests:
storage: 1G
- 리소스 생성: kubectl apply -f sample-statefulset.yaml
- 확인: kubectl get pods
- 볼륨 클레임 확인: kubectl get persistentvolumeclaims
- persistemvolume 확인: kubectl get persistentvolumes

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml
임의의 시점에 파드를 업데이트하는 경우는 데못셋과 연결된 파드를 kubectl delete pod 명령으로 수동으로 정지시키고 자동화된 복구 기능으로 새로운 파드를 생성해야 함
- 리소스를 위한 매니페스트 작성: sample-statefulset.yaml
apiVersion: apps/v1
name: sample-statefulset
serviceName: sample-statefulset
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
volumeMounts:
- name: www
mountPath: /usr/share/nginx/html
name: www
spec:
accessModes:
- ReadWriteOnce
resources:
requests:
storage: 1G
- 리소스 생성: kubectl apply -f sample-statefulset.yaml
- 확인: kubectl get pods
- 볼륨 클레임 확인: kubectl get persistentvolumeclaims
- persistemvolume 확인: kubectl get persistentvolumes

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml
임의의 시점에 파드를 업데이트하는 경우는 데못셋과 연결된 파드를 kubectl delete pod 명령으로 수동으로 정지시키고 자동화된 복구 기능으로 새로운 파드를 생성해야 함
- 리소스를 위한 매니페스트 작성: sample-statefulset.yaml
apiVersion: apps/v1
name: sample-statefulset
serviceName: sample-statefulset
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
volumeMounts:
- name: www
mountPath: /usr/share/nginx/html
name: www
spec:
accessModes:
- ReadWriteOnce
resources:
requests:
storage: 1G
- 리소스 생성: kubectl apply -f sample-statefulset.yaml
- 확인: kubectl get pods
- 볼륨 클레임 확인: kubectl get persistentvolumeclaims
- persistemvolume 확인: kubectl get persistentvolumes
kubectl scale statefulset sample-statefulset --replicas=5

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml
임의의 시점에 파드를 업데이트하는 경우는 데못셋과 연결된 파드를 kubectl delete pod 명령으로 수동으로 정지시키고 자동화된 복구 기능으로 새로운 파드를 생성해야 함
- 리소스를 위한 매니페스트 작성: sample-statefulset.yaml
apiVersion: apps/v1
name: sample-statefulset
serviceName: sample-statefulset
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
volumeMounts:
- name: www
mountPath: /usr/share/nginx/html
name: www
spec:
accessModes:
- ReadWriteOnce
resources:
requests:
storage: 1G
- 리소스 생성: kubectl apply -f sample-statefulset.yaml
- 확인: kubectl get pods
- 볼륨 클레임 확인: kubectl get persistentvolumeclaims
- persistemvolume 확인: kubectl get persistentvolumes
kubectl scale statefulset sample-statefulset --replicas=5

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml
임의의 시점에 파드를 업데이트하는 경우는 데못셋과 연결된 파드를 kubectl delete pod 명령으로 수동으로 정지시키고 자동화된 복구 기능으로 새로운 파드를 생성해야 함
- 리소스를 위한 매니페스트 작성: sample-statefulset.yaml
apiVersion: apps/v1
name: sample-statefulset
serviceName: sample-statefulset
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
volumeMounts:
- name: www
mountPath: /usr/share/nginx/html
name: www
spec:
accessModes:
- ReadWriteOnce
resources:
requests:
storage: 1G
- 리소스 생성: kubectl apply -f sample-statefulset.yaml
- 확인: kubectl get pods
- 볼륨 클레임 확인: kubectl get persistentvolumeclaims
- persistemvolume 확인: kubectl get persistentvolumes
kubectl scale statefulset sample-statefulset --replicas=5

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml
임의의 시점에 파드를 업데이트하는 경우는 데못셋과 연결된 파드를 kubectl delete pod 명령으로 수동으로 정지시키고 자동화된 복구 기능으로 새로운 파드를 생성해야 함
- 리소스를 위한 매니페스트 작성: sample-statefulset.yaml
apiVersion: apps/v1
name: sample-statefulset
serviceName: sample-statefulset
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
volumeMounts:
- name: www
mountPath: /usr/share/nginx/html
name: www
spec:
accessModes:
- ReadWriteOnce
resources:
requests:
storage: 1G
- 리소스 생성: kubectl apply -f sample-statefulset.yaml
- 확인: kubectl get pods
- 볼륨 클레임 확인: kubectl get persistentvolumeclaims
- persistemvolume 확인: kubectl get persistentvolumes
kubectl scale statefulset sample-statefulset --replicas=5

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml
임의의 시점에 파드를 업데이트하는 경우는 데못셋과 연결된 파드를 kubectl delete pod 명령으로 수동으로 정지시키고 자동화된 복구 기능으로 새로운 파드를 생성해야 함
- 리소스를 위한 매니페스트 작성: sample-statefulset.yaml
apiVersion: apps/v1
name: sample-statefulset
serviceName: sample-statefulset
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
volumeMounts:
- name: www
mountPath: /usr/share/nginx/html
name: www
spec:
accessModes:
- ReadWriteOnce
resources:
requests:
storage: 1G
- 리소스 생성: kubectl apply -f sample-statefulset.yaml
- 확인: kubectl get pods
- 볼륨 클레임 확인: kubectl get persistentvolumeclaims
- persistemvolume 확인: kubectl get persistentvolumes
kubectl scale statefulset sample-statefulset --replicas=5

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml
임의의 시점에 파드를 업데이트하는 경우는 데못셋과 연결된 파드를 kubectl delete pod 명령으로 수동으로 정지시키고 자동화된 복구 기능으로 새로운 파드를 생성해야 함
- 리소스를 위한 매니페스트 작성: sample-statefulset.yaml
apiVersion: apps/v1
name: sample-statefulset
serviceName: sample-statefulset
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
volumeMounts:
- name: www
mountPath: /usr/share/nginx/html
name: www
spec:
accessModes:
- ReadWriteOnce
resources:
requests:
storage: 1G
- 리소스 생성: kubectl apply -f sample-statefulset.yaml
- 확인: kubectl get pods
- 볼륨 클레임 확인: kubectl get persistentvolumeclaims
- persistemvolume 확인: kubectl get persistentvolumes
kubectl scale statefulset sample-statefulset --replicas=5

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml
임의의 시점에 파드를 업데이트하는 경우는 데못셋과 연결된 파드를 kubectl delete pod 명령으로 수동으로 정지시키고 자동화된 복구 기능으로 새로운 파드를 생성해야 함
- 리소스를 위한 매니페스트 작성: sample-statefulset.yaml
apiVersion: apps/v1
name: sample-statefulset
serviceName: sample-statefulset
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
volumeMounts:
- name: www
mountPath: /usr/share/nginx/html
name: www
spec:
accessModes:
- ReadWriteOnce
resources:
requests:
storage: 1G
- 리소스 생성: kubectl apply -f sample-statefulset.yaml
- 확인: kubectl get pods
- 볼륨 클레임 확인: kubectl get persistentvolumeclaims
- persistemvolume 확인: kubectl get persistentvolumes
kubectl scale statefulset sample-statefulset --replicas=5

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml
임의의 시점에 파드를 업데이트하는 경우는 데못셋과 연결된 파드를 kubectl delete pod 명령으로 수동으로 정지시키고 자동화된 복구 기능으로 새로운 파드를 생성해야 함
- 리소스를 위한 매니페스트 작성: sample-statefulset.yaml
apiVersion: apps/v1
name: sample-statefulset
serviceName: sample-statefulset
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
volumeMounts:
- name: www
mountPath: /usr/share/nginx/html
name: www
spec:
accessModes:
- ReadWriteOnce
resources:
requests:
storage: 1G
- 리소스 생성: kubectl apply -f sample-statefulset.yaml
- 확인: kubectl get pods
- 볼륨 클레임 확인: kubectl get persistentvolumeclaims
- persistemvolume 확인: kubectl get persistentvolumes
kubectl scale statefulset sample-statefulset --replicas=5

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml
임의의 시점에 파드를 업데이트하는 경우는 데못셋과 연결된 파드를 kubectl delete pod 명령으로 수동으로 정지시키고 자동화된 복구 기능으로 새로운 파드를 생성해야 함
- 리소스를 위한 매니페스트 작성: sample-statefulset.yaml
apiVersion: apps/v1
name: sample-statefulset
serviceName: sample-statefulset
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
volumeMounts:
- name: www
mountPath: /usr/share/nginx/html
name: www
spec:
accessModes:
- ReadWriteOnce
resources:
requests:
storage: 1G
- 리소스 생성: kubectl apply -f sample-statefulset.yaml
- 확인: kubectl get pods
- 볼륨 클레임 확인: kubectl get persistentvolumeclaims
- persistemvolume 확인: kubectl get persistentvolumes
kubectl scale statefulset sample-statefulset --replicas=5

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml
임의의 시점에 파드를 업데이트하는 경우는 데못셋과 연결된 파드를 kubectl delete pod 명령으로 수동으로 정지시키고 자동화된 복구 기능으로 새로운 파드를 생성해야 함
- 리소스를 위한 매니페스트 작성: sample-statefulset.yaml
apiVersion: apps/v1
name: sample-statefulset
serviceName: sample-statefulset
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
volumeMounts:
- name: www
mountPath: /usr/share/nginx/html
name: www
spec:
accessModes:
- ReadWriteOnce
resources:
requests:
storage: 1G
- 리소스 생성: kubectl apply -f sample-statefulset.yaml
- 확인: kubectl get pods
- 볼륨 클레임 확인: kubectl get persistentvolumeclaims
- persistemvolume 확인: kubectl get persistentvolumes
kubectl scale statefulset sample-statefulset --replicas=5

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml
임의의 시점에 파드를 업데이트하는 경우는 데못셋과 연결된 파드를 kubectl delete pod 명령으로 수동으로 정지시키고 자동화된 복구 기능으로 새로운 파드를 생성해야 함
- 리소스를 위한 매니페스트 작성: sample-statefulset.yaml
apiVersion: apps/v1
name: sample-statefulset
serviceName: sample-statefulset
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
volumeMounts:
- name: www
mountPath: /usr/share/nginx/html
name: www
spec:
accessModes:
- ReadWriteOnce
resources:
requests:
storage: 1G
- 리소스 생성: kubectl apply -f sample-statefulset.yaml
- 확인: kubectl get pods
- 볼륨 클레임 확인: kubectl get persistentvolumeclaims
- persistemvolume 확인: kubectl get persistentvolumes
kubectl scale statefulset sample-statefulset --replicas=5
apiVersion: batch/v1

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml
임의의 시점에 파드를 업데이트하는 경우는 데못셋과 연결된 파드를 kubectl delete pod 명령으로 수동으로 정지시키고 자동화된 복구 기능으로 새로운 파드를 생성해야 함
- 리소스를 위한 매니페스트 작성: sample-statefulset.yaml
apiVersion: apps/v1
name: sample-statefulset
serviceName: sample-statefulset
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
volumeMounts:
- name: www
mountPath: /usr/share/nginx/html
name: www
spec:
accessModes:
- ReadWriteOnce
resources:
requests:
storage: 1G
- 리소스 생성: kubectl apply -f sample-statefulset.yaml
- 확인: kubectl get pods
- 볼륨 클레임 확인: kubectl get persistentvolumeclaims
- persistemvolume 확인: kubectl get persistentvolumes
kubectl scale statefulset sample-statefulset --replicas=5
apiVersion: batch/v1

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml
임의의 시점에 파드를 업데이트하는 경우는 데못셋과 연결된 파드를 kubectl delete pod 명령으로 수동으로 정지시키고 자동화된 복구 기능으로 새로운 파드를 생성해야 함
- 리소스를 위한 매니페스트 작성: sample-statefulset.yaml
apiVersion: apps/v1
name: sample-statefulset
serviceName: sample-statefulset
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
volumeMounts:
- name: www
mountPath: /usr/share/nginx/html
name: www
spec:
accessModes:
- ReadWriteOnce
resources:
requests:
storage: 1G
- 리소스 생성: kubectl apply -f sample-statefulset.yaml
- 확인: kubectl get pods
- 볼륨 클레임 확인: kubectl get persistentvolumeclaims
- persistemvolume 확인: kubectl get persistentvolumes
kubectl scale statefulset sample-statefulset --replicas=5
apiVersion: batch/v1

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml
임의의 시점에 파드를 업데이트하는 경우는 데못셋과 연결된 파드를 kubectl delete pod 명령으로 수동으로 정지시키고 자동화된 복구 기능으로 새로운 파드를 생성해야 함
- 리소스를 위한 매니페스트 작성: sample-statefulset.yaml
apiVersion: apps/v1
name: sample-statefulset
serviceName: sample-statefulset
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
volumeMounts:
- name: www
mountPath: /usr/share/nginx/html
name: www
spec:
accessModes:
- ReadWriteOnce
resources:
requests:
storage: 1G
- 리소스 생성: kubectl apply -f sample-statefulset.yaml
- 확인: kubectl get pods
- 볼륨 클레임 확인: kubectl get persistentvolumeclaims
- persistemvolume 확인: kubectl get persistentvolumes
kubectl scale statefulset sample-statefulset --replicas=5
apiVersion: batch/v1

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml
임의의 시점에 파드를 업데이트하는 경우는 데못셋과 연결된 파드를 kubectl delete pod 명령으로 수동으로 정지시키고 자동화된 복구 기능으로 새로운 파드를 생성해야 함
- 리소스를 위한 매니페스트 작성: sample-statefulset.yaml
apiVersion: apps/v1
name: sample-statefulset
serviceName: sample-statefulset
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
volumeMounts:
- name: www
mountPath: /usr/share/nginx/html
name: www
spec:
accessModes:
- ReadWriteOnce
resources:
requests:
storage: 1G
- 리소스 생성: kubectl apply -f sample-statefulset.yaml
- 확인: kubectl get pods
- 볼륨 클레임 확인: kubectl get persistentvolumeclaims
- persistemvolume 확인: kubectl get persistentvolumes
kubectl scale statefulset sample-statefulset --replicas=5
apiVersion: batch/v1

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml
임의의 시점에 파드를 업데이트하는 경우는 데못셋과 연결된 파드를 kubectl delete pod 명령으로 수동으로 정지시키고 자동화된 복구 기능으로 새로운 파드를 생성해야 함
- 리소스를 위한 매니페스트 작성: sample-statefulset.yaml
apiVersion: apps/v1
name: sample-statefulset
serviceName: sample-statefulset
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
volumeMounts:
- name: www
mountPath: /usr/share/nginx/html
name: www
spec:
accessModes:
- ReadWriteOnce
resources:
requests:
storage: 1G
- 리소스 생성: kubectl apply -f sample-statefulset.yaml
- 확인: kubectl get pods
- 볼륨 클레임 확인: kubectl get persistentvolumeclaims
- persistemvolume 확인: kubectl get persistentvolumes
kubectl scale statefulset sample-statefulset --replicas=5
apiVersion: batch/v1
spec:
containers:
- name: pi
image: perl:5.34.0
command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"]
restartPolicy: Never

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml
임의의 시점에 파드를 업데이트하는 경우는 데못셋과 연결된 파드를 kubectl delete pod 명령으로 수동으로 정지시키고 자동화된 복구 기능으로 새로운 파드를 생성해야 함
- 리소스를 위한 매니페스트 작성: sample-statefulset.yaml
apiVersion: apps/v1
name: sample-statefulset
serviceName: sample-statefulset
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
volumeMounts:
- name: www
mountPath: /usr/share/nginx/html
name: www
spec:
accessModes:
- ReadWriteOnce
resources:
requests:
storage: 1G
- 리소스 생성: kubectl apply -f sample-statefulset.yaml
- 확인: kubectl get pods
- 볼륨 클레임 확인: kubectl get persistentvolumeclaims
- persistemvolume 확인: kubectl get persistentvolumes
kubectl scale statefulset sample-statefulset --replicas=5
apiVersion: batch/v1
spec:
containers:
- name: pi
image: perl:5.34.0
command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"]
restartPolicy: Never

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml
임의의 시점에 파드를 업데이트하는 경우는 데못셋과 연결된 파드를 kubectl delete pod 명령으로 수동으로 정지시키고 자동화된 복구 기능으로 새로운 파드를 생성해야 함
- 리소스를 위한 매니페스트 작성: sample-statefulset.yaml
apiVersion: apps/v1
name: sample-statefulset
serviceName: sample-statefulset
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
volumeMounts:
- name: www
mountPath: /usr/share/nginx/html
name: www
spec:
accessModes:
- ReadWriteOnce
resources:
requests:
storage: 1G
- 리소스 생성: kubectl apply -f sample-statefulset.yaml
- 확인: kubectl get pods
- 볼륨 클레임 확인: kubectl get persistentvolumeclaims
- persistemvolume 확인: kubectl get persistentvolumes
kubectl scale statefulset sample-statefulset --replicas=5
apiVersion: batch/v1
spec:
containers:
- name: pi
image: perl:5.34.0
command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"]
restartPolicy: Never

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml
임의의 시점에 파드를 업데이트하는 경우는 데못셋과 연결된 파드를 kubectl delete pod 명령으로 수동으로 정지시키고 자동화된 복구 기능으로 새로운 파드를 생성해야 함
- 리소스를 위한 매니페스트 작성: sample-statefulset.yaml
apiVersion: apps/v1
name: sample-statefulset
serviceName: sample-statefulset
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
volumeMounts:
- name: www
mountPath: /usr/share/nginx/html
name: www
spec:
accessModes:
- ReadWriteOnce
resources:
requests:
storage: 1G
- 리소스 생성: kubectl apply -f sample-statefulset.yaml
- 확인: kubectl get pods
- 볼륨 클레임 확인: kubectl get persistentvolumeclaims
- persistemvolume 확인: kubectl get persistentvolumes
kubectl scale statefulset sample-statefulset --replicas=5
apiVersion: batch/v1
spec:
containers:
- name: pi
image: perl:5.34.0
command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"]
restartPolicy: Never
kubectl apply -f sample-job.yaml

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml
임의의 시점에 파드를 업데이트하는 경우는 데못셋과 연결된 파드를 kubectl delete pod 명령으로 수동으로 정지시키고 자동화된 복구 기능으로 새로운 파드를 생성해야 함
- 리소스를 위한 매니페스트 작성: sample-statefulset.yaml
apiVersion: apps/v1
name: sample-statefulset
serviceName: sample-statefulset
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
volumeMounts:
- name: www
mountPath: /usr/share/nginx/html
name: www
spec:
accessModes:
- ReadWriteOnce
resources:
requests:
storage: 1G
- 리소스 생성: kubectl apply -f sample-statefulset.yaml
- 확인: kubectl get pods
- 볼륨 클레임 확인: kubectl get persistentvolumeclaims
- persistemvolume 확인: kubectl get persistentvolumes
kubectl scale statefulset sample-statefulset --replicas=5
apiVersion: batch/v1
spec:
containers:
- name: pi
image: perl:5.34.0
command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"]
restartPolicy: Never
kubectl apply -f sample-job.yaml

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml
임의의 시점에 파드를 업데이트하는 경우는 데못셋과 연결된 파드를 kubectl delete pod 명령으로 수동으로 정지시키고 자동화된 복구 기능으로 새로운 파드를 생성해야 함
- 리소스를 위한 매니페스트 작성: sample-statefulset.yaml
apiVersion: apps/v1
name: sample-statefulset
serviceName: sample-statefulset
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
volumeMounts:
- name: www
mountPath: /usr/share/nginx/html
name: www
spec:
accessModes:
- ReadWriteOnce
resources:
requests:
storage: 1G
- 리소스 생성: kubectl apply -f sample-statefulset.yaml
- 확인: kubectl get pods
- 볼륨 클레임 확인: kubectl get persistentvolumeclaims
- persistemvolume 확인: kubectl get persistentvolumes
kubectl scale statefulset sample-statefulset --replicas=5
apiVersion: batch/v1
spec:
containers:
- name: pi
image: perl:5.34.0
command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"]
restartPolicy: Never
kubectl apply -f sample-job.yaml
kubectl get jobs

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml
임의의 시점에 파드를 업데이트하는 경우는 데못셋과 연결된 파드를 kubectl delete pod 명령으로 수동으로 정지시키고 자동화된 복구 기능으로 새로운 파드를 생성해야 함
- 리소스를 위한 매니페스트 작성: sample-statefulset.yaml
apiVersion: apps/v1
name: sample-statefulset
serviceName: sample-statefulset
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
volumeMounts:
- name: www
mountPath: /usr/share/nginx/html
name: www
spec:
accessModes:
- ReadWriteOnce
resources:
requests:
storage: 1G
- 리소스 생성: kubectl apply -f sample-statefulset.yaml
- 확인: kubectl get pods
- 볼륨 클레임 확인: kubectl get persistentvolumeclaims
- persistemvolume 확인: kubectl get persistentvolumes
kubectl scale statefulset sample-statefulset --replicas=5
apiVersion: batch/v1
spec:
containers:
- name: pi
image: perl:5.34.0
command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"]
restartPolicy: Never
kubectl apply -f sample-job.yaml
kubectl get jobs

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml
임의의 시점에 파드를 업데이트하는 경우는 데못셋과 연결된 파드를 kubectl delete pod 명령으로 수동으로 정지시키고 자동화된 복구 기능으로 새로운 파드를 생성해야 함
- 리소스를 위한 매니페스트 작성: sample-statefulset.yaml
apiVersion: apps/v1
name: sample-statefulset
serviceName: sample-statefulset
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
volumeMounts:
- name: www
mountPath: /usr/share/nginx/html
name: www
spec:
accessModes:
- ReadWriteOnce
resources:
requests:
storage: 1G
- 리소스 생성: kubectl apply -f sample-statefulset.yaml
- 확인: kubectl get pods
- 볼륨 클레임 확인: kubectl get persistentvolumeclaims
- persistemvolume 확인: kubectl get persistentvolumes
kubectl scale statefulset sample-statefulset --replicas=5
apiVersion: batch/v1
spec:
containers:
- name: pi
image: perl:5.34.0
command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"]
restartPolicy: Never
kubectl apply -f sample-job.yaml
kubectl get jobs
kubectl get pods --watch

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml
임의의 시점에 파드를 업데이트하는 경우는 데못셋과 연결된 파드를 kubectl delete pod 명령으로 수동으로 정지시키고 자동화된 복구 기능으로 새로운 파드를 생성해야 함
- 리소스를 위한 매니페스트 작성: sample-statefulset.yaml
apiVersion: apps/v1
name: sample-statefulset
serviceName: sample-statefulset
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
volumeMounts:
- name: www
mountPath: /usr/share/nginx/html
name: www
spec:
accessModes:
- ReadWriteOnce
resources:
requests:
storage: 1G
- 리소스 생성: kubectl apply -f sample-statefulset.yaml
- 확인: kubectl get pods
- 볼륨 클레임 확인: kubectl get persistentvolumeclaims
- persistemvolume 확인: kubectl get persistentvolumes
kubectl scale statefulset sample-statefulset --replicas=5
apiVersion: batch/v1
spec:
containers:
- name: pi
image: perl:5.34.0
command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"]
restartPolicy: Never
kubectl apply -f sample-job.yaml
kubectl get jobs
kubectl get pods --watch

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml
임의의 시점에 파드를 업데이트하는 경우는 데못셋과 연결된 파드를 kubectl delete pod 명령으로 수동으로 정지시키고 자동화된 복구 기능으로 새로운 파드를 생성해야 함
- 리소스를 위한 매니페스트 작성: sample-statefulset.yaml
apiVersion: apps/v1
name: sample-statefulset
serviceName: sample-statefulset
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
volumeMounts:
- name: www
mountPath: /usr/share/nginx/html
name: www
spec:
accessModes:
- ReadWriteOnce
resources:
requests:
storage: 1G
- 리소스 생성: kubectl apply -f sample-statefulset.yaml
- 확인: kubectl get pods
- 볼륨 클레임 확인: kubectl get persistentvolumeclaims
- persistemvolume 확인: kubectl get persistentvolumes
kubectl scale statefulset sample-statefulset --replicas=5
apiVersion: batch/v1
spec:
containers:
- name: pi
image: perl:5.34.0
command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"]
restartPolicy: Never
kubectl apply -f sample-job.yaml
kubectl get jobs
kubectl get pods --watch
kubectl logs pi

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml
임의의 시점에 파드를 업데이트하는 경우는 데못셋과 연결된 파드를 kubectl delete pod 명령으로 수동으로 정지시키고 자동화된 복구 기능으로 새로운 파드를 생성해야 함
- 리소스를 위한 매니페스트 작성: sample-statefulset.yaml
apiVersion: apps/v1
name: sample-statefulset
serviceName: sample-statefulset
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
volumeMounts:
- name: www
mountPath: /usr/share/nginx/html
name: www
spec:
accessModes:
- ReadWriteOnce
resources:
requests:
storage: 1G
- 리소스 생성: kubectl apply -f sample-statefulset.yaml
- 확인: kubectl get pods
- 볼륨 클레임 확인: kubectl get persistentvolumeclaims
- persistemvolume 확인: kubectl get persistentvolumes
kubectl scale statefulset sample-statefulset --replicas=5
apiVersion: batch/v1
spec:
containers:
- name: pi
image: perl:5.34.0
command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"]
restartPolicy: Never
kubectl apply -f sample-job.yaml
kubectl get jobs
kubectl get pods --watch
kubectl logs pi

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml
임의의 시점에 파드를 업데이트하는 경우는 데못셋과 연결된 파드를 kubectl delete pod 명령으로 수동으로 정지시키고 자동화된 복구 기능으로 새로운 파드를 생성해야 함
- 리소스를 위한 매니페스트 작성: sample-statefulset.yaml
apiVersion: apps/v1
name: sample-statefulset
serviceName: sample-statefulset
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
volumeMounts:
- name: www
mountPath: /usr/share/nginx/html
name: www
spec:
accessModes:
- ReadWriteOnce
resources:
requests:
storage: 1G
- 리소스 생성: kubectl apply -f sample-statefulset.yaml
- 확인: kubectl get pods
- 볼륨 클레임 확인: kubectl get persistentvolumeclaims
- persistemvolume 확인: kubectl get persistentvolumes
kubectl scale statefulset sample-statefulset --replicas=5
apiVersion: batch/v1
spec:
containers:
- name: pi
image: perl:5.34.0
command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"]
restartPolicy: Never
kubectl apply -f sample-job.yaml
kubectl get jobs
kubectl get pods --watch
kubectl logs pi

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml
임의의 시점에 파드를 업데이트하는 경우는 데못셋과 연결된 파드를 kubectl delete pod 명령으로 수동으로 정지시키고 자동화된 복구 기능으로 새로운 파드를 생성해야 함
- 리소스를 위한 매니페스트 작성: sample-statefulset.yaml
apiVersion: apps/v1
name: sample-statefulset
serviceName: sample-statefulset
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
volumeMounts:
- name: www
mountPath: /usr/share/nginx/html
name: www
spec:
accessModes:
- ReadWriteOnce
resources:
requests:
storage: 1G
- 리소스 생성: kubectl apply -f sample-statefulset.yaml
- 확인: kubectl get pods
- 볼륨 클레임 확인: kubectl get persistentvolumeclaims
- persistemvolume 확인: kubectl get persistentvolumes
kubectl scale statefulset sample-statefulset --replicas=5
apiVersion: batch/v1
spec:
containers:
- name: pi
image: perl:5.34.0
command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"]
restartPolicy: Never
kubectl apply -f sample-job.yaml
kubectl get jobs
kubectl get pods --watch
kubectl logs pi

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml
임의의 시점에 파드를 업데이트하는 경우는 데못셋과 연결된 파드를 kubectl delete pod 명령으로 수동으로 정지시키고 자동화된 복구 기능으로 새로운 파드를 생성해야 함
- 리소스를 위한 매니페스트 작성: sample-statefulset.yaml
apiVersion: apps/v1
name: sample-statefulset
serviceName: sample-statefulset
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
volumeMounts:
- name: www
mountPath: /usr/share/nginx/html
name: www
spec:
accessModes:
- ReadWriteOnce
resources:
requests:
storage: 1G
- 리소스 생성: kubectl apply -f sample-statefulset.yaml
- 확인: kubectl get pods
- 볼륨 클레임 확인: kubectl get persistentvolumeclaims
- persistemvolume 확인: kubectl get persistentvolumes
kubectl scale statefulset sample-statefulset --replicas=5
apiVersion: batch/v1
spec:
containers:
- name: pi
image: perl:5.34.0
command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"]
restartPolicy: Never
kubectl apply -f sample-job.yaml
kubectl get jobs
kubectl get pods --watch
kubectl logs pi

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml
임의의 시점에 파드를 업데이트하는 경우는 데못셋과 연결된 파드를 kubectl delete pod 명령으로 수동으로 정지시키고 자동화된 복구 기능으로 새로운 파드를 생성해야 함
- 리소스를 위한 매니페스트 작성: sample-statefulset.yaml
apiVersion: apps/v1
name: sample-statefulset
serviceName: sample-statefulset
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
volumeMounts:
- name: www
mountPath: /usr/share/nginx/html
name: www
spec:
accessModes:
- ReadWriteOnce
resources:
requests:
storage: 1G
- 리소스 생성: kubectl apply -f sample-statefulset.yaml
- 확인: kubectl get pods
- 볼륨 클레임 확인: kubectl get persistentvolumeclaims
- persistemvolume 확인: kubectl get persistentvolumes
kubectl scale statefulset sample-statefulset --replicas=5
apiVersion: batch/v1
spec:
containers:
- name: pi
image: perl:5.34.0
command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"]
restartPolicy: Never
kubectl apply -f sample-job.yaml
kubectl get jobs
kubectl get pods --watch
kubectl logs pi
apiVersion: batch/v1

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml
임의의 시점에 파드를 업데이트하는 경우는 데못셋과 연결된 파드를 kubectl delete pod 명령으로 수동으로 정지시키고 자동화된 복구 기능으로 새로운 파드를 생성해야 함
- 리소스를 위한 매니페스트 작성: sample-statefulset.yaml
apiVersion: apps/v1
name: sample-statefulset
serviceName: sample-statefulset
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
volumeMounts:
- name: www
mountPath: /usr/share/nginx/html
name: www
spec:
accessModes:
- ReadWriteOnce
resources:
requests:
storage: 1G
- 리소스 생성: kubectl apply -f sample-statefulset.yaml
- 확인: kubectl get pods
- 볼륨 클레임 확인: kubectl get persistentvolumeclaims
- persistemvolume 확인: kubectl get persistentvolumes
kubectl scale statefulset sample-statefulset --replicas=5
apiVersion: batch/v1
spec:
containers:
- name: pi
image: perl:5.34.0
command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"]
restartPolicy: Never
kubectl apply -f sample-job.yaml
kubectl get jobs
kubectl get pods --watch
kubectl logs pi
apiVersion: batch/v1

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml
임의의 시점에 파드를 업데이트하는 경우는 데못셋과 연결된 파드를 kubectl delete pod 명령으로 수동으로 정지시키고 자동화된 복구 기능으로 새로운 파드를 생성해야 함
- 리소스를 위한 매니페스트 작성: sample-statefulset.yaml
apiVersion: apps/v1
name: sample-statefulset
serviceName: sample-statefulset
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
volumeMounts:
- name: www
mountPath: /usr/share/nginx/html
name: www
spec:
accessModes:
- ReadWriteOnce
resources:
requests:
storage: 1G
- 리소스 생성: kubectl apply -f sample-statefulset.yaml
- 확인: kubectl get pods
- 볼륨 클레임 확인: kubectl get persistentvolumeclaims
- persistemvolume 확인: kubectl get persistentvolumes
kubectl scale statefulset sample-statefulset --replicas=5
apiVersion: batch/v1
spec:
containers:
- name: pi
image: perl:5.34.0
command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"]
restartPolicy: Never
kubectl apply -f sample-job.yaml
kubectl get jobs
kubectl get pods --watch
kubectl logs pi
apiVersion: batch/v1

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml
임의의 시점에 파드를 업데이트하는 경우는 데못셋과 연결된 파드를 kubectl delete pod 명령으로 수동으로 정지시키고 자동화된 복구 기능으로 새로운 파드를 생성해야 함
- 리소스를 위한 매니페스트 작성: sample-statefulset.yaml
apiVersion: apps/v1
name: sample-statefulset
serviceName: sample-statefulset
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
volumeMounts:
- name: www
mountPath: /usr/share/nginx/html
name: www
spec:
accessModes:
- ReadWriteOnce
resources:
requests:
storage: 1G
- 리소스 생성: kubectl apply -f sample-statefulset.yaml
- 확인: kubectl get pods
- 볼륨 클레임 확인: kubectl get persistentvolumeclaims
- persistemvolume 확인: kubectl get persistentvolumes
kubectl scale statefulset sample-statefulset --replicas=5
apiVersion: batch/v1
spec:
containers:
- name: pi
image: perl:5.34.0
command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"]
restartPolicy: Never
kubectl apply -f sample-job.yaml
kubectl get jobs
kubectl get pods --watch
kubectl logs pi
apiVersion: batch/v1

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml
임의의 시점에 파드를 업데이트하는 경우는 데못셋과 연결된 파드를 kubectl delete pod 명령으로 수동으로 정지시키고 자동화된 복구 기능으로 새로운 파드를 생성해야 함
- 리소스를 위한 매니페스트 작성: sample-statefulset.yaml
apiVersion: apps/v1
name: sample-statefulset
serviceName: sample-statefulset
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
volumeMounts:
- name: www
mountPath: /usr/share/nginx/html
name: www
spec:
accessModes:
- ReadWriteOnce
resources:
requests:
storage: 1G
- 리소스 생성: kubectl apply -f sample-statefulset.yaml
- 확인: kubectl get pods
- 볼륨 클레임 확인: kubectl get persistentvolumeclaims
- persistemvolume 확인: kubectl get persistentvolumes
kubectl scale statefulset sample-statefulset --replicas=5
apiVersion: batch/v1
spec:
containers:
- name: pi
image: perl:5.34.0
command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"]
restartPolicy: Never
kubectl apply -f sample-job.yaml
kubectl get jobs
kubectl get pods --watch
kubectl logs pi
apiVersion: batch/v1

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml
임의의 시점에 파드를 업데이트하는 경우는 데못셋과 연결된 파드를 kubectl delete pod 명령으로 수동으로 정지시키고 자동화된 복구 기능으로 새로운 파드를 생성해야 함
- 리소스를 위한 매니페스트 작성: sample-statefulset.yaml
apiVersion: apps/v1
name: sample-statefulset
serviceName: sample-statefulset
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
volumeMounts:
- name: www
mountPath: /usr/share/nginx/html
name: www
spec:
accessModes:
- ReadWriteOnce
resources:
requests:
storage: 1G
- 리소스 생성: kubectl apply -f sample-statefulset.yaml
- 확인: kubectl get pods
- 볼륨 클레임 확인: kubectl get persistentvolumeclaims
- persistemvolume 확인: kubectl get persistentvolumes
kubectl scale statefulset sample-statefulset --replicas=5
apiVersion: batch/v1
spec:
containers:
- name: pi
image: perl:5.34.0
command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"]
restartPolicy: Never
kubectl apply -f sample-job.yaml
kubectl get jobs
kubectl get pods --watch
kubectl logs pi
apiVersion: batch/v1
spec:
containers:
- name: errorjob-unvalidcommand
image: busybox
command: ["ls", "unvalid path"]
restartPolicy: Never

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml
임의의 시점에 파드를 업데이트하는 경우는 데못셋과 연결된 파드를 kubectl delete pod 명령으로 수동으로 정지시키고 자동화된 복구 기능으로 새로운 파드를 생성해야 함
- 리소스를 위한 매니페스트 작성: sample-statefulset.yaml
apiVersion: apps/v1
name: sample-statefulset
serviceName: sample-statefulset
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
volumeMounts:
- name: www
mountPath: /usr/share/nginx/html
name: www
spec:
accessModes:
- ReadWriteOnce
resources:
requests:
storage: 1G
- 리소스 생성: kubectl apply -f sample-statefulset.yaml
- 확인: kubectl get pods
- 볼륨 클레임 확인: kubectl get persistentvolumeclaims
- persistemvolume 확인: kubectl get persistentvolumes
kubectl scale statefulset sample-statefulset --replicas=5
apiVersion: batch/v1
spec:
containers:
- name: pi
image: perl:5.34.0
command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"]
restartPolicy: Never
kubectl apply -f sample-job.yaml
kubectl get jobs
kubectl get pods --watch
kubectl logs pi
apiVersion: batch/v1
spec:
containers:
- name: errorjob-unvalidcommand
image: busybox
command: ["ls", "unvalid path"]
restartPolicy: Never
리소스 생성: kubectl apply -f sample-job-never-restart.yaml

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml
임의의 시점에 파드를 업데이트하는 경우는 데못셋과 연결된 파드를 kubectl delete pod 명령으로 수동으로 정지시키고 자동화된 복구 기능으로 새로운 파드를 생성해야 함
- 리소스를 위한 매니페스트 작성: sample-statefulset.yaml
apiVersion: apps/v1
name: sample-statefulset
serviceName: sample-statefulset
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
volumeMounts:
- name: www
mountPath: /usr/share/nginx/html
name: www
spec:
accessModes:
- ReadWriteOnce
resources:
requests:
storage: 1G
- 리소스 생성: kubectl apply -f sample-statefulset.yaml
- 확인: kubectl get pods
- 볼륨 클레임 확인: kubectl get persistentvolumeclaims
- persistemvolume 확인: kubectl get persistentvolumes
kubectl scale statefulset sample-statefulset --replicas=5
apiVersion: batch/v1
spec:
containers:
- name: pi
image: perl:5.34.0
command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"]
restartPolicy: Never
kubectl apply -f sample-job.yaml
kubectl get jobs
kubectl get pods --watch
kubectl logs pi
apiVersion: batch/v1
spec:
containers:
- name: errorjob-unvalidcommand
image: busybox
command: ["ls", "unvalid path"]
restartPolicy: Never
리소스 생성: kubectl apply -f sample-job-never-restart.yaml
파드 확인: kubectl get pods

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml
임의의 시점에 파드를 업데이트하는 경우는 데못셋과 연결된 파드를 kubectl delete pod 명령으로 수동으로 정지시키고 자동화된 복구 기능으로 새로운 파드를 생성해야 함
- 리소스를 위한 매니페스트 작성: sample-statefulset.yaml
apiVersion: apps/v1
name: sample-statefulset
serviceName: sample-statefulset
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
volumeMounts:
- name: www
mountPath: /usr/share/nginx/html
name: www
spec:
accessModes:
- ReadWriteOnce
resources:
requests:
storage: 1G
- 리소스 생성: kubectl apply -f sample-statefulset.yaml
- 확인: kubectl get pods
- 볼륨 클레임 확인: kubectl get persistentvolumeclaims
- persistemvolume 확인: kubectl get persistentvolumes
kubectl scale statefulset sample-statefulset --replicas=5
apiVersion: batch/v1
spec:
containers:
- name: pi
image: perl:5.34.0
command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"]
restartPolicy: Never
kubectl apply -f sample-job.yaml
kubectl get jobs
kubectl get pods --watch
kubectl logs pi
apiVersion: batch/v1
spec:
containers:
- name: errorjob-unvalidcommand
image: busybox
command: ["ls", "unvalid path"]
restartPolicy: Never
리소스 생성: kubectl apply -f sample-job-never-restart.yaml
파드 확인: kubectl get pods

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml
임의의 시점에 파드를 업데이트하는 경우는 데못셋과 연결된 파드를 kubectl delete pod 명령으로 수동으로 정지시키고 자동화된 복구 기능으로 새로운 파드를 생성해야 함
- 리소스를 위한 매니페스트 작성: sample-statefulset.yaml
apiVersion: apps/v1
name: sample-statefulset
serviceName: sample-statefulset
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
volumeMounts:
- name: www
mountPath: /usr/share/nginx/html
name: www
spec:
accessModes:
- ReadWriteOnce
resources:
requests:
storage: 1G
- 리소스 생성: kubectl apply -f sample-statefulset.yaml
- 확인: kubectl get pods
- 볼륨 클레임 확인: kubectl get persistentvolumeclaims
- persistemvolume 확인: kubectl get persistentvolumes
kubectl scale statefulset sample-statefulset --replicas=5
apiVersion: batch/v1
spec:
containers:
- name: pi
image: perl:5.34.0
command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"]
restartPolicy: Never
kubectl apply -f sample-job.yaml
kubectl get jobs
kubectl get pods --watch
kubectl logs pi
apiVersion: batch/v1
spec:
containers:
- name: errorjob-unvalidcommand
image: busybox
command: ["ls", "unvalid path"]
restartPolicy: Never
리소스 생성: kubectl apply -f sample-job-never-restart.yaml
파드 확인: kubectl get pods

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml
임의의 시점에 파드를 업데이트하는 경우는 데못셋과 연결된 파드를 kubectl delete pod 명령으로 수동으로 정지시키고 자동화된 복구 기능으로 새로운 파드를 생성해야 함
- 리소스를 위한 매니페스트 작성: sample-statefulset.yaml
apiVersion: apps/v1
name: sample-statefulset
serviceName: sample-statefulset
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
volumeMounts:
- name: www
mountPath: /usr/share/nginx/html
name: www
spec:
accessModes:
- ReadWriteOnce
resources:
requests:
storage: 1G
- 리소스 생성: kubectl apply -f sample-statefulset.yaml
- 확인: kubectl get pods
- 볼륨 클레임 확인: kubectl get persistentvolumeclaims
- persistemvolume 확인: kubectl get persistentvolumes
kubectl scale statefulset sample-statefulset --replicas=5
apiVersion: batch/v1
spec:
containers:
- name: pi
image: perl:5.34.0
command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"]
restartPolicy: Never
kubectl apply -f sample-job.yaml
kubectl get jobs
kubectl get pods --watch
kubectl logs pi
apiVersion: batch/v1
spec:
containers:
- name: errorjob-unvalidcommand
image: busybox
command: ["ls", "unvalid path"]
restartPolicy: Never
리소스 생성: kubectl apply -f sample-job-never-restart.yaml
파드 확인: kubectl get pods

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml
임의의 시점에 파드를 업데이트하는 경우는 데못셋과 연결된 파드를 kubectl delete pod 명령으로 수동으로 정지시키고 자동화된 복구 기능으로 새로운 파드를 생성해야 함
- 리소스를 위한 매니페스트 작성: sample-statefulset.yaml
apiVersion: apps/v1
name: sample-statefulset
serviceName: sample-statefulset
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
volumeMounts:
- name: www
mountPath: /usr/share/nginx/html
name: www
spec:
accessModes:
- ReadWriteOnce
resources:
requests:
storage: 1G
- 리소스 생성: kubectl apply -f sample-statefulset.yaml
- 확인: kubectl get pods
- 볼륨 클레임 확인: kubectl get persistentvolumeclaims
- persistemvolume 확인: kubectl get persistentvolumes
kubectl scale statefulset sample-statefulset --replicas=5
apiVersion: batch/v1
spec:
containers:
- name: pi
image: perl:5.34.0
command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"]
restartPolicy: Never
kubectl apply -f sample-job.yaml
kubectl get jobs
kubectl get pods --watch
kubectl logs pi
apiVersion: batch/v1
spec:
containers:
- name: errorjob-unvalidcommand
image: busybox
command: ["ls", "unvalid path"]
restartPolicy: Never
리소스 생성: kubectl apply -f sample-job-never-restart.yaml
파드 확인: kubectl get pods

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml
임의의 시점에 파드를 업데이트하는 경우는 데못셋과 연결된 파드를 kubectl delete pod 명령으로 수동으로 정지시키고 자동화된 복구 기능으로 새로운 파드를 생성해야 함
- 리소스를 위한 매니페스트 작성: sample-statefulset.yaml
apiVersion: apps/v1
name: sample-statefulset
serviceName: sample-statefulset
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
volumeMounts:
- name: www
mountPath: /usr/share/nginx/html
name: www
spec:
accessModes:
- ReadWriteOnce
resources:
requests:
storage: 1G
- 리소스 생성: kubectl apply -f sample-statefulset.yaml
- 확인: kubectl get pods
- 볼륨 클레임 확인: kubectl get persistentvolumeclaims
- persistemvolume 확인: kubectl get persistentvolumes
kubectl scale statefulset sample-statefulset --replicas=5
apiVersion: batch/v1
spec:
containers:
- name: pi
image: perl:5.34.0
command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"]
restartPolicy: Never
kubectl apply -f sample-job.yaml
kubectl get jobs
kubectl get pods --watch
kubectl logs pi
apiVersion: batch/v1
spec:
containers:
- name: errorjob-unvalidcommand
image: busybox
command: ["ls", "unvalid path"]
restartPolicy: Never
리소스 생성: kubectl apply -f sample-job-never-restart.yaml
파드 확인: kubectl get pods
apiVersion: batch/v1

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml
임의의 시점에 파드를 업데이트하는 경우는 데못셋과 연결된 파드를 kubectl delete pod 명령으로 수동으로 정지시키고 자동화된 복구 기능으로 새로운 파드를 생성해야 함
- 리소스를 위한 매니페스트 작성: sample-statefulset.yaml
apiVersion: apps/v1
name: sample-statefulset
serviceName: sample-statefulset
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
volumeMounts:
- name: www
mountPath: /usr/share/nginx/html
name: www
spec:
accessModes:
- ReadWriteOnce
resources:
requests:
storage: 1G
- 리소스 생성: kubectl apply -f sample-statefulset.yaml
- 확인: kubectl get pods
- 볼륨 클레임 확인: kubectl get persistentvolumeclaims
- persistemvolume 확인: kubectl get persistentvolumes
kubectl scale statefulset sample-statefulset --replicas=5
apiVersion: batch/v1
spec:
containers:
- name: pi
image: perl:5.34.0
command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"]
restartPolicy: Never
kubectl apply -f sample-job.yaml
kubectl get jobs
kubectl get pods --watch
kubectl logs pi
apiVersion: batch/v1
spec:
containers:
- name: errorjob-unvalidcommand
image: busybox
command: ["ls", "unvalid path"]
restartPolicy: Never
리소스 생성: kubectl apply -f sample-job-never-restart.yaml
파드 확인: kubectl get pods
apiVersion: batch/v1

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml
임의의 시점에 파드를 업데이트하는 경우는 데못셋과 연결된 파드를 kubectl delete pod 명령으로 수동으로 정지시키고 자동화된 복구 기능으로 새로운 파드를 생성해야 함
- 리소스를 위한 매니페스트 작성: sample-statefulset.yaml
apiVersion: apps/v1
name: sample-statefulset
serviceName: sample-statefulset
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
volumeMounts:
- name: www
mountPath: /usr/share/nginx/html
name: www
spec:
accessModes:
- ReadWriteOnce
resources:
requests:
storage: 1G
- 리소스 생성: kubectl apply -f sample-statefulset.yaml
- 확인: kubectl get pods
- 볼륨 클레임 확인: kubectl get persistentvolumeclaims
- persistemvolume 확인: kubectl get persistentvolumes
kubectl scale statefulset sample-statefulset --replicas=5
apiVersion: batch/v1
spec:
containers:
- name: pi
image: perl:5.34.0
command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"]
restartPolicy: Never
kubectl apply -f sample-job.yaml
kubectl get jobs
kubectl get pods --watch
kubectl logs pi
apiVersion: batch/v1
spec:
containers:
- name: errorjob-unvalidcommand
image: busybox
command: ["ls", "unvalid path"]
restartPolicy: Never
리소스 생성: kubectl apply -f sample-job-never-restart.yaml
파드 확인: kubectl get pods
apiVersion: batch/v1

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml
임의의 시점에 파드를 업데이트하는 경우는 데못셋과 연결된 파드를 kubectl delete pod 명령으로 수동으로 정지시키고 자동화된 복구 기능으로 새로운 파드를 생성해야 함
- 리소스를 위한 매니페스트 작성: sample-statefulset.yaml
apiVersion: apps/v1
name: sample-statefulset
serviceName: sample-statefulset
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
volumeMounts:
- name: www
mountPath: /usr/share/nginx/html
name: www
spec:
accessModes:
- ReadWriteOnce
resources:
requests:
storage: 1G
- 리소스 생성: kubectl apply -f sample-statefulset.yaml
- 확인: kubectl get pods
- 볼륨 클레임 확인: kubectl get persistentvolumeclaims
- persistemvolume 확인: kubectl get persistentvolumes
kubectl scale statefulset sample-statefulset --replicas=5
apiVersion: batch/v1
spec:
containers:
- name: pi
image: perl:5.34.0
command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"]
restartPolicy: Never
kubectl apply -f sample-job.yaml
kubectl get jobs
kubectl get pods --watch
kubectl logs pi
apiVersion: batch/v1
spec:
containers:
- name: errorjob-unvalidcommand
image: busybox
command: ["ls", "unvalid path"]
restartPolicy: Never
리소스 생성: kubectl apply -f sample-job-never-restart.yaml
파드 확인: kubectl get pods
apiVersion: batch/v1

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml
임의의 시점에 파드를 업데이트하는 경우는 데못셋과 연결된 파드를 kubectl delete pod 명령으로 수동으로 정지시키고 자동화된 복구 기능으로 새로운 파드를 생성해야 함
- 리소스를 위한 매니페스트 작성: sample-statefulset.yaml
apiVersion: apps/v1
name: sample-statefulset
serviceName: sample-statefulset
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
volumeMounts:
- name: www
mountPath: /usr/share/nginx/html
name: www
spec:
accessModes:
- ReadWriteOnce
resources:
requests:
storage: 1G
- 리소스 생성: kubectl apply -f sample-statefulset.yaml
- 확인: kubectl get pods
- 볼륨 클레임 확인: kubectl get persistentvolumeclaims
- persistemvolume 확인: kubectl get persistentvolumes
kubectl scale statefulset sample-statefulset --replicas=5
apiVersion: batch/v1
spec:
containers:
- name: pi
image: perl:5.34.0
command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"]
restartPolicy: Never
kubectl apply -f sample-job.yaml
kubectl get jobs
kubectl get pods --watch
kubectl logs pi
apiVersion: batch/v1
spec:
containers:
- name: errorjob-unvalidcommand
image: busybox
command: ["ls", "unvalid path"]
restartPolicy: Never
리소스 생성: kubectl apply -f sample-job-never-restart.yaml
파드 확인: kubectl get pods
apiVersion: batch/v1

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml
임의의 시점에 파드를 업데이트하는 경우는 데못셋과 연결된 파드를 kubectl delete pod 명령으로 수동으로 정지시키고 자동화된 복구 기능으로 새로운 파드를 생성해야 함
- 리소스를 위한 매니페스트 작성: sample-statefulset.yaml
apiVersion: apps/v1
name: sample-statefulset
serviceName: sample-statefulset
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
volumeMounts:
- name: www
mountPath: /usr/share/nginx/html
name: www
spec:
accessModes:
- ReadWriteOnce
resources:
requests:
storage: 1G
- 리소스 생성: kubectl apply -f sample-statefulset.yaml
- 확인: kubectl get pods
- 볼륨 클레임 확인: kubectl get persistentvolumeclaims
- persistemvolume 확인: kubectl get persistentvolumes
kubectl scale statefulset sample-statefulset --replicas=5
apiVersion: batch/v1
spec:
containers:
- name: pi
image: perl:5.34.0
command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"]
restartPolicy: Never
kubectl apply -f sample-job.yaml
kubectl get jobs
kubectl get pods --watch
kubectl logs pi
apiVersion: batch/v1
spec:
containers:
- name: errorjob-unvalidcommand
image: busybox
command: ["ls", "unvalid path"]
restartPolicy: Never
리소스 생성: kubectl apply -f sample-job-never-restart.yaml
파드 확인: kubectl get pods
apiVersion: batch/v1
spec:
containers:
- name: errorjob-unvalidcommand
image: busybox
command: ["ls", "unvalid path"]
restartPolicy: OnFailure

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml
임의의 시점에 파드를 업데이트하는 경우는 데못셋과 연결된 파드를 kubectl delete pod 명령으로 수동으로 정지시키고 자동화된 복구 기능으로 새로운 파드를 생성해야 함
- 리소스를 위한 매니페스트 작성: sample-statefulset.yaml
apiVersion: apps/v1
name: sample-statefulset
serviceName: sample-statefulset
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
volumeMounts:
- name: www
mountPath: /usr/share/nginx/html
name: www
spec:
accessModes:
- ReadWriteOnce
resources:
requests:
storage: 1G
- 리소스 생성: kubectl apply -f sample-statefulset.yaml
- 확인: kubectl get pods
- 볼륨 클레임 확인: kubectl get persistentvolumeclaims
- persistemvolume 확인: kubectl get persistentvolumes
kubectl scale statefulset sample-statefulset --replicas=5
apiVersion: batch/v1
spec:
containers:
- name: pi
image: perl:5.34.0
command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"]
restartPolicy: Never
kubectl apply -f sample-job.yaml
kubectl get jobs
kubectl get pods --watch
kubectl logs pi
apiVersion: batch/v1
spec:
containers:
- name: errorjob-unvalidcommand
image: busybox
command: ["ls", "unvalid path"]
restartPolicy: Never
리소스 생성: kubectl apply -f sample-job-never-restart.yaml
파드 확인: kubectl get pods
apiVersion: batch/v1
spec:
containers:
- name: errorjob-unvalidcommand
image: busybox
command: ["ls", "unvalid path"]
restartPolicy: OnFailure
리소스 생성: kubectl apply -fsample-job-onfailure-restart.yaml

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml
임의의 시점에 파드를 업데이트하는 경우는 데못셋과 연결된 파드를 kubectl delete pod 명령으로 수동으로 정지시키고 자동화된 복구 기능으로 새로운 파드를 생성해야 함
- 리소스를 위한 매니페스트 작성: sample-statefulset.yaml
apiVersion: apps/v1
name: sample-statefulset
serviceName: sample-statefulset
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
volumeMounts:
- name: www
mountPath: /usr/share/nginx/html
name: www
spec:
accessModes:
- ReadWriteOnce
resources:
requests:
storage: 1G
- 리소스 생성: kubectl apply -f sample-statefulset.yaml
- 확인: kubectl get pods
- 볼륨 클레임 확인: kubectl get persistentvolumeclaims
- persistemvolume 확인: kubectl get persistentvolumes
kubectl scale statefulset sample-statefulset --replicas=5
apiVersion: batch/v1
spec:
containers:
- name: pi
image: perl:5.34.0
command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"]
restartPolicy: Never
kubectl apply -f sample-job.yaml
kubectl get jobs
kubectl get pods --watch
kubectl logs pi
apiVersion: batch/v1
spec:
containers:
- name: errorjob-unvalidcommand
image: busybox
command: ["ls", "unvalid path"]
restartPolicy: Never
리소스 생성: kubectl apply -f sample-job-never-restart.yaml
파드 확인: kubectl get pods
apiVersion: batch/v1
spec:
containers:
- name: errorjob-unvalidcommand
image: busybox
command: ["ls", "unvalid path"]
restartPolicy: OnFailure
리소스 생성: kubectl apply -fsample-job-onfailure-restart.yaml
파드 확인: kubectl get pods --watch

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml
임의의 시점에 파드를 업데이트하는 경우는 데못셋과 연결된 파드를 kubectl delete pod 명령으로 수동으로 정지시키고 자동화된 복구 기능으로 새로운 파드를 생성해야 함
- 리소스를 위한 매니페스트 작성: sample-statefulset.yaml
apiVersion: apps/v1
name: sample-statefulset
serviceName: sample-statefulset
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
volumeMounts:
- name: www
mountPath: /usr/share/nginx/html
name: www
spec:
accessModes:
- ReadWriteOnce
resources:
requests:
storage: 1G
- 리소스 생성: kubectl apply -f sample-statefulset.yaml
- 확인: kubectl get pods
- 볼륨 클레임 확인: kubectl get persistentvolumeclaims
- persistemvolume 확인: kubectl get persistentvolumes
kubectl scale statefulset sample-statefulset --replicas=5
apiVersion: batch/v1
spec:
containers:
- name: pi
image: perl:5.34.0
command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"]
restartPolicy: Never
kubectl apply -f sample-job.yaml
kubectl get jobs
kubectl get pods --watch
kubectl logs pi
apiVersion: batch/v1
spec:
containers:
- name: errorjob-unvalidcommand
image: busybox
command: ["ls", "unvalid path"]
restartPolicy: Never
리소스 생성: kubectl apply -f sample-job-never-restart.yaml
파드 확인: kubectl get pods
apiVersion: batch/v1
spec:
containers:
- name: errorjob-unvalidcommand
image: busybox
command: ["ls", "unvalid path"]
restartPolicy: OnFailure
리소스 생성: kubectl apply -fsample-job-onfailure-restart.yaml
파드 확인: kubectl get pods --watch

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml
임의의 시점에 파드를 업데이트하는 경우는 데못셋과 연결된 파드를 kubectl delete pod 명령으로 수동으로 정지시키고 자동화된 복구 기능으로 새로운 파드를 생성해야 함
- 리소스를 위한 매니페스트 작성: sample-statefulset.yaml
apiVersion: apps/v1
name: sample-statefulset
serviceName: sample-statefulset
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
volumeMounts:
- name: www
mountPath: /usr/share/nginx/html
name: www
spec:
accessModes:
- ReadWriteOnce
resources:
requests:
storage: 1G
- 리소스 생성: kubectl apply -f sample-statefulset.yaml
- 확인: kubectl get pods
- 볼륨 클레임 확인: kubectl get persistentvolumeclaims
- persistemvolume 확인: kubectl get persistentvolumes
kubectl scale statefulset sample-statefulset --replicas=5
apiVersion: batch/v1
spec:
containers:
- name: pi
image: perl:5.34.0
command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"]
restartPolicy: Never
kubectl apply -f sample-job.yaml
kubectl get jobs
kubectl get pods --watch
kubectl logs pi
apiVersion: batch/v1
spec:
containers:
- name: errorjob-unvalidcommand
image: busybox
command: ["ls", "unvalid path"]
restartPolicy: Never
리소스 생성: kubectl apply -f sample-job-never-restart.yaml
파드 확인: kubectl get pods
apiVersion: batch/v1
spec:
containers:
- name: errorjob-unvalidcommand
image: busybox
command: ["ls", "unvalid path"]
restartPolicy: OnFailure
리소스 생성: kubectl apply -fsample-job-onfailure-restart.yaml
파드 확인: kubectl get pods --watch

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml
임의의 시점에 파드를 업데이트하는 경우는 데못셋과 연결된 파드를 kubectl delete pod 명령으로 수동으로 정지시키고 자동화된 복구 기능으로 새로운 파드를 생성해야 함
- 리소스를 위한 매니페스트 작성: sample-statefulset.yaml
apiVersion: apps/v1
name: sample-statefulset
serviceName: sample-statefulset
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
volumeMounts:
- name: www
mountPath: /usr/share/nginx/html
name: www
spec:
accessModes:
- ReadWriteOnce
resources:
requests:
storage: 1G
- 리소스 생성: kubectl apply -f sample-statefulset.yaml
- 확인: kubectl get pods
- 볼륨 클레임 확인: kubectl get persistentvolumeclaims
- persistemvolume 확인: kubectl get persistentvolumes
kubectl scale statefulset sample-statefulset --replicas=5
apiVersion: batch/v1
spec:
containers:
- name: pi
image: perl:5.34.0
command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"]
restartPolicy: Never
kubectl apply -f sample-job.yaml
kubectl get jobs
kubectl get pods --watch
kubectl logs pi
apiVersion: batch/v1
spec:
containers:
- name: errorjob-unvalidcommand
image: busybox
command: ["ls", "unvalid path"]
restartPolicy: Never
리소스 생성: kubectl apply -f sample-job-never-restart.yaml
파드 확인: kubectl get pods
apiVersion: batch/v1
spec:
containers:
- name: errorjob-unvalidcommand
image: busybox
command: ["ls", "unvalid path"]
restartPolicy: OnFailure
리소스 생성: kubectl apply -fsample-job-onfailure-restart.yaml
파드 확인: kubectl get pods --watch

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml
임의의 시점에 파드를 업데이트하는 경우는 데못셋과 연결된 파드를 kubectl delete pod 명령으로 수동으로 정지시키고 자동화된 복구 기능으로 새로운 파드를 생성해야 함
- 리소스를 위한 매니페스트 작성: sample-statefulset.yaml
apiVersion: apps/v1
name: sample-statefulset
serviceName: sample-statefulset
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
volumeMounts:
- name: www
mountPath: /usr/share/nginx/html
name: www
spec:
accessModes:
- ReadWriteOnce
resources:
requests:
storage: 1G
- 리소스 생성: kubectl apply -f sample-statefulset.yaml
- 확인: kubectl get pods
- 볼륨 클레임 확인: kubectl get persistentvolumeclaims
- persistemvolume 확인: kubectl get persistentvolumes
kubectl scale statefulset sample-statefulset --replicas=5
apiVersion: batch/v1
spec:
containers:
- name: pi
image: perl:5.34.0
command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"]
restartPolicy: Never
kubectl apply -f sample-job.yaml
kubectl get jobs
kubectl get pods --watch
kubectl logs pi
apiVersion: batch/v1
spec:
containers:
- name: errorjob-unvalidcommand
image: busybox
command: ["ls", "unvalid path"]
restartPolicy: Never
리소스 생성: kubectl apply -f sample-job-never-restart.yaml
파드 확인: kubectl get pods
apiVersion: batch/v1
spec:
containers:
- name: errorjob-unvalidcommand
image: busybox
command: ["ls", "unvalid path"]
restartPolicy: OnFailure
리소스 생성: kubectl apply -fsample-job-onfailure-restart.yaml
파드 확인: kubectl get pods --watch

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml
임의의 시점에 파드를 업데이트하는 경우는 데못셋과 연결된 파드를 kubectl delete pod 명령으로 수동으로 정지시키고 자동화된 복구 기능으로 새로운 파드를 생성해야 함
- 리소스를 위한 매니페스트 작성: sample-statefulset.yaml
apiVersion: apps/v1
name: sample-statefulset
serviceName: sample-statefulset
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
volumeMounts:
- name: www
mountPath: /usr/share/nginx/html
name: www
spec:
accessModes:
- ReadWriteOnce
resources:
requests:
storage: 1G
- 리소스 생성: kubectl apply -f sample-statefulset.yaml
- 확인: kubectl get pods
- 볼륨 클레임 확인: kubectl get persistentvolumeclaims
- persistemvolume 확인: kubectl get persistentvolumes
kubectl scale statefulset sample-statefulset --replicas=5
apiVersion: batch/v1
spec:
containers:
- name: pi
image: perl:5.34.0
command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"]
restartPolicy: Never
kubectl apply -f sample-job.yaml
kubectl get jobs
kubectl get pods --watch
kubectl logs pi
apiVersion: batch/v1
spec:
containers:
- name: errorjob-unvalidcommand
image: busybox
command: ["ls", "unvalid path"]
restartPolicy: Never
리소스 생성: kubectl apply -f sample-job-never-restart.yaml
파드 확인: kubectl get pods
apiVersion: batch/v1
spec:
containers:
- name: errorjob-unvalidcommand
image: busybox
command: ["ls", "unvalid path"]
restartPolicy: OnFailure
리소스 생성: kubectl apply -fsample-job-onfailure-restart.yaml
파드 확인: kubectl get pods --watch

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml
임의의 시점에 파드를 업데이트하는 경우는 데못셋과 연결된 파드를 kubectl delete pod 명령으로 수동으로 정지시키고 자동화된 복구 기능으로 새로운 파드를 생성해야 함
- 리소스를 위한 매니페스트 작성: sample-statefulset.yaml
apiVersion: apps/v1
name: sample-statefulset
serviceName: sample-statefulset
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
volumeMounts:
- name: www
mountPath: /usr/share/nginx/html
name: www
spec:
accessModes:
- ReadWriteOnce
resources:
requests:
storage: 1G
- 리소스 생성: kubectl apply -f sample-statefulset.yaml
- 확인: kubectl get pods
- 볼륨 클레임 확인: kubectl get persistentvolumeclaims
- persistemvolume 확인: kubectl get persistentvolumes
kubectl scale statefulset sample-statefulset --replicas=5
apiVersion: batch/v1
spec:
containers:
- name: pi
image: perl:5.34.0
command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"]
restartPolicy: Never
kubectl apply -f sample-job.yaml
kubectl get jobs
kubectl get pods --watch
kubectl logs pi
apiVersion: batch/v1
spec:
containers:
- name: errorjob-unvalidcommand
image: busybox
command: ["ls", "unvalid path"]
restartPolicy: Never
리소스 생성: kubectl apply -f sample-job-never-restart.yaml
파드 확인: kubectl get pods
apiVersion: batch/v1
spec:
containers:
- name: errorjob-unvalidcommand
image: busybox
command: ["ls", "unvalid path"]
restartPolicy: OnFailure
리소스 생성: kubectl apply -fsample-job-onfailure-restart.yaml
파드 확인: kubectl get pods --watch
completions	parallelism	backoffLimit

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml
임의의 시점에 파드를 업데이트하는 경우는 데못셋과 연결된 파드를 kubectl delete pod 명령으로 수동으로 정지시키고 자동화된 복구 기능으로 새로운 파드를 생성해야 함
- 리소스를 위한 매니페스트 작성: sample-statefulset.yaml
apiVersion: apps/v1
name: sample-statefulset
serviceName: sample-statefulset
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
volumeMounts:
- name: www
mountPath: /usr/share/nginx/html
name: www
spec:
accessModes:
- ReadWriteOnce
resources:
requests:
storage: 1G
- 리소스 생성: kubectl apply -f sample-statefulset.yaml
- 확인: kubectl get pods
- 볼륨 클레임 확인: kubectl get persistentvolumeclaims
- persistemvolume 확인: kubectl get persistentvolumes
kubectl scale statefulset sample-statefulset --replicas=5
apiVersion: batch/v1
spec:
containers:
- name: pi
image: perl:5.34.0
command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"]
restartPolicy: Never
kubectl apply -f sample-job.yaml
kubectl get jobs
kubectl get pods --watch
kubectl logs pi
apiVersion: batch/v1
spec:
containers:
- name: errorjob-unvalidcommand
image: busybox
command: ["ls", "unvalid path"]
restartPolicy: Never
리소스 생성: kubectl apply -f sample-job-never-restart.yaml
파드 확인: kubectl get pods
apiVersion: batch/v1
spec:
containers:
- name: errorjob-unvalidcommand
image: busybox
command: ["ls", "unvalid path"]
restartPolicy: OnFailure
리소스 생성: kubectl apply -fsample-job-onfailure-restart.yaml
파드 확인: kubectl get pods --watch
completions	parallelism	backoffLimit

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml
임의의 시점에 파드를 업데이트하는 경우는 데못셋과 연결된 파드를 kubectl delete pod 명령으로 수동으로 정지시키고 자동화된 복구 기능으로 새로운 파드를 생성해야 함
- 리소스를 위한 매니페스트 작성: sample-statefulset.yaml
apiVersion: apps/v1
name: sample-statefulset
serviceName: sample-statefulset
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
volumeMounts:
- name: www
mountPath: /usr/share/nginx/html
name: www
spec:
accessModes:
- ReadWriteOnce
resources:
requests:
storage: 1G
- 리소스 생성: kubectl apply -f sample-statefulset.yaml
- 확인: kubectl get pods
- 볼륨 클레임 확인: kubectl get persistentvolumeclaims
- persistemvolume 확인: kubectl get persistentvolumes
kubectl scale statefulset sample-statefulset --replicas=5
apiVersion: batch/v1
spec:
containers:
- name: pi
image: perl:5.34.0
command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"]
restartPolicy: Never
kubectl apply -f sample-job.yaml
kubectl get jobs
kubectl get pods --watch
kubectl logs pi
apiVersion: batch/v1
spec:
containers:
- name: errorjob-unvalidcommand
image: busybox
command: ["ls", "unvalid path"]
restartPolicy: Never
리소스 생성: kubectl apply -f sample-job-never-restart.yaml
파드 확인: kubectl get pods
apiVersion: batch/v1
spec:
containers:
- name: errorjob-unvalidcommand
image: busybox
command: ["ls", "unvalid path"]
restartPolicy: OnFailure
리소스 생성: kubectl apply -fsample-job-onfailure-restart.yaml
파드 확인: kubectl get pods --watch
completions	parallelism	backoffLimit

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml
임의의 시점에 파드를 업데이트하는 경우는 데못셋과 연결된 파드를 kubectl delete pod 명령으로 수동으로 정지시키고 자동화된 복구 기능으로 새로운 파드를 생성해야 함
- 리소스를 위한 매니페스트 작성: sample-statefulset.yaml
apiVersion: apps/v1
name: sample-statefulset
serviceName: sample-statefulset
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
volumeMounts:
- name: www
mountPath: /usr/share/nginx/html
name: www
spec:
accessModes:
- ReadWriteOnce
resources:
requests:
storage: 1G
- 리소스 생성: kubectl apply -f sample-statefulset.yaml
- 확인: kubectl get pods
- 볼륨 클레임 확인: kubectl get persistentvolumeclaims
- persistemvolume 확인: kubectl get persistentvolumes
kubectl scale statefulset sample-statefulset --replicas=5
apiVersion: batch/v1
spec:
containers:
- name: pi
image: perl:5.34.0
command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"]
restartPolicy: Never
kubectl apply -f sample-job.yaml
kubectl get jobs
kubectl get pods --watch
kubectl logs pi
apiVersion: batch/v1
spec:
containers:
- name: errorjob-unvalidcommand
image: busybox
command: ["ls", "unvalid path"]
restartPolicy: Never
리소스 생성: kubectl apply -f sample-job-never-restart.yaml
파드 확인: kubectl get pods
apiVersion: batch/v1
spec:
containers:
- name: errorjob-unvalidcommand
image: busybox
command: ["ls", "unvalid path"]
restartPolicy: OnFailure
리소스 생성: kubectl apply -fsample-job-onfailure-restart.yaml
파드 확인: kubectl get pods --watch
completions	parallelism	backoffLimit

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml
임의의 시점에 파드를 업데이트하는 경우는 데못셋과 연결된 파드를 kubectl delete pod 명령으로 수동으로 정지시키고 자동화된 복구 기능으로 새로운 파드를 생성해야 함
- 리소스를 위한 매니페스트 작성: sample-statefulset.yaml
apiVersion: apps/v1
name: sample-statefulset
serviceName: sample-statefulset
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
volumeMounts:
- name: www
mountPath: /usr/share/nginx/html
name: www
spec:
accessModes:
- ReadWriteOnce
resources:
requests:
storage: 1G
- 리소스 생성: kubectl apply -f sample-statefulset.yaml
- 확인: kubectl get pods
- 볼륨 클레임 확인: kubectl get persistentvolumeclaims
- persistemvolume 확인: kubectl get persistentvolumes
kubectl scale statefulset sample-statefulset --replicas=5
apiVersion: batch/v1
spec:
containers:
- name: pi
image: perl:5.34.0
command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"]
restartPolicy: Never
kubectl apply -f sample-job.yaml
kubectl get jobs
kubectl get pods --watch
kubectl logs pi
apiVersion: batch/v1
spec:
containers:
- name: errorjob-unvalidcommand
image: busybox
command: ["ls", "unvalid path"]
restartPolicy: Never
리소스 생성: kubectl apply -f sample-job-never-restart.yaml
파드 확인: kubectl get pods
apiVersion: batch/v1
spec:
containers:
- name: errorjob-unvalidcommand
image: busybox
command: ["ls", "unvalid path"]
restartPolicy: OnFailure
리소스 생성: kubectl apply -fsample-job-onfailure-restart.yaml
파드 확인: kubectl get pods --watch
completions	parallelism	backoffLimit

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml
임의의 시점에 파드를 업데이트하는 경우는 데못셋과 연결된 파드를 kubectl delete pod 명령으로 수동으로 정지시키고 자동화된 복구 기능으로 새로운 파드를 생성해야 함
- 리소스를 위한 매니페스트 작성: sample-statefulset.yaml
apiVersion: apps/v1
name: sample-statefulset
serviceName: sample-statefulset
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
volumeMounts:
- name: www
mountPath: /usr/share/nginx/html
name: www
spec:
accessModes:
- ReadWriteOnce
resources:
requests:
storage: 1G
- 리소스 생성: kubectl apply -f sample-statefulset.yaml
- 확인: kubectl get pods
- 볼륨 클레임 확인: kubectl get persistentvolumeclaims
- persistemvolume 확인: kubectl get persistentvolumes
kubectl scale statefulset sample-statefulset --replicas=5
apiVersion: batch/v1
spec:
containers:
- name: pi
image: perl:5.34.0
command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"]
restartPolicy: Never
kubectl apply -f sample-job.yaml
kubectl get jobs
kubectl get pods --watch
kubectl logs pi
apiVersion: batch/v1
spec:
containers:
- name: errorjob-unvalidcommand
image: busybox
command: ["ls", "unvalid path"]
restartPolicy: Never
리소스 생성: kubectl apply -f sample-job-never-restart.yaml
파드 확인: kubectl get pods
apiVersion: batch/v1
spec:
containers:
- name: errorjob-unvalidcommand
image: busybox
command: ["ls", "unvalid path"]
restartPolicy: OnFailure
리소스 생성: kubectl apply -fsample-job-onfailure-restart.yaml
파드 확인: kubectl get pods --watch
completions	parallelism	backoffLimit

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml
임의의 시점에 파드를 업데이트하는 경우는 데못셋과 연결된 파드를 kubectl delete pod 명령으로 수동으로 정지시키고 자동화된 복구 기능으로 새로운 파드를 생성해야 함
- 리소스를 위한 매니페스트 작성: sample-statefulset.yaml
apiVersion: apps/v1
name: sample-statefulset
serviceName: sample-statefulset
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
volumeMounts:
- name: www
mountPath: /usr/share/nginx/html
name: www
spec:
accessModes:
- ReadWriteOnce
resources:
requests:
storage: 1G
- 리소스 생성: kubectl apply -f sample-statefulset.yaml
- 확인: kubectl get pods
- 볼륨 클레임 확인: kubectl get persistentvolumeclaims
- persistemvolume 확인: kubectl get persistentvolumes
kubectl scale statefulset sample-statefulset --replicas=5
apiVersion: batch/v1
spec:
containers:
- name: pi
image: perl:5.34.0
command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"]
restartPolicy: Never
kubectl apply -f sample-job.yaml
kubectl get jobs
kubectl get pods --watch
kubectl logs pi
apiVersion: batch/v1
spec:
containers:
- name: errorjob-unvalidcommand
image: busybox
command: ["ls", "unvalid path"]
restartPolicy: Never
리소스 생성: kubectl apply -f sample-job-never-restart.yaml
파드 확인: kubectl get pods
apiVersion: batch/v1
spec:
containers:
- name: errorjob-unvalidcommand
image: busybox
command: ["ls", "unvalid path"]
restartPolicy: OnFailure
리소스 생성: kubectl apply -fsample-job-onfailure-restart.yaml
파드 확인: kubectl get pods --watch
completions	parallelism	backoffLimit

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml
임의의 시점에 파드를 업데이트하는 경우는 데못셋과 연결된 파드를 kubectl delete pod 명령으로 수동으로 정지시키고 자동화된 복구 기능으로 새로운 파드를 생성해야 함
- 리소스를 위한 매니페스트 작성: sample-statefulset.yaml
apiVersion: apps/v1
name: sample-statefulset
serviceName: sample-statefulset
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
volumeMounts:
- name: www
mountPath: /usr/share/nginx/html
name: www
spec:
accessModes:
- ReadWriteOnce
resources:
requests:
storage: 1G
- 리소스 생성: kubectl apply -f sample-statefulset.yaml
- 확인: kubectl get pods
- 볼륨 클레임 확인: kubectl get persistentvolumeclaims
- persistemvolume 확인: kubectl get persistentvolumes
kubectl scale statefulset sample-statefulset --replicas=5
apiVersion: batch/v1
spec:
containers:
- name: pi
image: perl:5.34.0
command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"]
restartPolicy: Never
kubectl apply -f sample-job.yaml
kubectl get jobs
kubectl get pods --watch
kubectl logs pi
apiVersion: batch/v1
spec:
containers:
- name: errorjob-unvalidcommand
image: busybox
command: ["ls", "unvalid path"]
restartPolicy: Never
리소스 생성: kubectl apply -f sample-job-never-restart.yaml
파드 확인: kubectl get pods
apiVersion: batch/v1
spec:
containers:
- name: errorjob-unvalidcommand
image: busybox
command: ["ls", "unvalid path"]
restartPolicy: OnFailure
리소스 생성: kubectl apply -fsample-job-onfailure-restart.yaml
파드 확인: kubectl get pods --watch
completions	parallelism	backoffLimit
apiVersion: batch/v1

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml
임의의 시점에 파드를 업데이트하는 경우는 데못셋과 연결된 파드를 kubectl delete pod 명령으로 수동으로 정지시키고 자동화된 복구 기능으로 새로운 파드를 생성해야 함
- 리소스를 위한 매니페스트 작성: sample-statefulset.yaml
apiVersion: apps/v1
name: sample-statefulset
serviceName: sample-statefulset
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
volumeMounts:
- name: www
mountPath: /usr/share/nginx/html
name: www
spec:
accessModes:
- ReadWriteOnce
resources:
requests:
storage: 1G
- 리소스 생성: kubectl apply -f sample-statefulset.yaml
- 확인: kubectl get pods
- 볼륨 클레임 확인: kubectl get persistentvolumeclaims
- persistemvolume 확인: kubectl get persistentvolumes
kubectl scale statefulset sample-statefulset --replicas=5
apiVersion: batch/v1
spec:
containers:
- name: pi
image: perl:5.34.0
command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"]
restartPolicy: Never
kubectl apply -f sample-job.yaml
kubectl get jobs
kubectl get pods --watch
kubectl logs pi
apiVersion: batch/v1
spec:
containers:
- name: errorjob-unvalidcommand
image: busybox
command: ["ls", "unvalid path"]
restartPolicy: Never
리소스 생성: kubectl apply -f sample-job-never-restart.yaml
파드 확인: kubectl get pods
apiVersion: batch/v1
spec:
containers:
- name: errorjob-unvalidcommand
image: busybox
command: ["ls", "unvalid path"]
restartPolicy: OnFailure
리소스 생성: kubectl apply -fsample-job-onfailure-restart.yaml
파드 확인: kubectl get pods --watch
completions	parallelism	backoffLimit
apiVersion: batch/v1

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml
임의의 시점에 파드를 업데이트하는 경우는 데못셋과 연결된 파드를 kubectl delete pod 명령으로 수동으로 정지시키고 자동화된 복구 기능으로 새로운 파드를 생성해야 함
- 리소스를 위한 매니페스트 작성: sample-statefulset.yaml
apiVersion: apps/v1
name: sample-statefulset
serviceName: sample-statefulset
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
volumeMounts:
- name: www
mountPath: /usr/share/nginx/html
name: www
spec:
accessModes:
- ReadWriteOnce
resources:
requests:
storage: 1G
- 리소스 생성: kubectl apply -f sample-statefulset.yaml
- 확인: kubectl get pods
- 볼륨 클레임 확인: kubectl get persistentvolumeclaims
- persistemvolume 확인: kubectl get persistentvolumes
kubectl scale statefulset sample-statefulset --replicas=5
apiVersion: batch/v1
spec:
containers:
- name: pi
image: perl:5.34.0
command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"]
restartPolicy: Never
kubectl apply -f sample-job.yaml
kubectl get jobs
kubectl get pods --watch
kubectl logs pi
apiVersion: batch/v1
spec:
containers:
- name: errorjob-unvalidcommand
image: busybox
command: ["ls", "unvalid path"]
restartPolicy: Never
리소스 생성: kubectl apply -f sample-job-never-restart.yaml
파드 확인: kubectl get pods
apiVersion: batch/v1
spec:
containers:
- name: errorjob-unvalidcommand
image: busybox
command: ["ls", "unvalid path"]
restartPolicy: OnFailure
리소스 생성: kubectl apply -fsample-job-onfailure-restart.yaml
파드 확인: kubectl get pods --watch
completions	parallelism	backoffLimit
apiVersion: batch/v1

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml
임의의 시점에 파드를 업데이트하는 경우는 데못셋과 연결된 파드를 kubectl delete pod 명령으로 수동으로 정지시키고 자동화된 복구 기능으로 새로운 파드를 생성해야 함
- 리소스를 위한 매니페스트 작성: sample-statefulset.yaml
apiVersion: apps/v1
name: sample-statefulset
serviceName: sample-statefulset
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
volumeMounts:
- name: www
mountPath: /usr/share/nginx/html
name: www
spec:
accessModes:
- ReadWriteOnce
resources:
requests:
storage: 1G
- 리소스 생성: kubectl apply -f sample-statefulset.yaml
- 확인: kubectl get pods
- 볼륨 클레임 확인: kubectl get persistentvolumeclaims
- persistemvolume 확인: kubectl get persistentvolumes
kubectl scale statefulset sample-statefulset --replicas=5
apiVersion: batch/v1
spec:
containers:
- name: pi
image: perl:5.34.0
command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"]
restartPolicy: Never
kubectl apply -f sample-job.yaml
kubectl get jobs
kubectl get pods --watch
kubectl logs pi
apiVersion: batch/v1
spec:
containers:
- name: errorjob-unvalidcommand
image: busybox
command: ["ls", "unvalid path"]
restartPolicy: Never
리소스 생성: kubectl apply -f sample-job-never-restart.yaml
파드 확인: kubectl get pods
apiVersion: batch/v1
spec:
containers:
- name: errorjob-unvalidcommand
image: busybox
command: ["ls", "unvalid path"]
restartPolicy: OnFailure
리소스 생성: kubectl apply -fsample-job-onfailure-restart.yaml
파드 확인: kubectl get pods --watch
completions	parallelism	backoffLimit
apiVersion: batch/v1

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml
임의의 시점에 파드를 업데이트하는 경우는 데못셋과 연결된 파드를 kubectl delete pod 명령으로 수동으로 정지시키고 자동화된 복구 기능으로 새로운 파드를 생성해야 함
- 리소스를 위한 매니페스트 작성: sample-statefulset.yaml
apiVersion: apps/v1
name: sample-statefulset
serviceName: sample-statefulset
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
volumeMounts:
- name: www
mountPath: /usr/share/nginx/html
name: www
spec:
accessModes:
- ReadWriteOnce
resources:
requests:
storage: 1G
- 리소스 생성: kubectl apply -f sample-statefulset.yaml
- 확인: kubectl get pods
- 볼륨 클레임 확인: kubectl get persistentvolumeclaims
- persistemvolume 확인: kubectl get persistentvolumes
kubectl scale statefulset sample-statefulset --replicas=5
apiVersion: batch/v1
spec:
containers:
- name: pi
image: perl:5.34.0
command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"]
restartPolicy: Never
kubectl apply -f sample-job.yaml
kubectl get jobs
kubectl get pods --watch
kubectl logs pi
apiVersion: batch/v1
spec:
containers:
- name: errorjob-unvalidcommand
image: busybox
command: ["ls", "unvalid path"]
restartPolicy: Never
리소스 생성: kubectl apply -f sample-job-never-restart.yaml
파드 확인: kubectl get pods
apiVersion: batch/v1
spec:
containers:
- name: errorjob-unvalidcommand
image: busybox
command: ["ls", "unvalid path"]
restartPolicy: OnFailure
리소스 생성: kubectl apply -fsample-job-onfailure-restart.yaml
파드 확인: kubectl get pods --watch
completions	parallelism	backoffLimit
apiVersion: batch/v1

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml
임의의 시점에 파드를 업데이트하는 경우는 데못셋과 연결된 파드를 kubectl delete pod 명령으로 수동으로 정지시키고 자동화된 복구 기능으로 새로운 파드를 생성해야 함
- 리소스를 위한 매니페스트 작성: sample-statefulset.yaml
apiVersion: apps/v1
name: sample-statefulset
serviceName: sample-statefulset
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
volumeMounts:
- name: www
mountPath: /usr/share/nginx/html
name: www
spec:
accessModes:
- ReadWriteOnce
resources:
requests:
storage: 1G
- 리소스 생성: kubectl apply -f sample-statefulset.yaml
- 확인: kubectl get pods
- 볼륨 클레임 확인: kubectl get persistentvolumeclaims
- persistemvolume 확인: kubectl get persistentvolumes
kubectl scale statefulset sample-statefulset --replicas=5
apiVersion: batch/v1
spec:
containers:
- name: pi
image: perl:5.34.0
command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"]
restartPolicy: Never
kubectl apply -f sample-job.yaml
kubectl get jobs
kubectl get pods --watch
kubectl logs pi
apiVersion: batch/v1
spec:
containers:
- name: errorjob-unvalidcommand
image: busybox
command: ["ls", "unvalid path"]
restartPolicy: Never
리소스 생성: kubectl apply -f sample-job-never-restart.yaml
파드 확인: kubectl get pods
apiVersion: batch/v1
spec:
containers:
- name: errorjob-unvalidcommand
image: busybox
command: ["ls", "unvalid path"]
restartPolicy: OnFailure
리소스 생성: kubectl apply -fsample-job-onfailure-restart.yaml
파드 확인: kubectl get pods --watch
completions	parallelism	backoffLimit
apiVersion: batch/v1

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml
임의의 시점에 파드를 업데이트하는 경우는 데못셋과 연결된 파드를 kubectl delete pod 명령으로 수동으로 정지시키고 자동화된 복구 기능으로 새로운 파드를 생성해야 함
- 리소스를 위한 매니페스트 작성: sample-statefulset.yaml
apiVersion: apps/v1
name: sample-statefulset
serviceName: sample-statefulset
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
volumeMounts:
- name: www
mountPath: /usr/share/nginx/html
name: www
spec:
accessModes:
- ReadWriteOnce
resources:
requests:
storage: 1G
- 리소스 생성: kubectl apply -f sample-statefulset.yaml
- 확인: kubectl get pods
- 볼륨 클레임 확인: kubectl get persistentvolumeclaims
- persistemvolume 확인: kubectl get persistentvolumes
kubectl scale statefulset sample-statefulset --replicas=5
apiVersion: batch/v1
spec:
containers:
- name: pi
image: perl:5.34.0
command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"]
restartPolicy: Never
kubectl apply -f sample-job.yaml
kubectl get jobs
kubectl get pods --watch
kubectl logs pi
apiVersion: batch/v1
spec:
containers:
- name: errorjob-unvalidcommand
image: busybox
command: ["ls", "unvalid path"]
restartPolicy: Never
리소스 생성: kubectl apply -f sample-job-never-restart.yaml
파드 확인: kubectl get pods
apiVersion: batch/v1
spec:
containers:
- name: errorjob-unvalidcommand
image: busybox
command: ["ls", "unvalid path"]
restartPolicy: OnFailure
리소스 생성: kubectl apply -fsample-job-onfailure-restart.yaml
파드 확인: kubectl get pods --watch
completions	parallelism	backoffLimit
apiVersion: batch/v1
spec:
containers:
- name: ttl
image: perl:5.34.0
command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"]
restartPolicy: Never

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml
임의의 시점에 파드를 업데이트하는 경우는 데못셋과 연결된 파드를 kubectl delete pod 명령으로 수동으로 정지시키고 자동화된 복구 기능으로 새로운 파드를 생성해야 함
- 리소스를 위한 매니페스트 작성: sample-statefulset.yaml
apiVersion: apps/v1
name: sample-statefulset
serviceName: sample-statefulset
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
volumeMounts:
- name: www
mountPath: /usr/share/nginx/html
name: www
spec:
accessModes:
- ReadWriteOnce
resources:
requests:
storage: 1G
- 리소스 생성: kubectl apply -f sample-statefulset.yaml
- 확인: kubectl get pods
- 볼륨 클레임 확인: kubectl get persistentvolumeclaims
- persistemvolume 확인: kubectl get persistentvolumes
kubectl scale statefulset sample-statefulset --replicas=5
apiVersion: batch/v1
spec:
containers:
- name: pi
image: perl:5.34.0
command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"]
restartPolicy: Never
kubectl apply -f sample-job.yaml
kubectl get jobs
kubectl get pods --watch
kubectl logs pi
apiVersion: batch/v1
spec:
containers:
- name: errorjob-unvalidcommand
image: busybox
command: ["ls", "unvalid path"]
restartPolicy: Never
리소스 생성: kubectl apply -f sample-job-never-restart.yaml
파드 확인: kubectl get pods
apiVersion: batch/v1
spec:
containers:
- name: errorjob-unvalidcommand
image: busybox
command: ["ls", "unvalid path"]
restartPolicy: OnFailure
리소스 생성: kubectl apply -fsample-job-onfailure-restart.yaml
파드 확인: kubectl get pods --watch
completions	parallelism	backoffLimit
apiVersion: batch/v1
spec:
containers:
- name: ttl
image: perl:5.34.0
command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"]
restartPolicy: Never

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml
임의의 시점에 파드를 업데이트하는 경우는 데못셋과 연결된 파드를 kubectl delete pod 명령으로 수동으로 정지시키고 자동화된 복구 기능으로 새로운 파드를 생성해야 함
- 리소스를 위한 매니페스트 작성: sample-statefulset.yaml
apiVersion: apps/v1
name: sample-statefulset
serviceName: sample-statefulset
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
volumeMounts:
- name: www
mountPath: /usr/share/nginx/html
name: www
spec:
accessModes:
- ReadWriteOnce
resources:
requests:
storage: 1G
- 리소스 생성: kubectl apply -f sample-statefulset.yaml
- 확인: kubectl get pods
- 볼륨 클레임 확인: kubectl get persistentvolumeclaims
- persistemvolume 확인: kubectl get persistentvolumes
kubectl scale statefulset sample-statefulset --replicas=5
apiVersion: batch/v1
spec:
containers:
- name: pi
image: perl:5.34.0
command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"]
restartPolicy: Never
kubectl apply -f sample-job.yaml
kubectl get jobs
kubectl get pods --watch
kubectl logs pi
apiVersion: batch/v1
spec:
containers:
- name: errorjob-unvalidcommand
image: busybox
command: ["ls", "unvalid path"]
restartPolicy: Never
리소스 생성: kubectl apply -f sample-job-never-restart.yaml
파드 확인: kubectl get pods
apiVersion: batch/v1
spec:
containers:
- name: errorjob-unvalidcommand
image: busybox
command: ["ls", "unvalid path"]
restartPolicy: OnFailure
리소스 생성: kubectl apply -fsample-job-onfailure-restart.yaml
파드 확인: kubectl get pods --watch
completions	parallelism	backoffLimit
apiVersion: batch/v1
spec:
containers:
- name: ttl
image: perl:5.34.0
command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"]
restartPolicy: Never

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml
임의의 시점에 파드를 업데이트하는 경우는 데못셋과 연결된 파드를 kubectl delete pod 명령으로 수동으로 정지시키고 자동화된 복구 기능으로 새로운 파드를 생성해야 함
- 리소스를 위한 매니페스트 작성: sample-statefulset.yaml
apiVersion: apps/v1
name: sample-statefulset
serviceName: sample-statefulset
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
volumeMounts:
- name: www
mountPath: /usr/share/nginx/html
name: www
spec:
accessModes:
- ReadWriteOnce
resources:
requests:
storage: 1G
- 리소스 생성: kubectl apply -f sample-statefulset.yaml
- 확인: kubectl get pods
- 볼륨 클레임 확인: kubectl get persistentvolumeclaims
- persistemvolume 확인: kubectl get persistentvolumes
kubectl scale statefulset sample-statefulset --replicas=5
apiVersion: batch/v1
spec:
containers:
- name: pi
image: perl:5.34.0
command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"]
restartPolicy: Never
kubectl apply -f sample-job.yaml
kubectl get jobs
kubectl get pods --watch
kubectl logs pi
apiVersion: batch/v1
spec:
containers:
- name: errorjob-unvalidcommand
image: busybox
command: ["ls", "unvalid path"]
restartPolicy: Never
리소스 생성: kubectl apply -f sample-job-never-restart.yaml
파드 확인: kubectl get pods
apiVersion: batch/v1
spec:
containers:
- name: errorjob-unvalidcommand
image: busybox
command: ["ls", "unvalid path"]
restartPolicy: OnFailure
리소스 생성: kubectl apply -fsample-job-onfailure-restart.yaml
파드 확인: kubectl get pods --watch
completions	parallelism	backoffLimit
apiVersion: batch/v1
spec:
containers:
- name: ttl
image: perl:5.34.0
command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"]
restartPolicy: Never

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml
임의의 시점에 파드를 업데이트하는 경우는 데못셋과 연결된 파드를 kubectl delete pod 명령으로 수동으로 정지시키고 자동화된 복구 기능으로 새로운 파드를 생성해야 함
- 리소스를 위한 매니페스트 작성: sample-statefulset.yaml
apiVersion: apps/v1
name: sample-statefulset
serviceName: sample-statefulset
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
volumeMounts:
- name: www
mountPath: /usr/share/nginx/html
name: www
spec:
accessModes:
- ReadWriteOnce
resources:
requests:
storage: 1G
- 리소스 생성: kubectl apply -f sample-statefulset.yaml
- 확인: kubectl get pods
- 볼륨 클레임 확인: kubectl get persistentvolumeclaims
- persistemvolume 확인: kubectl get persistentvolumes
kubectl scale statefulset sample-statefulset --replicas=5
apiVersion: batch/v1
spec:
containers:
- name: pi
image: perl:5.34.0
command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"]
restartPolicy: Never
kubectl apply -f sample-job.yaml
kubectl get jobs
kubectl get pods --watch
kubectl logs pi
apiVersion: batch/v1
spec:
containers:
- name: errorjob-unvalidcommand
image: busybox
command: ["ls", "unvalid path"]
restartPolicy: Never
리소스 생성: kubectl apply -f sample-job-never-restart.yaml
파드 확인: kubectl get pods
apiVersion: batch/v1
spec:
containers:
- name: errorjob-unvalidcommand
image: busybox
command: ["ls", "unvalid path"]
restartPolicy: OnFailure
리소스 생성: kubectl apply -fsample-job-onfailure-restart.yaml
파드 확인: kubectl get pods --watch
completions	parallelism	backoffLimit
apiVersion: batch/v1
spec:
containers:
- name: ttl
image: perl:5.34.0
command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"]
restartPolicy: Never

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml
임의의 시점에 파드를 업데이트하는 경우는 데못셋과 연결된 파드를 kubectl delete pod 명령으로 수동으로 정지시키고 자동화된 복구 기능으로 새로운 파드를 생성해야 함
- 리소스를 위한 매니페스트 작성: sample-statefulset.yaml
apiVersion: apps/v1
name: sample-statefulset
serviceName: sample-statefulset
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
volumeMounts:
- name: www
mountPath: /usr/share/nginx/html
name: www
spec:
accessModes:
- ReadWriteOnce
resources:
requests:
storage: 1G
- 리소스 생성: kubectl apply -f sample-statefulset.yaml
- 확인: kubectl get pods
- 볼륨 클레임 확인: kubectl get persistentvolumeclaims
- persistemvolume 확인: kubectl get persistentvolumes
kubectl scale statefulset sample-statefulset --replicas=5
apiVersion: batch/v1
spec:
containers:
- name: pi
image: perl:5.34.0
command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"]
restartPolicy: Never
kubectl apply -f sample-job.yaml
kubectl get jobs
kubectl get pods --watch
kubectl logs pi
apiVersion: batch/v1
spec:
containers:
- name: errorjob-unvalidcommand
image: busybox
command: ["ls", "unvalid path"]
restartPolicy: Never
리소스 생성: kubectl apply -f sample-job-never-restart.yaml
파드 확인: kubectl get pods
apiVersion: batch/v1
spec:
containers:
- name: errorjob-unvalidcommand
image: busybox
command: ["ls", "unvalid path"]
restartPolicy: OnFailure
리소스 생성: kubectl apply -fsample-job-onfailure-restart.yaml
파드 확인: kubectl get pods --watch
completions	parallelism	backoffLimit
apiVersion: batch/v1
spec:
containers:
- name: ttl
image: perl:5.34.0
command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"]
restartPolicy: Never

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml
임의의 시점에 파드를 업데이트하는 경우는 데못셋과 연결된 파드를 kubectl delete pod 명령으로 수동으로 정지시키고 자동화된 복구 기능으로 새로운 파드를 생성해야 함
- 리소스를 위한 매니페스트 작성: sample-statefulset.yaml
apiVersion: apps/v1
name: sample-statefulset
serviceName: sample-statefulset
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
volumeMounts:
- name: www
mountPath: /usr/share/nginx/html
name: www
spec:
accessModes:
- ReadWriteOnce
resources:
requests:
storage: 1G
- 리소스 생성: kubectl apply -f sample-statefulset.yaml
- 확인: kubectl get pods
- 볼륨 클레임 확인: kubectl get persistentvolumeclaims
- persistemvolume 확인: kubectl get persistentvolumes
kubectl scale statefulset sample-statefulset --replicas=5
apiVersion: batch/v1
spec:
containers:
- name: pi
image: perl:5.34.0
command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"]
restartPolicy: Never
kubectl apply -f sample-job.yaml
kubectl get jobs
kubectl get pods --watch
kubectl logs pi
apiVersion: batch/v1
spec:
containers:
- name: errorjob-unvalidcommand
image: busybox
command: ["ls", "unvalid path"]
restartPolicy: Never
리소스 생성: kubectl apply -f sample-job-never-restart.yaml
파드 확인: kubectl get pods
apiVersion: batch/v1
spec:
containers:
- name: errorjob-unvalidcommand
image: busybox
command: ["ls", "unvalid path"]
restartPolicy: OnFailure
리소스 생성: kubectl apply -fsample-job-onfailure-restart.yaml
파드 확인: kubectl get pods --watch
completions	parallelism	backoffLimit
apiVersion: batch/v1
spec:
containers:
- name: ttl
image: perl:5.34.0
command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"]
restartPolicy: Never

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml
임의의 시점에 파드를 업데이트하는 경우는 데못셋과 연결된 파드를 kubectl delete pod 명령으로 수동으로 정지시키고 자동화된 복구 기능으로 새로운 파드를 생성해야 함
- 리소스를 위한 매니페스트 작성: sample-statefulset.yaml
apiVersion: apps/v1
name: sample-statefulset
serviceName: sample-statefulset
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
volumeMounts:
- name: www
mountPath: /usr/share/nginx/html
name: www
spec:
accessModes:
- ReadWriteOnce
resources:
requests:
storage: 1G
- 리소스 생성: kubectl apply -f sample-statefulset.yaml
- 확인: kubectl get pods
- 볼륨 클레임 확인: kubectl get persistentvolumeclaims
- persistemvolume 확인: kubectl get persistentvolumes
kubectl scale statefulset sample-statefulset --replicas=5
apiVersion: batch/v1
spec:
containers:
- name: pi
image: perl:5.34.0
command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"]
restartPolicy: Never
kubectl apply -f sample-job.yaml
kubectl get jobs
kubectl get pods --watch
kubectl logs pi
apiVersion: batch/v1
spec:
containers:
- name: errorjob-unvalidcommand
image: busybox
command: ["ls", "unvalid path"]
restartPolicy: Never
리소스 생성: kubectl apply -f sample-job-never-restart.yaml
파드 확인: kubectl get pods
apiVersion: batch/v1
spec:
containers:
- name: errorjob-unvalidcommand
image: busybox
command: ["ls", "unvalid path"]
restartPolicy: OnFailure
리소스 생성: kubectl apply -fsample-job-onfailure-restart.yaml
파드 확인: kubectl get pods --watch
completions	parallelism	backoffLimit
apiVersion: batch/v1
spec:
containers:
- name: ttl
image: perl:5.34.0
command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"]
restartPolicy: Never

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml
임의의 시점에 파드를 업데이트하는 경우는 데못셋과 연결된 파드를 kubectl delete pod 명령으로 수동으로 정지시키고 자동화된 복구 기능으로 새로운 파드를 생성해야 함
- 리소스를 위한 매니페스트 작성: sample-statefulset.yaml
apiVersion: apps/v1
name: sample-statefulset
serviceName: sample-statefulset
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
volumeMounts:
- name: www
mountPath: /usr/share/nginx/html
name: www
spec:
accessModes:
- ReadWriteOnce
resources:
requests:
storage: 1G
- 리소스 생성: kubectl apply -f sample-statefulset.yaml
- 확인: kubectl get pods
- 볼륨 클레임 확인: kubectl get persistentvolumeclaims
- persistemvolume 확인: kubectl get persistentvolumes
kubectl scale statefulset sample-statefulset --replicas=5
apiVersion: batch/v1
spec:
containers:
- name: pi
image: perl:5.34.0
command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"]
restartPolicy: Never
kubectl apply -f sample-job.yaml
kubectl get jobs
kubectl get pods --watch
kubectl logs pi
apiVersion: batch/v1
spec:
containers:
- name: errorjob-unvalidcommand
image: busybox
command: ["ls", "unvalid path"]
restartPolicy: Never
리소스 생성: kubectl apply -f sample-job-never-restart.yaml
파드 확인: kubectl get pods
apiVersion: batch/v1
spec:
containers:
- name: errorjob-unvalidcommand
image: busybox
command: ["ls", "unvalid path"]
restartPolicy: OnFailure
리소스 생성: kubectl apply -fsample-job-onfailure-restart.yaml
파드 확인: kubectl get pods --watch
completions	parallelism	backoffLimit
apiVersion: batch/v1
spec:
containers:
- name: ttl
image: perl:5.34.0
command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"]
restartPolicy: Never
apiVersion: batch/v1

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml
임의의 시점에 파드를 업데이트하는 경우는 데못셋과 연결된 파드를 kubectl delete pod 명령으로 수동으로 정지시키고 자동화된 복구 기능으로 새로운 파드를 생성해야 함
- 리소스를 위한 매니페스트 작성: sample-statefulset.yaml
apiVersion: apps/v1
name: sample-statefulset
serviceName: sample-statefulset
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
volumeMounts:
- name: www
mountPath: /usr/share/nginx/html
name: www
spec:
accessModes:
- ReadWriteOnce
resources:
requests:
storage: 1G
- 리소스 생성: kubectl apply -f sample-statefulset.yaml
- 확인: kubectl get pods
- 볼륨 클레임 확인: kubectl get persistentvolumeclaims
- persistemvolume 확인: kubectl get persistentvolumes
kubectl scale statefulset sample-statefulset --replicas=5
apiVersion: batch/v1
spec:
containers:
- name: pi
image: perl:5.34.0
command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"]
restartPolicy: Never
kubectl apply -f sample-job.yaml
kubectl get jobs
kubectl get pods --watch
kubectl logs pi
apiVersion: batch/v1
spec:
containers:
- name: errorjob-unvalidcommand
image: busybox
command: ["ls", "unvalid path"]
restartPolicy: Never
리소스 생성: kubectl apply -f sample-job-never-restart.yaml
파드 확인: kubectl get pods
apiVersion: batch/v1
spec:
containers:
- name: errorjob-unvalidcommand
image: busybox
command: ["ls", "unvalid path"]
restartPolicy: OnFailure
리소스 생성: kubectl apply -fsample-job-onfailure-restart.yaml
파드 확인: kubectl get pods --watch
completions	parallelism	backoffLimit
apiVersion: batch/v1
spec:
containers:
- name: ttl
image: perl:5.34.0
command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"]
restartPolicy: Never
apiVersion: batch/v1

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml
임의의 시점에 파드를 업데이트하는 경우는 데못셋과 연결된 파드를 kubectl delete pod 명령으로 수동으로 정지시키고 자동화된 복구 기능으로 새로운 파드를 생성해야 함
- 리소스를 위한 매니페스트 작성: sample-statefulset.yaml
apiVersion: apps/v1
name: sample-statefulset
serviceName: sample-statefulset
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
volumeMounts:
- name: www
mountPath: /usr/share/nginx/html
name: www
spec:
accessModes:
- ReadWriteOnce
resources:
requests:
storage: 1G
- 리소스 생성: kubectl apply -f sample-statefulset.yaml
- 확인: kubectl get pods
- 볼륨 클레임 확인: kubectl get persistentvolumeclaims
- persistemvolume 확인: kubectl get persistentvolumes
kubectl scale statefulset sample-statefulset --replicas=5
apiVersion: batch/v1
spec:
containers:
- name: pi
image: perl:5.34.0
command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"]
restartPolicy: Never
kubectl apply -f sample-job.yaml
kubectl get jobs
kubectl get pods --watch
kubectl logs pi
apiVersion: batch/v1
spec:
containers:
- name: errorjob-unvalidcommand
image: busybox
command: ["ls", "unvalid path"]
restartPolicy: Never
리소스 생성: kubectl apply -f sample-job-never-restart.yaml
파드 확인: kubectl get pods
apiVersion: batch/v1
spec:
containers:
- name: errorjob-unvalidcommand
image: busybox
command: ["ls", "unvalid path"]
restartPolicy: OnFailure
리소스 생성: kubectl apply -fsample-job-onfailure-restart.yaml
파드 확인: kubectl get pods --watch
completions	parallelism	backoffLimit
apiVersion: batch/v1
spec:
containers:
- name: ttl
image: perl:5.34.0
command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"]
restartPolicy: Never
apiVersion: batch/v1

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml
임의의 시점에 파드를 업데이트하는 경우는 데못셋과 연결된 파드를 kubectl delete pod 명령으로 수동으로 정지시키고 자동화된 복구 기능으로 새로운 파드를 생성해야 함
- 리소스를 위한 매니페스트 작성: sample-statefulset.yaml
apiVersion: apps/v1
name: sample-statefulset
serviceName: sample-statefulset
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
volumeMounts:
- name: www
mountPath: /usr/share/nginx/html
name: www
spec:
accessModes:
- ReadWriteOnce
resources:
requests:
storage: 1G
- 리소스 생성: kubectl apply -f sample-statefulset.yaml
- 확인: kubectl get pods
- 볼륨 클레임 확인: kubectl get persistentvolumeclaims
- persistemvolume 확인: kubectl get persistentvolumes
kubectl scale statefulset sample-statefulset --replicas=5
apiVersion: batch/v1
spec:
containers:
- name: pi
image: perl:5.34.0
command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"]
restartPolicy: Never
kubectl apply -f sample-job.yaml
kubectl get jobs
kubectl get pods --watch
kubectl logs pi
apiVersion: batch/v1
spec:
containers:
- name: errorjob-unvalidcommand
image: busybox
command: ["ls", "unvalid path"]
restartPolicy: Never
리소스 생성: kubectl apply -f sample-job-never-restart.yaml
파드 확인: kubectl get pods
apiVersion: batch/v1
spec:
containers:
- name: errorjob-unvalidcommand
image: busybox
command: ["ls", "unvalid path"]
restartPolicy: OnFailure
리소스 생성: kubectl apply -fsample-job-onfailure-restart.yaml
파드 확인: kubectl get pods --watch
completions	parallelism	backoffLimit
apiVersion: batch/v1
spec:
containers:
- name: ttl
image: perl:5.34.0
command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"]
restartPolicy: Never
apiVersion: batch/v1

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml
임의의 시점에 파드를 업데이트하는 경우는 데못셋과 연결된 파드를 kubectl delete pod 명령으로 수동으로 정지시키고 자동화된 복구 기능으로 새로운 파드를 생성해야 함
- 리소스를 위한 매니페스트 작성: sample-statefulset.yaml
apiVersion: apps/v1
name: sample-statefulset
serviceName: sample-statefulset
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
volumeMounts:
- name: www
mountPath: /usr/share/nginx/html
name: www
spec:
accessModes:
- ReadWriteOnce
resources:
requests:
storage: 1G
- 리소스 생성: kubectl apply -f sample-statefulset.yaml
- 확인: kubectl get pods
- 볼륨 클레임 확인: kubectl get persistentvolumeclaims
- persistemvolume 확인: kubectl get persistentvolumes
kubectl scale statefulset sample-statefulset --replicas=5
apiVersion: batch/v1
spec:
containers:
- name: pi
image: perl:5.34.0
command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"]
restartPolicy: Never
kubectl apply -f sample-job.yaml
kubectl get jobs
kubectl get pods --watch
kubectl logs pi
apiVersion: batch/v1
spec:
containers:
- name: errorjob-unvalidcommand
image: busybox
command: ["ls", "unvalid path"]
restartPolicy: Never
리소스 생성: kubectl apply -f sample-job-never-restart.yaml
파드 확인: kubectl get pods
apiVersion: batch/v1
spec:
containers:
- name: errorjob-unvalidcommand
image: busybox
command: ["ls", "unvalid path"]
restartPolicy: OnFailure
리소스 생성: kubectl apply -fsample-job-onfailure-restart.yaml
파드 확인: kubectl get pods --watch
completions	parallelism	backoffLimit
apiVersion: batch/v1
spec:
containers:
- name: ttl
image: perl:5.34.0
command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"]
restartPolicy: Never
apiVersion: batch/v1

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml
임의의 시점에 파드를 업데이트하는 경우는 데못셋과 연결된 파드를 kubectl delete pod 명령으로 수동으로 정지시키고 자동화된 복구 기능으로 새로운 파드를 생성해야 함
- 리소스를 위한 매니페스트 작성: sample-statefulset.yaml
apiVersion: apps/v1
name: sample-statefulset
serviceName: sample-statefulset
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
volumeMounts:
- name: www
mountPath: /usr/share/nginx/html
name: www
spec:
accessModes:
- ReadWriteOnce
resources:
requests:
storage: 1G
- 리소스 생성: kubectl apply -f sample-statefulset.yaml
- 확인: kubectl get pods
- 볼륨 클레임 확인: kubectl get persistentvolumeclaims
- persistemvolume 확인: kubectl get persistentvolumes
kubectl scale statefulset sample-statefulset --replicas=5
apiVersion: batch/v1
spec:
containers:
- name: pi
image: perl:5.34.0
command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"]
restartPolicy: Never
kubectl apply -f sample-job.yaml
kubectl get jobs
kubectl get pods --watch
kubectl logs pi
apiVersion: batch/v1
spec:
containers:
- name: errorjob-unvalidcommand
image: busybox
command: ["ls", "unvalid path"]
restartPolicy: Never
리소스 생성: kubectl apply -f sample-job-never-restart.yaml
파드 확인: kubectl get pods
apiVersion: batch/v1
spec:
containers:
- name: errorjob-unvalidcommand
image: busybox
command: ["ls", "unvalid path"]
restartPolicy: OnFailure
리소스 생성: kubectl apply -fsample-job-onfailure-restart.yaml
파드 확인: kubectl get pods --watch
completions	parallelism	backoffLimit
apiVersion: batch/v1
spec:
containers:
- name: ttl
image: perl:5.34.0
command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"]
restartPolicy: Never
apiVersion: batch/v1

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml
임의의 시점에 파드를 업데이트하는 경우는 데못셋과 연결된 파드를 kubectl delete pod 명령으로 수동으로 정지시키고 자동화된 복구 기능으로 새로운 파드를 생성해야 함
- 리소스를 위한 매니페스트 작성: sample-statefulset.yaml
apiVersion: apps/v1
name: sample-statefulset
serviceName: sample-statefulset
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
volumeMounts:
- name: www
mountPath: /usr/share/nginx/html
name: www
spec:
accessModes:
- ReadWriteOnce
resources:
requests:
storage: 1G
- 리소스 생성: kubectl apply -f sample-statefulset.yaml
- 확인: kubectl get pods
- 볼륨 클레임 확인: kubectl get persistentvolumeclaims
- persistemvolume 확인: kubectl get persistentvolumes
kubectl scale statefulset sample-statefulset --replicas=5
apiVersion: batch/v1
spec:
containers:
- name: pi
image: perl:5.34.0
command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"]
restartPolicy: Never
kubectl apply -f sample-job.yaml
kubectl get jobs
kubectl get pods --watch
kubectl logs pi
apiVersion: batch/v1
spec:
containers:
- name: errorjob-unvalidcommand
image: busybox
command: ["ls", "unvalid path"]
restartPolicy: Never
리소스 생성: kubectl apply -f sample-job-never-restart.yaml
파드 확인: kubectl get pods
apiVersion: batch/v1
spec:
containers:
- name: errorjob-unvalidcommand
image: busybox
command: ["ls", "unvalid path"]
restartPolicy: OnFailure
리소스 생성: kubectl apply -fsample-job-onfailure-restart.yaml
파드 확인: kubectl get pods --watch
completions	parallelism	backoffLimit
apiVersion: batch/v1
spec:
containers:
- name: ttl
image: perl:5.34.0
command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"]
restartPolicy: Never
apiVersion: batch/v1
spec:
template:
spec:
containers:
- name: name
image: busybox
imagePullPolicy: IfNotPresent
command:
- /bin/sh
- -c
- date; echo Hello from Kubernetes Cluster
restartPolicy: OnFailure

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml
임의의 시점에 파드를 업데이트하는 경우는 데못셋과 연결된 파드를 kubectl delete pod 명령으로 수동으로 정지시키고 자동화된 복구 기능으로 새로운 파드를 생성해야 함
- 리소스를 위한 매니페스트 작성: sample-statefulset.yaml
apiVersion: apps/v1
name: sample-statefulset
serviceName: sample-statefulset
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
volumeMounts:
- name: www
mountPath: /usr/share/nginx/html
name: www
spec:
accessModes:
- ReadWriteOnce
resources:
requests:
storage: 1G
- 리소스 생성: kubectl apply -f sample-statefulset.yaml
- 확인: kubectl get pods
- 볼륨 클레임 확인: kubectl get persistentvolumeclaims
- persistemvolume 확인: kubectl get persistentvolumes
kubectl scale statefulset sample-statefulset --replicas=5
apiVersion: batch/v1
spec:
containers:
- name: pi
image: perl:5.34.0
command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"]
restartPolicy: Never
kubectl apply -f sample-job.yaml
kubectl get jobs
kubectl get pods --watch
kubectl logs pi
apiVersion: batch/v1
spec:
containers:
- name: errorjob-unvalidcommand
image: busybox
command: ["ls", "unvalid path"]
restartPolicy: Never
리소스 생성: kubectl apply -f sample-job-never-restart.yaml
파드 확인: kubectl get pods
apiVersion: batch/v1
spec:
containers:
- name: errorjob-unvalidcommand
image: busybox
command: ["ls", "unvalid path"]
restartPolicy: OnFailure
리소스 생성: kubectl apply -fsample-job-onfailure-restart.yaml
파드 확인: kubectl get pods --watch
completions	parallelism	backoffLimit
apiVersion: batch/v1
spec:
containers:
- name: ttl
image: perl:5.34.0
command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"]
restartPolicy: Never
apiVersion: batch/v1
spec:
template:
spec:
containers:
- name: name
image: busybox
imagePullPolicy: IfNotPresent
command:
- /bin/sh
- -c
- date; echo Hello from Kubernetes Cluster
restartPolicy: OnFailure

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml
임의의 시점에 파드를 업데이트하는 경우는 데못셋과 연결된 파드를 kubectl delete pod 명령으로 수동으로 정지시키고 자동화된 복구 기능으로 새로운 파드를 생성해야 함
- 리소스를 위한 매니페스트 작성: sample-statefulset.yaml
apiVersion: apps/v1
name: sample-statefulset
serviceName: sample-statefulset
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
volumeMounts:
- name: www
mountPath: /usr/share/nginx/html
name: www
spec:
accessModes:
- ReadWriteOnce
resources:
requests:
storage: 1G
- 리소스 생성: kubectl apply -f sample-statefulset.yaml
- 확인: kubectl get pods
- 볼륨 클레임 확인: kubectl get persistentvolumeclaims
- persistemvolume 확인: kubectl get persistentvolumes
kubectl scale statefulset sample-statefulset --replicas=5
apiVersion: batch/v1
spec:
containers:
- name: pi
image: perl:5.34.0
command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"]
restartPolicy: Never
kubectl apply -f sample-job.yaml
kubectl get jobs
kubectl get pods --watch
kubectl logs pi
apiVersion: batch/v1
spec:
containers:
- name: errorjob-unvalidcommand
image: busybox
command: ["ls", "unvalid path"]
restartPolicy: Never
리소스 생성: kubectl apply -f sample-job-never-restart.yaml
파드 확인: kubectl get pods
apiVersion: batch/v1
spec:
containers:
- name: errorjob-unvalidcommand
image: busybox
command: ["ls", "unvalid path"]
restartPolicy: OnFailure
리소스 생성: kubectl apply -fsample-job-onfailure-restart.yaml
파드 확인: kubectl get pods --watch
completions	parallelism	backoffLimit
apiVersion: batch/v1
spec:
containers:
- name: ttl
image: perl:5.34.0
command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"]
restartPolicy: Never
apiVersion: batch/v1
spec:
template:
spec:
containers:
- name: name
image: busybox
imagePullPolicy: IfNotPresent
command:
- /bin/sh
- -c
- date; echo Hello from Kubernetes Cluster
restartPolicy: OnFailure

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml
임의의 시점에 파드를 업데이트하는 경우는 데못셋과 연결된 파드를 kubectl delete pod 명령으로 수동으로 정지시키고 자동화된 복구 기능으로 새로운 파드를 생성해야 함
- 리소스를 위한 매니페스트 작성: sample-statefulset.yaml
apiVersion: apps/v1
name: sample-statefulset
serviceName: sample-statefulset
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
volumeMounts:
- name: www
mountPath: /usr/share/nginx/html
name: www
spec:
accessModes:
- ReadWriteOnce
resources:
requests:
storage: 1G
- 리소스 생성: kubectl apply -f sample-statefulset.yaml
- 확인: kubectl get pods
- 볼륨 클레임 확인: kubectl get persistentvolumeclaims
- persistemvolume 확인: kubectl get persistentvolumes
kubectl scale statefulset sample-statefulset --replicas=5
apiVersion: batch/v1
spec:
containers:
- name: pi
image: perl:5.34.0
command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"]
restartPolicy: Never
kubectl apply -f sample-job.yaml
kubectl get jobs
kubectl get pods --watch
kubectl logs pi
apiVersion: batch/v1
spec:
containers:
- name: errorjob-unvalidcommand
image: busybox
command: ["ls", "unvalid path"]
restartPolicy: Never
리소스 생성: kubectl apply -f sample-job-never-restart.yaml
파드 확인: kubectl get pods
apiVersion: batch/v1
spec:
containers:
- name: errorjob-unvalidcommand
image: busybox
command: ["ls", "unvalid path"]
restartPolicy: OnFailure
리소스 생성: kubectl apply -fsample-job-onfailure-restart.yaml
파드 확인: kubectl get pods --watch
completions	parallelism	backoffLimit
apiVersion: batch/v1
spec:
containers:
- name: ttl
image: perl:5.34.0
command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"]
restartPolicy: Never
apiVersion: batch/v1
spec:
template:
spec:
containers:
- name: name
image: busybox
imagePullPolicy: IfNotPresent
command:
- /bin/sh
- -c
- date; echo Hello from Kubernetes Cluster
restartPolicy: OnFailure

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml
임의의 시점에 파드를 업데이트하는 경우는 데못셋과 연결된 파드를 kubectl delete pod 명령으로 수동으로 정지시키고 자동화된 복구 기능으로 새로운 파드를 생성해야 함
- 리소스를 위한 매니페스트 작성: sample-statefulset.yaml
apiVersion: apps/v1
name: sample-statefulset
serviceName: sample-statefulset
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
volumeMounts:
- name: www
mountPath: /usr/share/nginx/html
name: www
spec:
accessModes:
- ReadWriteOnce
resources:
requests:
storage: 1G
- 리소스 생성: kubectl apply -f sample-statefulset.yaml
- 확인: kubectl get pods
- 볼륨 클레임 확인: kubectl get persistentvolumeclaims
- persistemvolume 확인: kubectl get persistentvolumes
kubectl scale statefulset sample-statefulset --replicas=5
apiVersion: batch/v1
spec:
containers:
- name: pi
image: perl:5.34.0
command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"]
restartPolicy: Never
kubectl apply -f sample-job.yaml
kubectl get jobs
kubectl get pods --watch
kubectl logs pi
apiVersion: batch/v1
spec:
containers:
- name: errorjob-unvalidcommand
image: busybox
command: ["ls", "unvalid path"]
restartPolicy: Never
리소스 생성: kubectl apply -f sample-job-never-restart.yaml
파드 확인: kubectl get pods
apiVersion: batch/v1
spec:
containers:
- name: errorjob-unvalidcommand
image: busybox
command: ["ls", "unvalid path"]
restartPolicy: OnFailure
리소스 생성: kubectl apply -fsample-job-onfailure-restart.yaml
파드 확인: kubectl get pods --watch
completions	parallelism	backoffLimit
apiVersion: batch/v1
spec:
containers:
- name: ttl
image: perl:5.34.0
command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"]
restartPolicy: Never
apiVersion: batch/v1
spec:
template:
spec:
containers:
- name: name
image: busybox
imagePullPolicy: IfNotPresent
command:
- /bin/sh
- -c
- date; echo Hello from Kubernetes Cluster
restartPolicy: OnFailure

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml
임의의 시점에 파드를 업데이트하는 경우는 데못셋과 연결된 파드를 kubectl delete pod 명령으로 수동으로 정지시키고 자동화된 복구 기능으로 새로운 파드를 생성해야 함
- 리소스를 위한 매니페스트 작성: sample-statefulset.yaml
apiVersion: apps/v1
name: sample-statefulset
serviceName: sample-statefulset
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
volumeMounts:
- name: www
mountPath: /usr/share/nginx/html
name: www
spec:
accessModes:
- ReadWriteOnce
resources:
requests:
storage: 1G
- 리소스 생성: kubectl apply -f sample-statefulset.yaml
- 확인: kubectl get pods
- 볼륨 클레임 확인: kubectl get persistentvolumeclaims
- persistemvolume 확인: kubectl get persistentvolumes
kubectl scale statefulset sample-statefulset --replicas=5
apiVersion: batch/v1
spec:
containers:
- name: pi
image: perl:5.34.0
command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"]
restartPolicy: Never
kubectl apply -f sample-job.yaml
kubectl get jobs
kubectl get pods --watch
kubectl logs pi
apiVersion: batch/v1
spec:
containers:
- name: errorjob-unvalidcommand
image: busybox
command: ["ls", "unvalid path"]
restartPolicy: Never
리소스 생성: kubectl apply -f sample-job-never-restart.yaml
파드 확인: kubectl get pods
apiVersion: batch/v1
spec:
containers:
- name: errorjob-unvalidcommand
image: busybox
command: ["ls", "unvalid path"]
restartPolicy: OnFailure
리소스 생성: kubectl apply -fsample-job-onfailure-restart.yaml
파드 확인: kubectl get pods --watch
completions	parallelism	backoffLimit
apiVersion: batch/v1
spec:
containers:
- name: ttl
image: perl:5.34.0
command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"]
restartPolicy: Never
apiVersion: batch/v1
spec:
template:
spec:
containers:
- name: name
image: busybox
imagePullPolicy: IfNotPresent
command:
- /bin/sh
- -c
- date; echo Hello from Kubernetes Cluster
restartPolicy: OnFailure

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml
임의의 시점에 파드를 업데이트하는 경우는 데못셋과 연결된 파드를 kubectl delete pod 명령으로 수동으로 정지시키고 자동화된 복구 기능으로 새로운 파드를 생성해야 함
- 리소스를 위한 매니페스트 작성: sample-statefulset.yaml
apiVersion: apps/v1
name: sample-statefulset
serviceName: sample-statefulset
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
volumeMounts:
- name: www
mountPath: /usr/share/nginx/html
name: www
spec:
accessModes:
- ReadWriteOnce
resources:
requests:
storage: 1G
- 리소스 생성: kubectl apply -f sample-statefulset.yaml
- 확인: kubectl get pods
- 볼륨 클레임 확인: kubectl get persistentvolumeclaims
- persistemvolume 확인: kubectl get persistentvolumes
kubectl scale statefulset sample-statefulset --replicas=5
apiVersion: batch/v1
spec:
containers:
- name: pi
image: perl:5.34.0
command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"]
restartPolicy: Never
kubectl apply -f sample-job.yaml
kubectl get jobs
kubectl get pods --watch
kubectl logs pi
apiVersion: batch/v1
spec:
containers:
- name: errorjob-unvalidcommand
image: busybox
command: ["ls", "unvalid path"]
restartPolicy: Never
리소스 생성: kubectl apply -f sample-job-never-restart.yaml
파드 확인: kubectl get pods
apiVersion: batch/v1
spec:
containers:
- name: errorjob-unvalidcommand
image: busybox
command: ["ls", "unvalid path"]
restartPolicy: OnFailure
리소스 생성: kubectl apply -fsample-job-onfailure-restart.yaml
파드 확인: kubectl get pods --watch
completions	parallelism	backoffLimit
apiVersion: batch/v1
spec:
containers:
- name: ttl
image: perl:5.34.0
command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"]
restartPolicy: Never
apiVersion: batch/v1
spec:
template:
spec:
containers:
- name: name
image: busybox
imagePullPolicy: IfNotPresent
command:
- /bin/sh
- -c
- date; echo Hello from Kubernetes Cluster
restartPolicy: OnFailure

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml
임의의 시점에 파드를 업데이트하는 경우는 데못셋과 연결된 파드를 kubectl delete pod 명령으로 수동으로 정지시키고 자동화된 복구 기능으로 새로운 파드를 생성해야 함
- 리소스를 위한 매니페스트 작성: sample-statefulset.yaml
apiVersion: apps/v1
name: sample-statefulset
serviceName: sample-statefulset
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
volumeMounts:
- name: www
mountPath: /usr/share/nginx/html
name: www
spec:
accessModes:
- ReadWriteOnce
resources:
requests:
storage: 1G
- 리소스 생성: kubectl apply -f sample-statefulset.yaml
- 확인: kubectl get pods
- 볼륨 클레임 확인: kubectl get persistentvolumeclaims
- persistemvolume 확인: kubectl get persistentvolumes
kubectl scale statefulset sample-statefulset --replicas=5
apiVersion: batch/v1
spec:
containers:
- name: pi
image: perl:5.34.0
command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"]
restartPolicy: Never
kubectl apply -f sample-job.yaml
kubectl get jobs
kubectl get pods --watch
kubectl logs pi
apiVersion: batch/v1
spec:
containers:
- name: errorjob-unvalidcommand
image: busybox
command: ["ls", "unvalid path"]
restartPolicy: Never
리소스 생성: kubectl apply -f sample-job-never-restart.yaml
파드 확인: kubectl get pods
apiVersion: batch/v1
spec:
containers:
- name: errorjob-unvalidcommand
image: busybox
command: ["ls", "unvalid path"]
restartPolicy: OnFailure
리소스 생성: kubectl apply -fsample-job-onfailure-restart.yaml
파드 확인: kubectl get pods --watch
completions	parallelism	backoffLimit
apiVersion: batch/v1
spec:
containers:
- name: ttl
image: perl:5.34.0
command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"]
restartPolicy: Never
apiVersion: batch/v1
spec:
template:
spec:
containers:
- name: name
image: busybox
imagePullPolicy: IfNotPresent
command:
- /bin/sh
- -c
- date; echo Hello from Kubernetes Cluster
restartPolicy: OnFailure

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml
임의의 시점에 파드를 업데이트하는 경우는 데못셋과 연결된 파드를 kubectl delete pod 명령으로 수동으로 정지시키고 자동화된 복구 기능으로 새로운 파드를 생성해야 함
- 리소스를 위한 매니페스트 작성: sample-statefulset.yaml
apiVersion: apps/v1
name: sample-statefulset
serviceName: sample-statefulset
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
volumeMounts:
- name: www
mountPath: /usr/share/nginx/html
name: www
spec:
accessModes:
- ReadWriteOnce
resources:
requests:
storage: 1G
- 리소스 생성: kubectl apply -f sample-statefulset.yaml
- 확인: kubectl get pods
- 볼륨 클레임 확인: kubectl get persistentvolumeclaims
- persistemvolume 확인: kubectl get persistentvolumes
kubectl scale statefulset sample-statefulset --replicas=5
apiVersion: batch/v1
spec:
containers:
- name: pi
image: perl:5.34.0
command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"]
restartPolicy: Never
kubectl apply -f sample-job.yaml
kubectl get jobs
kubectl get pods --watch
kubectl logs pi
apiVersion: batch/v1
spec:
containers:
- name: errorjob-unvalidcommand
image: busybox
command: ["ls", "unvalid path"]
restartPolicy: Never
리소스 생성: kubectl apply -f sample-job-never-restart.yaml
파드 확인: kubectl get pods
apiVersion: batch/v1
spec:
containers:
- name: errorjob-unvalidcommand
image: busybox
command: ["ls", "unvalid path"]
restartPolicy: OnFailure
리소스 생성: kubectl apply -fsample-job-onfailure-restart.yaml
파드 확인: kubectl get pods --watch
completions	parallelism	backoffLimit
apiVersion: batch/v1
spec:
containers:
- name: ttl
image: perl:5.34.0
command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"]
restartPolicy: Never
apiVersion: batch/v1
spec:
template:
spec:
containers:
- name: name
image: busybox
imagePullPolicy: IfNotPresent
command:
- /bin/sh
- -c
- date; echo Hello from Kubernetes Cluster
restartPolicy: OnFailure

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml
임의의 시점에 파드를 업데이트하는 경우는 데못셋과 연결된 파드를 kubectl delete pod 명령으로 수동으로 정지시키고 자동화된 복구 기능으로 새로운 파드를 생성해야 함
- 리소스를 위한 매니페스트 작성: sample-statefulset.yaml
apiVersion: apps/v1
name: sample-statefulset
serviceName: sample-statefulset
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
volumeMounts:
- name: www
mountPath: /usr/share/nginx/html
name: www
spec:
accessModes:
- ReadWriteOnce
resources:
requests:
storage: 1G
- 리소스 생성: kubectl apply -f sample-statefulset.yaml
- 확인: kubectl get pods
- 볼륨 클레임 확인: kubectl get persistentvolumeclaims
- persistemvolume 확인: kubectl get persistentvolumes
kubectl scale statefulset sample-statefulset --replicas=5
apiVersion: batch/v1
spec:
containers:
- name: pi
image: perl:5.34.0
command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"]
restartPolicy: Never
kubectl apply -f sample-job.yaml
kubectl get jobs
kubectl get pods --watch
kubectl logs pi
apiVersion: batch/v1
spec:
containers:
- name: errorjob-unvalidcommand
image: busybox
command: ["ls", "unvalid path"]
restartPolicy: Never
리소스 생성: kubectl apply -f sample-job-never-restart.yaml
파드 확인: kubectl get pods
apiVersion: batch/v1
spec:
containers:
- name: errorjob-unvalidcommand
image: busybox
command: ["ls", "unvalid path"]
restartPolicy: OnFailure
리소스 생성: kubectl apply -fsample-job-onfailure-restart.yaml
파드 확인: kubectl get pods --watch
completions	parallelism	backoffLimit
apiVersion: batch/v1
spec:
containers:
- name: ttl
image: perl:5.34.0
command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"]
restartPolicy: Never
apiVersion: batch/v1
spec:
template:
spec:
containers:
- name: name
image: busybox
imagePullPolicy: IfNotPresent
command:
- /bin/sh
- -c
- date; echo Hello from Kubernetes Cluster
restartPolicy: OnFailure

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml
임의의 시점에 파드를 업데이트하는 경우는 데못셋과 연결된 파드를 kubectl delete pod 명령으로 수동으로 정지시키고 자동화된 복구 기능으로 새로운 파드를 생성해야 함
- 리소스를 위한 매니페스트 작성: sample-statefulset.yaml
apiVersion: apps/v1
name: sample-statefulset
serviceName: sample-statefulset
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
volumeMounts:
- name: www
mountPath: /usr/share/nginx/html
name: www
spec:
accessModes:
- ReadWriteOnce
resources:
requests:
storage: 1G
- 리소스 생성: kubectl apply -f sample-statefulset.yaml
- 확인: kubectl get pods
- 볼륨 클레임 확인: kubectl get persistentvolumeclaims
- persistemvolume 확인: kubectl get persistentvolumes
kubectl scale statefulset sample-statefulset --replicas=5
apiVersion: batch/v1
spec:
containers:
- name: pi
image: perl:5.34.0
command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"]
restartPolicy: Never
kubectl apply -f sample-job.yaml
kubectl get jobs
kubectl get pods --watch
kubectl logs pi
apiVersion: batch/v1
spec:
containers:
- name: errorjob-unvalidcommand
image: busybox
command: ["ls", "unvalid path"]
restartPolicy: Never
리소스 생성: kubectl apply -f sample-job-never-restart.yaml
파드 확인: kubectl get pods
apiVersion: batch/v1
spec:
containers:
- name: errorjob-unvalidcommand
image: busybox
command: ["ls", "unvalid path"]
restartPolicy: OnFailure
리소스 생성: kubectl apply -fsample-job-onfailure-restart.yaml
파드 확인: kubectl get pods --watch
completions	parallelism	backoffLimit
apiVersion: batch/v1
spec:
containers:
- name: ttl
image: perl:5.34.0
command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"]
restartPolicy: Never
apiVersion: batch/v1
spec:
template:
spec:
containers:
- name: name
image: busybox
imagePullPolicy: IfNotPresent
command:
- /bin/sh
- -c
- date; echo Hello from Kubernetes Cluster
restartPolicy: OnFailure

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml
임의의 시점에 파드를 업데이트하는 경우는 데못셋과 연결된 파드를 kubectl delete pod 명령으로 수동으로 정지시키고 자동화된 복구 기능으로 새로운 파드를 생성해야 함
- 리소스를 위한 매니페스트 작성: sample-statefulset.yaml
apiVersion: apps/v1
name: sample-statefulset
serviceName: sample-statefulset
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
volumeMounts:
- name: www
mountPath: /usr/share/nginx/html
name: www
spec:
accessModes:
- ReadWriteOnce
resources:
requests:
storage: 1G
- 리소스 생성: kubectl apply -f sample-statefulset.yaml
- 확인: kubectl get pods
- 볼륨 클레임 확인: kubectl get persistentvolumeclaims
- persistemvolume 확인: kubectl get persistentvolumes
kubectl scale statefulset sample-statefulset --replicas=5
apiVersion: batch/v1
spec:
containers:
- name: pi
image: perl:5.34.0
command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"]
restartPolicy: Never
kubectl apply -f sample-job.yaml
kubectl get jobs
kubectl get pods --watch
kubectl logs pi
apiVersion: batch/v1
spec:
containers:
- name: errorjob-unvalidcommand
image: busybox
command: ["ls", "unvalid path"]
restartPolicy: Never
리소스 생성: kubectl apply -f sample-job-never-restart.yaml
파드 확인: kubectl get pods
apiVersion: batch/v1
spec:
containers:
- name: errorjob-unvalidcommand
image: busybox
command: ["ls", "unvalid path"]
restartPolicy: OnFailure
리소스 생성: kubectl apply -fsample-job-onfailure-restart.yaml
파드 확인: kubectl get pods --watch
completions	parallelism	backoffLimit
apiVersion: batch/v1
spec:
containers:
- name: ttl
image: perl:5.34.0
command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"]
restartPolicy: Never
apiVersion: batch/v1
spec:
template:
spec:
containers:
- name: name
image: busybox
imagePullPolicy: IfNotPresent
command:
- /bin/sh
- -c
- date; echo Hello from Kubernetes Cluster
restartPolicy: OnFailure

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml
임의의 시점에 파드를 업데이트하는 경우는 데못셋과 연결된 파드를 kubectl delete pod 명령으로 수동으로 정지시키고 자동화된 복구 기능으로 새로운 파드를 생성해야 함
- 리소스를 위한 매니페스트 작성: sample-statefulset.yaml
apiVersion: apps/v1
name: sample-statefulset
serviceName: sample-statefulset
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
volumeMounts:
- name: www
mountPath: /usr/share/nginx/html
name: www
spec:
accessModes:
- ReadWriteOnce
resources:
requests:
storage: 1G
- 리소스 생성: kubectl apply -f sample-statefulset.yaml
- 확인: kubectl get pods
- 볼륨 클레임 확인: kubectl get persistentvolumeclaims
- persistemvolume 확인: kubectl get persistentvolumes
kubectl scale statefulset sample-statefulset --replicas=5
apiVersion: batch/v1
spec:
containers:
- name: pi
image: perl:5.34.0
command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"]
restartPolicy: Never
kubectl apply -f sample-job.yaml
kubectl get jobs
kubectl get pods --watch
kubectl logs pi
apiVersion: batch/v1
spec:
containers:
- name: errorjob-unvalidcommand
image: busybox
command: ["ls", "unvalid path"]
restartPolicy: Never
리소스 생성: kubectl apply -f sample-job-never-restart.yaml
파드 확인: kubectl get pods
apiVersion: batch/v1
spec:
containers:
- name: errorjob-unvalidcommand
image: busybox
command: ["ls", "unvalid path"]
restartPolicy: OnFailure
리소스 생성: kubectl apply -fsample-job-onfailure-restart.yaml
파드 확인: kubectl get pods --watch
completions	parallelism	backoffLimit
apiVersion: batch/v1
spec:
containers:
- name: ttl
image: perl:5.34.0
command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"]
restartPolicy: Never
apiVersion: batch/v1
spec:
template:
spec:
containers:
- name: name
image: busybox
imagePullPolicy: IfNotPresent
command:
- /bin/sh
- -c
- date; echo Hello from Kubernetes Cluster
restartPolicy: OnFailure

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml
임의의 시점에 파드를 업데이트하는 경우는 데못셋과 연결된 파드를 kubectl delete pod 명령으로 수동으로 정지시키고 자동화된 복구 기능으로 새로운 파드를 생성해야 함
- 리소스를 위한 매니페스트 작성: sample-statefulset.yaml
apiVersion: apps/v1
name: sample-statefulset
serviceName: sample-statefulset
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
volumeMounts:
- name: www
mountPath: /usr/share/nginx/html
name: www
spec:
accessModes:
- ReadWriteOnce
resources:
requests:
storage: 1G
- 리소스 생성: kubectl apply -f sample-statefulset.yaml
- 확인: kubectl get pods
- 볼륨 클레임 확인: kubectl get persistentvolumeclaims
- persistemvolume 확인: kubectl get persistentvolumes
kubectl scale statefulset sample-statefulset --replicas=5
apiVersion: batch/v1
spec:
containers:
- name: pi
image: perl:5.34.0
command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"]
restartPolicy: Never
kubectl apply -f sample-job.yaml
kubectl get jobs
kubectl get pods --watch
kubectl logs pi
apiVersion: batch/v1
spec:
containers:
- name: errorjob-unvalidcommand
image: busybox
command: ["ls", "unvalid path"]
restartPolicy: Never
리소스 생성: kubectl apply -f sample-job-never-restart.yaml
파드 확인: kubectl get pods
apiVersion: batch/v1
spec:
containers:
- name: errorjob-unvalidcommand
image: busybox
command: ["ls", "unvalid path"]
restartPolicy: OnFailure
리소스 생성: kubectl apply -fsample-job-onfailure-restart.yaml
파드 확인: kubectl get pods --watch
completions	parallelism	backoffLimit
apiVersion: batch/v1
spec:
containers:
- name: ttl
image: perl:5.34.0
command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"]
restartPolicy: Never
apiVersion: batch/v1
spec:
template:
spec:
containers:
- name: name
image: busybox
imagePullPolicy: IfNotPresent
command:
- /bin/sh
- -c
- date; echo Hello from Kubernetes Cluster
restartPolicy: OnFailure

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml
임의의 시점에 파드를 업데이트하는 경우는 데못셋과 연결된 파드를 kubectl delete pod 명령으로 수동으로 정지시키고 자동화된 복구 기능으로 새로운 파드를 생성해야 함
- 리소스를 위한 매니페스트 작성: sample-statefulset.yaml
apiVersion: apps/v1
name: sample-statefulset
serviceName: sample-statefulset
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
volumeMounts:
- name: www
mountPath: /usr/share/nginx/html
name: www
spec:
accessModes:
- ReadWriteOnce
resources:
requests:
storage: 1G
- 리소스 생성: kubectl apply -f sample-statefulset.yaml
- 확인: kubectl get pods
- 볼륨 클레임 확인: kubectl get persistentvolumeclaims
- persistemvolume 확인: kubectl get persistentvolumes
kubectl scale statefulset sample-statefulset --replicas=5
apiVersion: batch/v1
spec:
containers:
- name: pi
image: perl:5.34.0
command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"]
restartPolicy: Never
kubectl apply -f sample-job.yaml
kubectl get jobs
kubectl get pods --watch
kubectl logs pi
apiVersion: batch/v1
spec:
containers:
- name: errorjob-unvalidcommand
image: busybox
command: ["ls", "unvalid path"]
restartPolicy: Never
리소스 생성: kubectl apply -f sample-job-never-restart.yaml
파드 확인: kubectl get pods
apiVersion: batch/v1
spec:
containers:
- name: errorjob-unvalidcommand
image: busybox
command: ["ls", "unvalid path"]
restartPolicy: OnFailure
리소스 생성: kubectl apply -fsample-job-onfailure-restart.yaml
파드 확인: kubectl get pods --watch
completions	parallelism	backoffLimit
apiVersion: batch/v1
spec:
containers:
- name: ttl
image: perl:5.34.0
command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"]
restartPolicy: Never
apiVersion: batch/v1
spec:
template:
spec:
containers:
- name: name
image: busybox
imagePullPolicy: IfNotPresent
command:
- /bin/sh
- -c
- date; echo Hello from Kubernetes Cluster
restartPolicy: OnFailure

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml
임의의 시점에 파드를 업데이트하는 경우는 데못셋과 연결된 파드를 kubectl delete pod 명령으로 수동으로 정지시키고 자동화된 복구 기능으로 새로운 파드를 생성해야 함
- 리소스를 위한 매니페스트 작성: sample-statefulset.yaml
apiVersion: apps/v1
name: sample-statefulset
serviceName: sample-statefulset
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
volumeMounts:
- name: www
mountPath: /usr/share/nginx/html
name: www
spec:
accessModes:
- ReadWriteOnce
resources:
requests:
storage: 1G
- 리소스 생성: kubectl apply -f sample-statefulset.yaml
- 확인: kubectl get pods
- 볼륨 클레임 확인: kubectl get persistentvolumeclaims
- persistemvolume 확인: kubectl get persistentvolumes
kubectl scale statefulset sample-statefulset --replicas=5
apiVersion: batch/v1
spec:
containers:
- name: pi
image: perl:5.34.0
command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"]
restartPolicy: Never
kubectl apply -f sample-job.yaml
kubectl get jobs
kubectl get pods --watch
kubectl logs pi
apiVersion: batch/v1
spec:
containers:
- name: errorjob-unvalidcommand
image: busybox
command: ["ls", "unvalid path"]
restartPolicy: Never
리소스 생성: kubectl apply -f sample-job-never-restart.yaml
파드 확인: kubectl get pods
apiVersion: batch/v1
spec:
containers:
- name: errorjob-unvalidcommand
image: busybox
command: ["ls", "unvalid path"]
restartPolicy: OnFailure
리소스 생성: kubectl apply -fsample-job-onfailure-restart.yaml
파드 확인: kubectl get pods --watch
completions	parallelism	backoffLimit
apiVersion: batch/v1
spec:
containers:
- name: ttl
image: perl:5.34.0
command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"]
restartPolicy: Never
apiVersion: batch/v1
spec:
template:
spec:
containers:
- name: name
image: busybox
imagePullPolicy: IfNotPresent
command:
- /bin/sh
- -c
- date; echo Hello from Kubernetes Cluster
restartPolicy: OnFailure

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml
임의의 시점에 파드를 업데이트하는 경우는 데못셋과 연결된 파드를 kubectl delete pod 명령으로 수동으로 정지시키고 자동화된 복구 기능으로 새로운 파드를 생성해야 함
- 리소스를 위한 매니페스트 작성: sample-statefulset.yaml
apiVersion: apps/v1
name: sample-statefulset
serviceName: sample-statefulset
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
volumeMounts:
- name: www
mountPath: /usr/share/nginx/html
name: www
spec:
accessModes:
- ReadWriteOnce
resources:
requests:
storage: 1G
- 리소스 생성: kubectl apply -f sample-statefulset.yaml
- 확인: kubectl get pods
- 볼륨 클레임 확인: kubectl get persistentvolumeclaims
- persistemvolume 확인: kubectl get persistentvolumes
kubectl scale statefulset sample-statefulset --replicas=5
apiVersion: batch/v1
spec:
containers:
- name: pi
image: perl:5.34.0
command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"]
restartPolicy: Never
kubectl apply -f sample-job.yaml
kubectl get jobs
kubectl get pods --watch
kubectl logs pi
apiVersion: batch/v1
spec:
containers:
- name: errorjob-unvalidcommand
image: busybox
command: ["ls", "unvalid path"]
restartPolicy: Never
리소스 생성: kubectl apply -f sample-job-never-restart.yaml
파드 확인: kubectl get pods
apiVersion: batch/v1
spec:
containers:
- name: errorjob-unvalidcommand
image: busybox
command: ["ls", "unvalid path"]
restartPolicy: OnFailure
리소스 생성: kubectl apply -fsample-job-onfailure-restart.yaml
파드 확인: kubectl get pods --watch
completions	parallelism	backoffLimit
apiVersion: batch/v1
spec:
containers:
- name: ttl
image: perl:5.34.0
command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"]
restartPolicy: Never
apiVersion: batch/v1
spec:
template:
spec:
containers:
- name: name
image: busybox
imagePullPolicy: IfNotPresent
command:
- /bin/sh
- -c
- date; echo Hello from Kubernetes Cluster
restartPolicy: OnFailure

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml
임의의 시점에 파드를 업데이트하는 경우는 데못셋과 연결된 파드를 kubectl delete pod 명령으로 수동으로 정지시키고 자동화된 복구 기능으로 새로운 파드를 생성해야 함
- 리소스를 위한 매니페스트 작성: sample-statefulset.yaml
apiVersion: apps/v1
name: sample-statefulset
serviceName: sample-statefulset
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
volumeMounts:
- name: www
mountPath: /usr/share/nginx/html
name: www
spec:
accessModes:
- ReadWriteOnce
resources:
requests:
storage: 1G
- 리소스 생성: kubectl apply -f sample-statefulset.yaml
- 확인: kubectl get pods
- 볼륨 클레임 확인: kubectl get persistentvolumeclaims
- persistemvolume 확인: kubectl get persistentvolumes
kubectl scale statefulset sample-statefulset --replicas=5
apiVersion: batch/v1
spec:
containers:
- name: pi
image: perl:5.34.0
command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"]
restartPolicy: Never
kubectl apply -f sample-job.yaml
kubectl get jobs
kubectl get pods --watch
kubectl logs pi
apiVersion: batch/v1
spec:
containers:
- name: errorjob-unvalidcommand
image: busybox
command: ["ls", "unvalid path"]
restartPolicy: Never
리소스 생성: kubectl apply -f sample-job-never-restart.yaml
파드 확인: kubectl get pods
apiVersion: batch/v1
spec:
containers:
- name: errorjob-unvalidcommand
image: busybox
command: ["ls", "unvalid path"]
restartPolicy: OnFailure
리소스 생성: kubectl apply -fsample-job-onfailure-restart.yaml
파드 확인: kubectl get pods --watch
completions	parallelism	backoffLimit
apiVersion: batch/v1
spec:
containers:
- name: ttl
image: perl:5.34.0
command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"]
restartPolicy: Never
apiVersion: batch/v1
spec:
template:
spec:
containers:
- name: name
image: busybox
imagePullPolicy: IfNotPresent
command:
- /bin/sh
- -c
- date; echo Hello from Kubernetes Cluster
restartPolicy: OnFailure

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml
임의의 시점에 파드를 업데이트하는 경우는 데못셋과 연결된 파드를 kubectl delete pod 명령으로 수동으로 정지시키고 자동화된 복구 기능으로 새로운 파드를 생성해야 함
- 리소스를 위한 매니페스트 작성: sample-statefulset.yaml
apiVersion: apps/v1
name: sample-statefulset
serviceName: sample-statefulset
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
volumeMounts:
- name: www
mountPath: /usr/share/nginx/html
name: www
spec:
accessModes:
- ReadWriteOnce
resources:
requests:
storage: 1G
- 리소스 생성: kubectl apply -f sample-statefulset.yaml
- 확인: kubectl get pods
- 볼륨 클레임 확인: kubectl get persistentvolumeclaims
- persistemvolume 확인: kubectl get persistentvolumes
kubectl scale statefulset sample-statefulset --replicas=5
apiVersion: batch/v1
spec:
containers:
- name: pi
image: perl:5.34.0
command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"]
restartPolicy: Never
kubectl apply -f sample-job.yaml
kubectl get jobs
kubectl get pods --watch
kubectl logs pi
apiVersion: batch/v1
spec:
containers:
- name: errorjob-unvalidcommand
image: busybox
command: ["ls", "unvalid path"]
restartPolicy: Never
리소스 생성: kubectl apply -f sample-job-never-restart.yaml
파드 확인: kubectl get pods
apiVersion: batch/v1
spec:
containers:
- name: errorjob-unvalidcommand
image: busybox
command: ["ls", "unvalid path"]
restartPolicy: OnFailure
리소스 생성: kubectl apply -fsample-job-onfailure-restart.yaml
파드 확인: kubectl get pods --watch
completions	parallelism	backoffLimit
apiVersion: batch/v1
spec:
containers:
- name: ttl
image: perl:5.34.0
command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"]
restartPolicy: Never
apiVersion: batch/v1
spec:
template:
spec:
containers:
- name: name
image: busybox
imagePullPolicy: IfNotPresent
command:
- /bin/sh
- -c
- date; echo Hello from Kubernetes Cluster
restartPolicy: OnFailure

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml
임의의 시점에 파드를 업데이트하는 경우는 데못셋과 연결된 파드를 kubectl delete pod 명령으로 수동으로 정지시키고 자동화된 복구 기능으로 새로운 파드를 생성해야 함
- 리소스를 위한 매니페스트 작성: sample-statefulset.yaml
apiVersion: apps/v1
name: sample-statefulset
serviceName: sample-statefulset
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
volumeMounts:
- name: www
mountPath: /usr/share/nginx/html
name: www
spec:
accessModes:
- ReadWriteOnce
resources:
requests:
storage: 1G
- 리소스 생성: kubectl apply -f sample-statefulset.yaml
- 확인: kubectl get pods
- 볼륨 클레임 확인: kubectl get persistentvolumeclaims
- persistemvolume 확인: kubectl get persistentvolumes
kubectl scale statefulset sample-statefulset --replicas=5
apiVersion: batch/v1
spec:
containers:
- name: pi
image: perl:5.34.0
command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"]
restartPolicy: Never
kubectl apply -f sample-job.yaml
kubectl get jobs
kubectl get pods --watch
kubectl logs pi
apiVersion: batch/v1
spec:
containers:
- name: errorjob-unvalidcommand
image: busybox
command: ["ls", "unvalid path"]
restartPolicy: Never
리소스 생성: kubectl apply -f sample-job-never-restart.yaml
파드 확인: kubectl get pods
apiVersion: batch/v1
spec:
containers:
- name: errorjob-unvalidcommand
image: busybox
command: ["ls", "unvalid path"]
restartPolicy: OnFailure
리소스 생성: kubectl apply -fsample-job-onfailure-restart.yaml
파드 확인: kubectl get pods --watch
completions	parallelism	backoffLimit
apiVersion: batch/v1
spec:
containers:
- name: ttl
image: perl:5.34.0
command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"]
restartPolicy: Never
apiVersion: batch/v1
spec:
template:
spec:
containers:
- name: name
image: busybox
imagePullPolicy: IfNotPresent
command:
- /bin/sh
- -c
- date; echo Hello from Kubernetes Cluster
restartPolicy: OnFailure

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml
임의의 시점에 파드를 업데이트하는 경우는 데못셋과 연결된 파드를 kubectl delete pod 명령으로 수동으로 정지시키고 자동화된 복구 기능으로 새로운 파드를 생성해야 함
- 리소스를 위한 매니페스트 작성: sample-statefulset.yaml
apiVersion: apps/v1
name: sample-statefulset
serviceName: sample-statefulset
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
volumeMounts:
- name: www
mountPath: /usr/share/nginx/html
name: www
spec:
accessModes:
- ReadWriteOnce
resources:
requests:
storage: 1G
- 리소스 생성: kubectl apply -f sample-statefulset.yaml
- 확인: kubectl get pods
- 볼륨 클레임 확인: kubectl get persistentvolumeclaims
- persistemvolume 확인: kubectl get persistentvolumes
kubectl scale statefulset sample-statefulset --replicas=5
apiVersion: batch/v1
spec:
containers:
- name: pi
image: perl:5.34.0
command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"]
restartPolicy: Never
kubectl apply -f sample-job.yaml
kubectl get jobs
kubectl get pods --watch
kubectl logs pi
apiVersion: batch/v1
spec:
containers:
- name: errorjob-unvalidcommand
image: busybox
command: ["ls", "unvalid path"]
restartPolicy: Never
리소스 생성: kubectl apply -f sample-job-never-restart.yaml
파드 확인: kubectl get pods
apiVersion: batch/v1
spec:
containers:
- name: errorjob-unvalidcommand
image: busybox
command: ["ls", "unvalid path"]
restartPolicy: OnFailure
리소스 생성: kubectl apply -fsample-job-onfailure-restart.yaml
파드 확인: kubectl get pods --watch
completions	parallelism	backoffLimit
apiVersion: batch/v1
spec:
containers:
- name: ttl
image: perl:5.34.0
command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"]
restartPolicy: Never
apiVersion: batch/v1
spec:
template:
spec:
containers:
- name: name
image: busybox
imagePullPolicy: IfNotPresent
command:
- /bin/sh
- -c
- date; echo Hello from Kubernetes Cluster
restartPolicy: OnFailure

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml
임의의 시점에 파드를 업데이트하는 경우는 데못셋과 연결된 파드를 kubectl delete pod 명령으로 수동으로 정지시키고 자동화된 복구 기능으로 새로운 파드를 생성해야 함
- 리소스를 위한 매니페스트 작성: sample-statefulset.yaml
apiVersion: apps/v1
name: sample-statefulset
serviceName: sample-statefulset
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
volumeMounts:
- name: www
mountPath: /usr/share/nginx/html
name: www
spec:
accessModes:
- ReadWriteOnce
resources:
requests:
storage: 1G
- 리소스 생성: kubectl apply -f sample-statefulset.yaml
- 확인: kubectl get pods
- 볼륨 클레임 확인: kubectl get persistentvolumeclaims
- persistemvolume 확인: kubectl get persistentvolumes
kubectl scale statefulset sample-statefulset --replicas=5
apiVersion: batch/v1
spec:
containers:
- name: pi
image: perl:5.34.0
command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"]
restartPolicy: Never
kubectl apply -f sample-job.yaml
kubectl get jobs
kubectl get pods --watch
kubectl logs pi
apiVersion: batch/v1
spec:
containers:
- name: errorjob-unvalidcommand
image: busybox
command: ["ls", "unvalid path"]
restartPolicy: Never
리소스 생성: kubectl apply -f sample-job-never-restart.yaml
파드 확인: kubectl get pods
apiVersion: batch/v1
spec:
containers:
- name: errorjob-unvalidcommand
image: busybox
command: ["ls", "unvalid path"]
restartPolicy: OnFailure
리소스 생성: kubectl apply -fsample-job-onfailure-restart.yaml
파드 확인: kubectl get pods --watch
completions	parallelism	backoffLimit
apiVersion: batch/v1
spec:
containers:
- name: ttl
image: perl:5.34.0
command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"]
restartPolicy: Never
apiVersion: batch/v1
spec:
template:
spec:
containers:
- name: name
image: busybox
imagePullPolicy: IfNotPresent
command:
- /bin/sh
- -c
- date; echo Hello from Kubernetes Cluster
restartPolicy: OnFailure

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml
임의의 시점에 파드를 업데이트하는 경우는 데못셋과 연결된 파드를 kubectl delete pod 명령으로 수동으로 정지시키고 자동화된 복구 기능으로 새로운 파드를 생성해야 함
- 리소스를 위한 매니페스트 작성: sample-statefulset.yaml
apiVersion: apps/v1
name: sample-statefulset
serviceName: sample-statefulset
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
volumeMounts:
- name: www
mountPath: /usr/share/nginx/html
name: www
spec:
accessModes:
- ReadWriteOnce
resources:
requests:
storage: 1G
- 리소스 생성: kubectl apply -f sample-statefulset.yaml
- 확인: kubectl get pods
- 볼륨 클레임 확인: kubectl get persistentvolumeclaims
- persistemvolume 확인: kubectl get persistentvolumes
kubectl scale statefulset sample-statefulset --replicas=5
apiVersion: batch/v1
spec:
containers:
- name: pi
image: perl:5.34.0
command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"]
restartPolicy: Never
kubectl apply -f sample-job.yaml
kubectl get jobs
kubectl get pods --watch
kubectl logs pi
apiVersion: batch/v1
spec:
containers:
- name: errorjob-unvalidcommand
image: busybox
command: ["ls", "unvalid path"]
restartPolicy: Never
리소스 생성: kubectl apply -f sample-job-never-restart.yaml
파드 확인: kubectl get pods
apiVersion: batch/v1
spec:
containers:
- name: errorjob-unvalidcommand
image: busybox
command: ["ls", "unvalid path"]
restartPolicy: OnFailure
리소스 생성: kubectl apply -fsample-job-onfailure-restart.yaml
파드 확인: kubectl get pods --watch
completions	parallelism	backoffLimit
apiVersion: batch/v1
spec:
containers:
- name: ttl
image: perl:5.34.0
command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"]
restartPolicy: Never
apiVersion: batch/v1
spec:
template:
spec:
containers:
- name: name
image: busybox
imagePullPolicy: IfNotPresent
command:
- /bin/sh
- -c
- date; echo Hello from Kubernetes Cluster
restartPolicy: OnFailure

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml
임의의 시점에 파드를 업데이트하는 경우는 데못셋과 연결된 파드를 kubectl delete pod 명령으로 수동으로 정지시키고 자동화된 복구 기능으로 새로운 파드를 생성해야 함
- 리소스를 위한 매니페스트 작성: sample-statefulset.yaml
apiVersion: apps/v1
name: sample-statefulset
serviceName: sample-statefulset
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
volumeMounts:
- name: www
mountPath: /usr/share/nginx/html
name: www
spec:
accessModes:
- ReadWriteOnce
resources:
requests:
storage: 1G
- 리소스 생성: kubectl apply -f sample-statefulset.yaml
- 확인: kubectl get pods
- 볼륨 클레임 확인: kubectl get persistentvolumeclaims
- persistemvolume 확인: kubectl get persistentvolumes
kubectl scale statefulset sample-statefulset --replicas=5
apiVersion: batch/v1
spec:
containers:
- name: pi
image: perl:5.34.0
command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"]
restartPolicy: Never
kubectl apply -f sample-job.yaml
kubectl get jobs
kubectl get pods --watch
kubectl logs pi
apiVersion: batch/v1
spec:
containers:
- name: errorjob-unvalidcommand
image: busybox
command: ["ls", "unvalid path"]
restartPolicy: Never
리소스 생성: kubectl apply -f sample-job-never-restart.yaml
파드 확인: kubectl get pods
apiVersion: batch/v1
spec:
containers:
- name: errorjob-unvalidcommand
image: busybox
command: ["ls", "unvalid path"]
restartPolicy: OnFailure
리소스 생성: kubectl apply -fsample-job-onfailure-restart.yaml
파드 확인: kubectl get pods --watch
completions	parallelism	backoffLimit
apiVersion: batch/v1
spec:
containers:
- name: ttl
image: perl:5.34.0
command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"]
restartPolicy: Never
apiVersion: batch/v1
spec:
template:
spec:
containers:
- name: name
image: busybox
imagePullPolicy: IfNotPresent
command:
- /bin/sh
- -c
- date; echo Hello from Kubernetes Cluster
restartPolicy: OnFailure

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml
임의의 시점에 파드를 업데이트하는 경우는 데못셋과 연결된 파드를 kubectl delete pod 명령으로 수동으로 정지시키고 자동화된 복구 기능으로 새로운 파드를 생성해야 함
- 리소스를 위한 매니페스트 작성: sample-statefulset.yaml
apiVersion: apps/v1
name: sample-statefulset
serviceName: sample-statefulset
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
volumeMounts:
- name: www
mountPath: /usr/share/nginx/html
name: www
spec:
accessModes:
- ReadWriteOnce
resources:
requests:
storage: 1G
- 리소스 생성: kubectl apply -f sample-statefulset.yaml
- 확인: kubectl get pods
- 볼륨 클레임 확인: kubectl get persistentvolumeclaims
- persistemvolume 확인: kubectl get persistentvolumes
kubectl scale statefulset sample-statefulset --replicas=5
apiVersion: batch/v1
spec:
containers:
- name: pi
image: perl:5.34.0
command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"]
restartPolicy: Never
kubectl apply -f sample-job.yaml
kubectl get jobs
kubectl get pods --watch
kubectl logs pi
apiVersion: batch/v1
spec:
containers:
- name: errorjob-unvalidcommand
image: busybox
command: ["ls", "unvalid path"]
restartPolicy: Never
리소스 생성: kubectl apply -f sample-job-never-restart.yaml
파드 확인: kubectl get pods
apiVersion: batch/v1
spec:
containers:
- name: errorjob-unvalidcommand
image: busybox
command: ["ls", "unvalid path"]
restartPolicy: OnFailure
리소스 생성: kubectl apply -fsample-job-onfailure-restart.yaml
파드 확인: kubectl get pods --watch
completions	parallelism	backoffLimit
apiVersion: batch/v1
spec:
containers:
- name: ttl
image: perl:5.34.0
command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"]
restartPolicy: Never
apiVersion: batch/v1
spec:
template:
spec:
containers:
- name: name
image: busybox
imagePullPolicy: IfNotPresent
command:
- /bin/sh
- -c
- date; echo Hello from Kubernetes Cluster
restartPolicy: OnFailure

파드 <-> ReplicationControler
- 컨테이너 런타임으로 containerd 나 docker를 사용
apiVersion: v1
image: nginx
image: redis:3.2
kubectl apply -f sample-2pod.yaml
kubectl get pods
apiVersion: v1
image: nginx:1.17
image: nginx
- 매니페스트를 실행: kubectl apply -f sample-2pod-fail.yaml
- 파드를 확인: kubectl get pods
- 컨테이너 로그 확인: kubectl logs sample-2pod-fail -c nginx-container-113
kubectl exec -it sample-pod -- /bin/bash
kubectl exec -it sample-2pod -- /bin/bash
kubectl exec -it sample-2pod -c nginx-container -- /bin/bash
apiVersion: v1
image: nginx:1.17
command: ["/bin/sleep"]
args: ["3600"]
kubectl get pod sample-pod -o wide
kubectl get node worker1 -o wide
apiVersion: v1
image: nginx
- 리소스 생성: kubectl apply -f sample-hostnetwork.yaml
kubectl get pod sample-hostnetwork -o wide
kubectl get node worker1 -o wide
클러스터 내부 DNS를 사용해서 이름을 해석
서비스 디스커버리나 클러스터 내부 로드 밸런싱에서 사용
기본적으로 클러스터 내부의 DNS 서버에 질의를 하고 클러스터 내부 DNS에서 해석이 안되는 도메인에 대해서는 업스트림 DNS 서버에 질의
클러스터 외부 DNS를 사용
클러스터 내부 DNS를 사용하지 않음
쿠버네티스 노드의 DNS 설정을 그대로 상속받는 것
hostNetwork를 사용한 파드에 클러스터 내부의 DNS를 참조하고 싶은 경우 사용
apiVersion: v1
image: nginx
hostnames:
- google-dns
- google-public-dns
- 리소스 생성: kubectl apply -f sample-hostaliases.yaml
- 확인: kubectl exec -it sample-hostaliases -- cat /etc/hosts
apiVersion: v1
image: nginx
workingDir: /tmp
- 리소스 생성: kubectl apply -f sample-workingdir.yaml
- 확인: kubectl exec -it sample-workingdir -- pwd
### 3)ReplicaSet/ReplicationController
- 원래 레플리카를 생성하는 리소스의 이름은 ReplicationController 였는데 ReplicaSet을 추가하고 일부 기능을 추가
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs.yaml
- 리소스 확인: kubectl get replicasets -o wide
kubectl get pods -l app=sample-app -o wide
- 파드 삭제:  kubectl delete pods sample-rs-5sc4p
- 파드 확인: kubectl get pods -l app=sample-app -o wide
- 증감 이력 확인: kubectl describe replicaset sample-rs
- 레이블은 spec.selector 와 spec.tempate.metadata.labels 부분에 해당
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app-fail
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-rs-fail.yaml
The ReplicaSet "sample-rs-fail" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{"app":"sample-app-fail"}: `selector` does not match template `labels`
매니페스트를 수정하여 kubectl apply -f 명령어를 수행
kubectl scale 명령어를 사용하여 스케일 관리
kubectl scale replicaset sample-rs --replicas 5
apiVersion: apps/v1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment.yaml
kubectl get deployments
kubectl get replicasets
kubectl get pods
kubectl apply -f sample-deployment.yaml --record=true
kubectl get replicasets -o yaml | head
kubectl set image deployment sample-deployment nginx-container=nginx:1.17 --record
kubectl rollout status deployment sample-deployment
kubectl get deployments
kubectl get replicasets
- 변경 이력을 확인할 때는 kubectl rollout history 명령을 사용하는데 CHANGE-CAUSE 부분이 디플로이먼트를 생성할 때 --record 옵션을 사용하여 이력 내용이 있지만 --record 옵션을 사용하지 않으면 none
- 변경 롤백 확인: kubectl rollout history deployment sample-deployment
- 상세하게 보기:  kubectl rollout history deployment sample-deployment --revision 넘버
kubectl rollout undo deployment 이름 --to-revision 번호
kubectl rollout undo deployment sample-deployment --to-revision 2
kubectl rollout undo deployment 이름 
- 실제 환경에서는 롤백 기능을 사용하는 경우는 많지 않은데 CI/CD 파이프라인에서 롤백을 하는 경우 kubectl rollout 명령보다는 이전 매니페스트를 다시 kubectl apply 명령어로 실행하여 적용하는 것이 호환성 면에서 좋기 때문이며 spec.template을 같은 내용으로 되도렸을 경우에는 pod-template 해시 값이 같으므로 kubectl rollout 처럼 기존있던 레플리카셋의 파드가 기동됨
kubectl rollout pause deployment 디플로이먼트이름
kubectl rollout pause deployment sample-deployment
kubectl set image deployment sample-deployment nginx-container=nginx:1.17
kubectl rollout status deployment sample-deployment
kubectl rollout undo deployment sample-deployment
kubectl rollout resume deployment sample-deployment
apiVersion: apps/v1
type: Recreate
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-recreate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
Rolling 업데이트를 위해 최대로 생성할 수 있는 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
Rolling 업데이트 중 최대로 삭제할 pod 개수
배포를 빠르게 적용 가능
% 또는 개수로 지정
설정하지 않을 경우 기본 값은 25%
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
- 리소스 생성: kubectl apply -f sample-deployment-rollingupdate.yaml
- 이미지 변경: kubectl set image deployment sample-deployment-recreate nginx-container=nginx:1.17
- 레플리카셋 확인: kubectl get replicasets --watch
apiVersion: apps/v1
type: RollingUpdate
rollingUpdate:
maxUnavailable: 0
maxSurge: 1
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
kubectl scale deployment sample-deployment --replicas=5
kubectl create deployment 이름 --image 이미지
kubectl create deployment sample-deployment-cli --image nginx
apiVersion: apps/v1
matchLabels:
tier: monitoring
name: prometheus-exporter
metadata:
labels:
tier: monitoring
name: prometheus-exporter
spec:
containers:
- name: prometheus
image: prom/node-exporter
ports:
- containerPort: 80
- 리소스 생성: kubectl apply -f daemonsets.yaml
- 상세 정보를 확인: kubectl describe daemonsets/prometheus-daemonset
- 파드 확인: kubectl get pods
- 데몬셋 삭제: kubectl delete -f daemonsets.yaml
임의의 시점에 파드를 업데이트하는 경우는 데못셋과 연결된 파드를 kubectl delete pod 명령으로 수동으로 정지시키고 자동화된 복구 기능으로 새로운 파드를 생성해야 함
- 리소스를 위한 매니페스트 작성: sample-statefulset.yaml
apiVersion: apps/v1
name: sample-statefulset
serviceName: sample-statefulset
matchLabels:
app: sample-app
metadata:
labels:
app: sample-app
spec:
containers:
- name: nginx-container
image: nginx
volumeMounts:
- name: www
mountPath: /usr/share/nginx/html
name: www
spec:
accessModes:
- ReadWriteOnce
resources:
requests:
storage: 1G
- 리소스 생성: kubectl apply -f sample-statefulset.yaml
- 확인: kubectl get pods
- 볼륨 클레임 확인: kubectl get persistentvolumeclaims
- persistemvolume 확인: kubectl get persistentvolumes
kubectl scale statefulset sample-statefulset --replicas=5
apiVersion: batch/v1
spec:
containers:
- name: pi
image: perl:5.34.0
command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"]
restartPolicy: Never
kubectl apply -f sample-job.yaml
kubectl get jobs
kubectl get pods --watch
kubectl logs pi
apiVersion: batch/v1
spec:
containers:
- name: errorjob-unvalidcommand
image: busybox
command: ["ls", "unvalid path"]
restartPolicy: Never
리소스 생성: kubectl apply -f sample-job-never-restart.yaml
파드 확인: kubectl get pods
apiVersion: batch/v1
spec:
containers:
- name: errorjob-unvalidcommand
image: busybox
command: ["ls", "unvalid path"]
restartPolicy: OnFailure
리소스 생성: kubectl apply -fsample-job-onfailure-restart.yaml
파드 확인: kubectl get pods --watch
completions	parallelism	backoffLimit
apiVersion: batch/v1
spec:
containers:
- name: ttl
image: perl:5.34.0
command: ["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"]
restartPolicy: Never
apiVersion: batch/v1
spec:
template:
spec:
containers:
- name: name
image: busybox
imagePullPolicy: IfNotPresent
command:
- /bin/sh
- -c
- date; echo Hello from Kubernetes Cluster
restartPolicy: OnFailure
